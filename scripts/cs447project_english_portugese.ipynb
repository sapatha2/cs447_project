{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cs447project_english_portugese.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMjvLunU4nRnpyTPGYpFUaQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bnqyBxYPLyG6","executionInfo":{"status":"ok","timestamp":1607030739975,"user_tz":360,"elapsed":8086,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"0a41a8d4-2fc2-4df7-8444-d6dbf0be67c2"},"source":["# PyTorch \n","!pip install --upgrade torch\n","!pip install --upgrade torchtext"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n","Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Collecting torchtext\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/23/8499af6d9c22b29b01f66a2c11d38ce71cd1cafa2655913c29818ed4a00f/torchtext-0.8.0-cp36-cp36m-manylinux1_x86_64.whl (6.9MB)\n","\u001b[K     |████████████████████████████████| 6.9MB 4.0MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n","Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.7.0+cu101)\n","Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.11.8)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.8)\n","Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n","Installing collected packages: torchtext\n","  Found existing installation: torchtext 0.3.1\n","    Uninstalling torchtext-0.3.1:\n","      Successfully uninstalled torchtext-0.3.1\n","Successfully installed torchtext-0.8.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bnDeYuA_LnXJ","executionInfo":{"status":"ok","timestamp":1607030744011,"user_tz":360,"elapsed":12116,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["from collections import defaultdict\n","import nltk\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction, corpus_bleu\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import re\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm.notebook import tqdm\n","import unicodedata\n","\n","if __name__ == '__main__':\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zQ4KX9-QTZcI"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"tUeHdn_uTfZ1","executionInfo":{"status":"ok","timestamp":1607030744014,"user_tz":360,"elapsed":12116,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Converts the unicode file to ascii\n","def unicode_to_ascii(s):\n","    \"\"\"Normalizes latin chars with accent to their canonical decomposition\"\"\"\n","    return ''.join(c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn')\n","\n","# Preprocessing the sentence to add the start, end tokens and make them lower-case\n","def preprocess_sentence(w):\n","    w = unicode_to_ascii(w.lower().strip())\n","    w = re.sub(r'([?.!,¿])', r' \\1 ', w)\n","    w = re.sub(r'[\" \"]+', ' ', w)\n","    w = re.sub(r'[^\\w\\s]', '', w) \n","\n","    w = re.sub(r'[^a-zA-Z?.!,¿]+', ' ', w)\n","    \n","    w = w.rstrip().strip()\n","    w = '<start> ' + w + ' <end>'\n","    return w\n","\n","def max_length(tensor):\n","    return max(len(t) for t in tensor)\n","\n","def pad_sequences(x, max_len):\n","    padded = np.zeros((max_len), dtype=np.int64)\n","    if len(x) > max_len:\n","        padded[:] = x[:max_len]\n","    else:\n","        padded[:len(x)] = x\n","    return padded\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2indexFull = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n","        self.word2countFull = {\"<start>\": 1e10, \"<end>\": 1e10, \"<unk>\": 1e10, \"<pad>\": 1e10}\n","        self.index2wordFull = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n","        self.word2index = {}\n","        self.index2word = {}\n","        self.n_wordsFull = 4  # Count SOS and EOS\n","        self.n_words = 0\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2indexFull:\n","            self.word2indexFull[word] = self.n_wordsFull\n","            self.word2countFull[word] = 1\n","            self.index2wordFull[self.n_wordsFull] = word\n","            self.n_wordsFull += 1\n","        else:\n","            self.word2countFull[word] += 1\n","\n","    def reduceDictionary(self, threshold = 50):\n","        n_words = 0\n","        for word in self.word2indexFull.keys():\n","            if self.word2countFull[word] >= threshold:\n","                self.word2index[word] = n_words\n","                self.index2word[n_words] = word\n","                n_words += 1\n","        self.n_words = n_words\n","    \n","    def sentence2Index(self, sentence):\n","        output = []\n","        for word in sentence.split(' '):\n","            if word in self.word2index.keys():\n","                output.append(self.word2index[word])\n","            else:\n","                output.append(self.word2index[\"<unk>\"])\n","        return output\n","\n","def build_dataset(target_language, threshold):\n","    # Load in and process sentences\n","    lines = open(target_language+'.txt', encoding='UTF-8').read().strip().split('\\n')\n","    original_word_pairs = [[w for w in l.split('\\t')][:2] for l in lines]\n","\n","    data = pd.DataFrame(original_word_pairs, columns=['eng', target_language])\n","    data['eng'] = data.eng.apply(lambda w: preprocess_sentence(w))\n","    data[target_language] = data[target_language].apply(lambda w: preprocess_sentence(w))\n","\n","    # Remove all sentences with length longer than 10 (+ 2 for start/end)\n","    data['len_eng'] = data.eng.apply(lambda w: len(w.split(\" \")))\n","    data['len_'+target_language] = data[target_language].apply(lambda w: len(w.split(\" \")))\n","    data = data[(data['len_eng'] <= MAX_LEN + 2)*(data['len_'+target_language] <= MAX_LEN + 2)]\n","    data = data[['eng',target_language]]\n","\n","    # Build language dictionaries \n","    input_lang = Lang('eng')\n","    output_lang = Lang(target_language)\n","    for sentence in data['eng']:\n","      input_lang.addSentence(sentence)\n","\n","    for sentence in data[target_language]:\n","      output_lang.addSentence(sentence)\n","    input_lang.reduceDictionary(threshold)\n","    output_lang.reduceDictionary(threshold)\n","\n","    data['eng'] = data.eng.apply(lambda w: input_lang.sentence2Index(w))\n","    data[target_language] = data[target_language].apply(lambda w: output_lang.sentence2Index(w))\n","    data['eng'] = data.eng.apply(lambda w: pad_sequences(w, MAX_LEN + 2))\n","    data[target_language] = data[target_language].apply(lambda w: pad_sequences(w, MAX_LEN + 2))\n","\n","    # Filter out sentences that have more than 1 (10%) UNK\n","    eng_filter = data['eng'].apply(lambda w: np.sum(w == input_lang.word2index['<unk>']))\n","    target_filter = data[target_language].apply(lambda w: np.sum(w == output_lang.word2index['<unk>']))\n","    data = data[(eng_filter <= MAX_UNK) * (target_filter <= MAX_UNK)]\n","\n","    return input_lang, output_lang, data"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bJBt9wUfT3Jo","executionInfo":{"status":"ok","timestamp":1607030746188,"user_tz":360,"elapsed":14287,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"c6d8118b-5f65-48f7-c5fb-5f370c9ff922"},"source":["!wget http://www.manythings.org/anki/por-eng.zip\n","!unzip -o por-eng.zip"],"execution_count":4,"outputs":[{"output_type":"stream","text":["--2020-12-03 21:25:43--  http://www.manythings.org/anki/por-eng.zip\n","Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 172.67.173.198, ...\n","Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5639779 (5.4M) [application/zip]\n","Saving to: ‘por-eng.zip’\n","\n","por-eng.zip         100%[===================>]   5.38M  4.23MB/s    in 1.3s    \n","\n","2020-12-03 21:25:45 (4.23 MB/s) - ‘por-eng.zip’ saved [5639779/5639779]\n","\n","Archive:  por-eng.zip\n","  inflating: _about.txt              \n","  inflating: por.txt                 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zkGty2WyUFNT","executionInfo":{"status":"ok","timestamp":1607030760433,"user_tz":360,"elapsed":28530,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"cb9cf367-9e82-4aaf-9a28-6f3334f2e40c"},"source":["MAX_LEN = 10 # (+2 for <start>, <end>)\n","MAX_UNK = 1000 \n","THRESHOLD = 100\n","input_lang, output_lang, data = build_dataset('por', THRESHOLD) #\n","print(\"Input words {}, Output words {}, N sentences {}\".format(input_lang.n_words, output_lang.n_words, data.shape[0]))\n","print(data)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/computation/expressions.py:204: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n","  f\"evaluating in Python space because the {repr(op_str)} \"\n"],"name":"stderr"},{"output_type":"stream","text":["Input words 860, Output words 901, N sentences 158723\n","                                                      eng                                          por\n","0                    [1, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]         [1, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","1                    [1, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]         [1, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","2                    [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]         [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","3                    [1, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]         [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","4                    [1, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]         [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","...                                                   ...                                          ...\n","167866        [1, 3, 3, 174, 284, 3, 3, 24, 238, 3, 3, 2]  [1, 51, 3, 3, 43, 90, 766, 3, 896, 3, 3, 2]\n","168015     [1, 394, 3, 3, 570, 3, 280, 3, 434, 171, 3, 2]   [1, 51, 620, 3, 3, 3, 281, 3, 43, 3, 2, 0]\n","168038  [1, 39, 264, 3, 3, 434, 675, 3, 649, 818, 756, 2]   [1, 160, 3, 3, 43, 693, 3, 15, 3, 3, 2, 0]\n","168125           [1, 71, 98, 3, 3, 434, 3, 3, 3, 2, 0, 0]    [1, 100, 262, 3, 3, 43, 3, 3, 3, 2, 0, 0]\n","168234      [1, 24, 3, 394, 3, 242, 3, 334, 3, 3, 434, 2]    [1, 46, 3, 51, 3, 34, 3, 3, 61, 43, 2, 0]\n","\n","[158723 rows x 2 columns]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/computation/expressions.py:204: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n","  f\"evaluating in Python space because the {repr(op_str)} \"\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"MRYEK7LNLUgE"},"source":["# Models"]},{"cell_type":"code","metadata":{"id":"-fFrQ7qnLSNd","executionInfo":{"status":"ok","timestamp":1607030760434,"user_tz":360,"elapsed":28528,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Encoder (Takes a sentence seq_len -> returns output, hidden)\n","class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers=1):\n","        super(EncoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, num_layers = num_layers)\n","\n","    def forward(self, input_sentence):\n","        embedded = self.embedding(input_sentence)\n","        output, hidden = self.gru(embedded)  \n","\n","        # For deep\n","        hidden = hidden[-1].unsqueeze(0)         \n","        return output, hidden"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"RqjwRA2jRAcr","executionInfo":{"status":"ok","timestamp":1607030760436,"user_tz":360,"elapsed":28529,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Decoder (Takes a sentence seq_len -> returns output)\n","class DecoderRNN(nn.Module):\n","    def __init__(self, output_size, hidden_size):\n","        super(DecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim = 2)\n","\n","    def forward(self, input_sentence, hidden, encoder_output):\n","        embedded = self.embedding(input_sentence)              \n","        output, decoder_hidden = self.gru(embedded, hidden)                    \n","        output = self.out(output).squeeze(0)                              \n","        return output, decoder_hidden"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jex7T-VWurHx","executionInfo":{"status":"ok","timestamp":1607030760437,"user_tz":360,"elapsed":28527,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Decoder with attention (Takes a sentence seq_len -> returns output)\n","class AttentionDecoderRNN(nn.Module):\n","    def __init__(self, output_size, hidden_size):\n","        super(AttentionDecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","\n","        self.score = nn.Sequential(\n","            nn.Linear(2 * hidden_size, hidden_size),\n","            nn.Tanh(),\n","            nn.Linear(hidden_size, 1)\n","        )\n","\n","        self.attention = nn.Sequential(\n","            nn.Linear(2 * hidden_size, hidden_size),\n","            nn.Tanh()\n","        )\n","\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim = 2)\n","\n","    def forward(self, input_sentence, hidden, encoder_output):\n","        embedded = self.embedding(input_sentence) # [1, batch_size, ]\n","\n","        # Compute score vector\n","        score_vector = self.score(\n","            torch.cat([torch.cat((MAX_LEN + 2)*[hidden]), encoder_output], dim = 2)\n","        ).squeeze(-1) # [seq_len, batch_size] \n","\n","        # Compute attention weights\n","        attention_weights = F.softmax(score_vector, dim = 0) # [seq_len, batch_size]\n","\n","        # Compute context vector\n","        context_vector = torch.einsum('sb, sbh -> bh', attention_weights, encoder_output) # [batch_size, hidden_size]\n","\n","        # Compute attention vector\n","        attention_vector = self.attention(torch.cat([context_vector.unsqueeze(0), embedded], dim = 2))\n","\n","        # Pass into decoder\n","        output, decoder_hidden = self.gru(attention_vector, hidden)                    \n","        output = self.out(output).squeeze(0)                              \n","        return output, decoder_hidden"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zOxsduH8c-Yt"},"source":["#  Training"]},{"cell_type":"code","metadata":{"id":"nWq4Mfj6dB7C","executionInfo":{"status":"ok","timestamp":1607030760438,"user_tz":360,"elapsed":28526,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["def translate_sentence(encoder, decoder, pair, ref_lang, targ_lang):\n","    \"\"\"\n","    Translate single sentence, returns\n","\n","    reference\n","    target\n","    candidate\n","    \"\"\"\n","    test_loss = 0\n","    candidate = []\n","    with torch.no_grad():\n","        reference = torch.tensor(pair[0]).unsqueeze(1).to(device)\n","        target =    torch.tensor(pair[1]).unsqueeze(1).to(device)\n","\n","        # Encoder pass\n","        encoder_output, encoder_hidden = encoder(reference) \n","  \n","        # Decoder pass\n","        decoder_input = target[0].unsqueeze(0)\n","        decoder_hidden = encoder_hidden\n","        candidate.append(decoder_input)\n","        for j in range(1, len(target)):\n","            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output) \n","            test_loss += loss_function(target[j], decoder_output) / len(target)\n","            decoder_input = F.log_softmax(decoder_output.unsqueeze(0), dim=-1).argmax(dim = -1)\n","            candidate.append(decoder_input)\n","            if decoder_input == targ_lang.word2index['<end>']:\n","                break\n","    \n","    reference = reference[reference > 0]\n","    target = target[target > 0]\n","\n","    reference = [ref_lang.index2word[int(s)] for s in reference]\n","    target =    [targ_lang.index2word[int(s)] for s in target]\n","    candidate = [targ_lang.index2word[int(s)] for s in candidate]\n","\n","    smoother = SmoothingFunction()\n","    bleu1 = sentence_bleu([target[1:]], candidate[1:], weights=(1,), smoothing_function=smoother.method1)\n","    bleu2 = sentence_bleu([target[1:]], candidate[1:], weights=(1/2, 1/2), smoothing_function=smoother.method1)\n","    bleu3 = sentence_bleu([target[1:]], candidate[1:], weights=(1/3, 1/3, 1/3), smoothing_function=smoother.method1)\n","    bleu4 = sentence_bleu([target[1:]], candidate[1:], weights=(1/4, 1/4, 1/4, 1/4), smoothing_function=smoother.method1)\n","    return bleu1, bleu2, bleu3, bleu4, test_loss, \" \".join(reference), \" \".join(target), \" \".join(candidate)\n","\n","def loss_function(real, pred):\n","    \"\"\" Only consider non-pad inputs in the loss; mask needed \"\"\"\n","    mask = real.ge(1).float()\n","    \n","    loss_ = F.cross_entropy(pred, real) * mask \n","    return torch.mean(loss_)\n","\n","def train_model(encoder, decoder, targ_lang, train, num_epochs, learning_rate, batch_size, breakp = 1e10):\n","    # Return training losses\n","    losses = []\n","    \n","    # Model, optimizer, criterion\n","    encoder = encoder.to(device)\n","    decoder = decoder.to(device)\n","    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n","\n","    # Build batches\n","    batches = [df for g, df in train.groupby(np.arange(len(train)) // batch_size)]\n","\n","    # Train\n","    for i in range(num_epochs):\n","        epoch_loss = 0\n","        for b in range(len(batches)):\n","            batch = batches[b]\n","            if len(batch) == batch_size: # Discard partial batches\n","                target = torch.tensor([s for s in batch[list(batch)[1]]]).T.to(device)\n","                reference = torch.tensor([s for s in batch[list(batch)[0]]]).T.to(device)\n","\n","                # Encoder pass: [max_len, batch_size, hidden_size], [1, batch_size, hidden_size]\n","                encoder_output, encoder_hidden = encoder(reference) \n","\n","                # Decoder pass: teacher forcing\n","                loss = 0\n","                decoder_input = target[0].unsqueeze(0) # [1, batch_size]\n","                decoder_hidden = encoder_hidden\n","                for j in range(1, len(target)):\n","                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output) # [batch_size, output_size], [1, batch_size, hidden_size]\n","                    loss += loss_function(target[j], decoder_output)\n","                    decoder_input = target[j].unsqueeze(0)\n","\n","                # Step\n","                loss.backward()\n","                encoder_optimizer.step()\n","                decoder_optimizer.step()\n","                encoder_optimizer.zero_grad()\n","                decoder_optimizer.zero_grad()\n","\n","                # Prints\n","                epoch_loss += loss.item() / (len(target) * len(batches))\n","\n","        # Training losses\n","        print(\"EPOCH {}/{}, LOSS {}\".format(i + 1, num_epochs, epoch_loss))\n","        losses.append(epoch_loss)\n","    return losses "],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9R9Gj0aKS7lw"},"source":["# Base model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XeFcWwFpqpLW","executionInfo":{"status":"ok","timestamp":1607030760439,"user_tz":360,"elapsed":28525,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"07027f88-b1d8-4622-f9dc-024889a3ccb1"},"source":["# Train test split\n","data = data.sample(frac = 1, replace = False)\n","train = data.iloc[:data.shape[0]//4 * 3]\n","test = data.iloc[data.shape[0]//4 * 3:]\n","\n","# Model\n","HIDDEN_DIM = 128\n","encoder = EncoderRNN(input_lang.n_words, HIDDEN_DIM)\n","decoder = DecoderRNN(output_lang.n_words, HIDDEN_DIM)\n","print(\"N Params: \", sum(p.numel() for p in encoder.parameters()) + sum(p.numel() for p in decoder.parameters()))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["N Params:  539781\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KLv65YtkgxlA","executionInfo":{"status":"ok","timestamp":1607031047209,"user_tz":360,"elapsed":315292,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"948913a1-d313-4973-ae52-e612d0773560"},"source":["losses = train_model(encoder, decoder, output_lang, train, 200, 1e-3, len(train)//9)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["EPOCH 1/200, LOSS 3.2147253672281906\n","EPOCH 2/200, LOSS 2.62541921933492\n","EPOCH 3/200, LOSS 2.0813798586527508\n","EPOCH 4/200, LOSS 1.9076512177785239\n","EPOCH 5/200, LOSS 1.852983395258586\n","EPOCH 6/200, LOSS 1.816539796193441\n","EPOCH 7/200, LOSS 1.7868217309316\n","EPOCH 8/200, LOSS 1.7589053471883138\n","EPOCH 9/200, LOSS 1.7317227045694987\n","EPOCH 10/200, LOSS 1.7050889333089194\n","EPOCH 11/200, LOSS 1.6792881488800049\n","EPOCH 12/200, LOSS 1.654192860921224\n","EPOCH 13/200, LOSS 1.6295547803243002\n","EPOCH 14/200, LOSS 1.6052305062611896\n","EPOCH 15/200, LOSS 1.5814118862152098\n","EPOCH 16/200, LOSS 1.557776467005412\n","EPOCH 17/200, LOSS 1.534564240773519\n","EPOCH 18/200, LOSS 1.5121285279591878\n","EPOCH 19/200, LOSS 1.4904966831207276\n","EPOCH 20/200, LOSS 1.4696675300598143\n","EPOCH 21/200, LOSS 1.4495645840962728\n","EPOCH 22/200, LOSS 1.4301082293192546\n","EPOCH 23/200, LOSS 1.4107642650604248\n","EPOCH 24/200, LOSS 1.392001756032308\n","EPOCH 25/200, LOSS 1.3737262566884358\n","EPOCH 26/200, LOSS 1.355661424001058\n","EPOCH 27/200, LOSS 1.3381236394246419\n","EPOCH 28/200, LOSS 1.3212238311767581\n","EPOCH 29/200, LOSS 1.30532062848409\n","EPOCH 30/200, LOSS 1.2901702562967936\n","EPOCH 31/200, LOSS 1.2753488063812255\n","EPOCH 32/200, LOSS 1.2610210100809733\n","EPOCH 33/200, LOSS 1.2469944794972738\n","EPOCH 34/200, LOSS 1.233369557062785\n","EPOCH 35/200, LOSS 1.220033597946167\n","EPOCH 36/200, LOSS 1.206748080253601\n","EPOCH 37/200, LOSS 1.193804852167765\n","EPOCH 38/200, LOSS 1.1812080939610798\n","EPOCH 39/200, LOSS 1.1691403388977053\n","EPOCH 40/200, LOSS 1.1573198874791462\n","EPOCH 41/200, LOSS 1.1457509676615398\n","EPOCH 42/200, LOSS 1.134483281771342\n","EPOCH 43/200, LOSS 1.1233299652735391\n","EPOCH 44/200, LOSS 1.1119991540908813\n","EPOCH 45/200, LOSS 1.1009076754252116\n","EPOCH 46/200, LOSS 1.0898063580195108\n","EPOCH 47/200, LOSS 1.078540333112081\n","EPOCH 48/200, LOSS 1.0676218430201212\n","EPOCH 49/200, LOSS 1.0567116022109986\n","EPOCH 50/200, LOSS 1.0456111113230389\n","EPOCH 51/200, LOSS 1.0351158459981282\n","EPOCH 52/200, LOSS 1.0246639251708984\n","EPOCH 53/200, LOSS 1.0143689950307209\n","EPOCH 54/200, LOSS 1.0040811936060587\n","EPOCH 55/200, LOSS 0.994114883740743\n","EPOCH 56/200, LOSS 0.9844725211461384\n","EPOCH 57/200, LOSS 0.9748923142751057\n","EPOCH 58/200, LOSS 0.9659340461095174\n","EPOCH 59/200, LOSS 0.9570945183436075\n","EPOCH 60/200, LOSS 0.9485494931538899\n","EPOCH 61/200, LOSS 0.9399262428283691\n","EPOCH 62/200, LOSS 0.9317735592524211\n","EPOCH 63/200, LOSS 0.9237960974375407\n","EPOCH 64/200, LOSS 0.9159945090611775\n","EPOCH 65/200, LOSS 0.9083660682042439\n","EPOCH 66/200, LOSS 0.9008599360783895\n","EPOCH 67/200, LOSS 0.8935060342152913\n","EPOCH 68/200, LOSS 0.8864235401153565\n","EPOCH 69/200, LOSS 0.8794992208480834\n","EPOCH 70/200, LOSS 0.8726050059000651\n","EPOCH 71/200, LOSS 0.8656076749165852\n","EPOCH 72/200, LOSS 0.8590276082356771\n","EPOCH 73/200, LOSS 0.852474013964335\n","EPOCH 74/200, LOSS 0.8461541175842286\n","EPOCH 75/200, LOSS 0.8399440050125122\n","EPOCH 76/200, LOSS 0.8338289976119995\n","EPOCH 77/200, LOSS 0.8277983824412029\n","EPOCH 78/200, LOSS 0.8218643188476564\n","EPOCH 79/200, LOSS 0.8160502433776855\n","EPOCH 80/200, LOSS 0.8104008992513021\n","EPOCH 81/200, LOSS 0.8049384117126465\n","EPOCH 82/200, LOSS 0.8000161488850912\n","EPOCH 83/200, LOSS 0.7945410410563151\n","EPOCH 84/200, LOSS 0.7889643430709838\n","EPOCH 85/200, LOSS 0.7837211847305299\n","EPOCH 86/200, LOSS 0.7787530501683554\n","EPOCH 87/200, LOSS 0.7737804651260376\n","EPOCH 88/200, LOSS 0.7689743041992189\n","EPOCH 89/200, LOSS 0.7642236312230428\n","EPOCH 90/200, LOSS 0.7595452706019084\n","EPOCH 91/200, LOSS 0.7549072821935017\n","EPOCH 92/200, LOSS 0.7502284288406372\n","EPOCH 93/200, LOSS 0.745579973856608\n","EPOCH 94/200, LOSS 0.7410460472106934\n","EPOCH 95/200, LOSS 0.7366146802902223\n","EPOCH 96/200, LOSS 0.7322859287261962\n","EPOCH 97/200, LOSS 0.7280623435974122\n","EPOCH 98/200, LOSS 0.7239662249883018\n","EPOCH 99/200, LOSS 0.7201248884201049\n","EPOCH 100/200, LOSS 0.7168052434921266\n","EPOCH 101/200, LOSS 0.7138163805007933\n","EPOCH 102/200, LOSS 0.7088010629018148\n","EPOCH 103/200, LOSS 0.7042996883392334\n","EPOCH 104/200, LOSS 0.7001533428827922\n","EPOCH 105/200, LOSS 0.6964226325352987\n","EPOCH 106/200, LOSS 0.6927057027816772\n","EPOCH 107/200, LOSS 0.6889685551325481\n","EPOCH 108/200, LOSS 0.6853922923405965\n","EPOCH 109/200, LOSS 0.6817546049753824\n","EPOCH 110/200, LOSS 0.6782107671101887\n","EPOCH 111/200, LOSS 0.6747195164362589\n","EPOCH 112/200, LOSS 0.6712503592173258\n","EPOCH 113/200, LOSS 0.6678406238555908\n","EPOCH 114/200, LOSS 0.6645005782445271\n","EPOCH 115/200, LOSS 0.6612166086832681\n","EPOCH 116/200, LOSS 0.6579835255940756\n","EPOCH 117/200, LOSS 0.6548041582107544\n","EPOCH 118/200, LOSS 0.6516853173573811\n","EPOCH 119/200, LOSS 0.6486330032348633\n","EPOCH 120/200, LOSS 0.6456503550211589\n","EPOCH 121/200, LOSS 0.6427334944407144\n","EPOCH 122/200, LOSS 0.6398632287979127\n","EPOCH 123/200, LOSS 0.6369968811670939\n","EPOCH 124/200, LOSS 0.6341051816940309\n","EPOCH 125/200, LOSS 0.631224513053894\n","EPOCH 126/200, LOSS 0.6283816893895466\n","EPOCH 127/200, LOSS 0.6255563735961913\n","EPOCH 128/200, LOSS 0.622765024503072\n","EPOCH 129/200, LOSS 0.6200416088104247\n","EPOCH 130/200, LOSS 0.617373275756836\n","EPOCH 131/200, LOSS 0.6147605260213217\n","EPOCH 132/200, LOSS 0.6122419118881226\n","EPOCH 133/200, LOSS 0.6096624533335367\n","EPOCH 134/200, LOSS 0.6070408741633097\n","EPOCH 135/200, LOSS 0.6045645475387573\n","EPOCH 136/200, LOSS 0.6020687898000081\n","EPOCH 137/200, LOSS 0.5996822079022724\n","EPOCH 138/200, LOSS 0.5972925265630086\n","EPOCH 139/200, LOSS 0.5949618736902872\n","EPOCH 140/200, LOSS 0.5927445888519287\n","EPOCH 141/200, LOSS 0.5905633091926574\n","EPOCH 142/200, LOSS 0.588460926214854\n","EPOCH 143/200, LOSS 0.5864779313405355\n","EPOCH 144/200, LOSS 0.5846316854159038\n","EPOCH 145/200, LOSS 0.5831113616625467\n","EPOCH 146/200, LOSS 0.5808586438496908\n","EPOCH 147/200, LOSS 0.5782031059265137\n","EPOCH 148/200, LOSS 0.5758473237355549\n","EPOCH 149/200, LOSS 0.5738792737325032\n","EPOCH 150/200, LOSS 0.5718202352523803\n","EPOCH 151/200, LOSS 0.5698078831036886\n","EPOCH 152/200, LOSS 0.5678521275520325\n","EPOCH 153/200, LOSS 0.5659050027529399\n","EPOCH 154/200, LOSS 0.5639899452527364\n","EPOCH 155/200, LOSS 0.5621122161547343\n","EPOCH 156/200, LOSS 0.5602540930112202\n","EPOCH 157/200, LOSS 0.5584025502204896\n","EPOCH 158/200, LOSS 0.5565577824910481\n","EPOCH 159/200, LOSS 0.5547399044036865\n","EPOCH 160/200, LOSS 0.5529468218485515\n","EPOCH 161/200, LOSS 0.5511430700620016\n","EPOCH 162/200, LOSS 0.549326233069102\n","EPOCH 163/200, LOSS 0.5475507179896038\n","EPOCH 164/200, LOSS 0.5458632747332254\n","EPOCH 165/200, LOSS 0.5442730903625488\n","EPOCH 166/200, LOSS 0.5427745461463928\n","EPOCH 167/200, LOSS 0.5413359840710958\n","EPOCH 168/200, LOSS 0.5398328900337219\n","EPOCH 169/200, LOSS 0.5382238626480103\n","EPOCH 170/200, LOSS 0.5367005507151286\n","EPOCH 171/200, LOSS 0.5351460655530295\n","EPOCH 172/200, LOSS 0.5334288160006205\n","EPOCH 173/200, LOSS 0.5317570288976033\n","EPOCH 174/200, LOSS 0.5300348122914633\n","EPOCH 175/200, LOSS 0.5284903287887572\n","EPOCH 176/200, LOSS 0.5270140767097473\n","EPOCH 177/200, LOSS 0.5255850950876872\n","EPOCH 178/200, LOSS 0.5241994222005208\n","EPOCH 179/200, LOSS 0.5228141109148661\n","EPOCH 180/200, LOSS 0.5214588602383932\n","EPOCH 181/200, LOSS 0.5201500614484151\n","EPOCH 182/200, LOSS 0.5188598871231078\n","EPOCH 183/200, LOSS 0.5175840934117635\n","EPOCH 184/200, LOSS 0.516345238685608\n","EPOCH 185/200, LOSS 0.5151719848314922\n","EPOCH 186/200, LOSS 0.5141305685043335\n","EPOCH 187/200, LOSS 0.5131434082984924\n","EPOCH 188/200, LOSS 0.5120128750801086\n","EPOCH 189/200, LOSS 0.510854430993398\n","EPOCH 190/200, LOSS 0.5095425009727478\n","EPOCH 191/200, LOSS 0.5080500523249308\n","EPOCH 192/200, LOSS 0.5066235899925231\n","EPOCH 193/200, LOSS 0.5052494247754414\n","EPOCH 194/200, LOSS 0.5038419882456462\n","EPOCH 195/200, LOSS 0.5025020877520243\n","EPOCH 196/200, LOSS 0.501232616106669\n","EPOCH 197/200, LOSS 0.4999568661053976\n","EPOCH 198/200, LOSS 0.49863882859547937\n","EPOCH 199/200, LOSS 0.4973881483078002\n","EPOCH 200/200, LOSS 0.49621375799179085\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NkNIfoBzDXKq"},"source":["# Base model + attention"]},{"cell_type":"code","metadata":{"id":"H6oXqCrmDXKq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607031047211,"user_tz":360,"elapsed":315292,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"82f1e9dc-6112-47a9-96f7-25af18788573"},"source":["# Model\n","HIDDEN_DIM = 128\n","attention_encoder = EncoderRNN(input_lang.n_words, HIDDEN_DIM)\n","attention_decoder = AttentionDecoderRNN(output_lang.n_words, HIDDEN_DIM)\n","print(\"N Params: \", sum(p.numel() for p in attention_encoder.parameters()) + sum(p.numel() for p in attention_decoder.parameters()))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["N Params:  605702\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g-JUGP1nDXKr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607031634494,"user_tz":360,"elapsed":902573,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"d5d10a7c-9e6c-41cf-b42c-3df9f780ba10"},"source":["attention_losses = train_model(attention_encoder, attention_decoder, output_lang, train, 200, 1e-3, len(train)//9)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["EPOCH 1/200, LOSS 3.104578018188477\n","EPOCH 2/200, LOSS 2.3449677785237633\n","EPOCH 3/200, LOSS 2.0311857541402185\n","EPOCH 4/200, LOSS 1.9337039470672606\n","EPOCH 5/200, LOSS 1.8807778199513754\n","EPOCH 6/200, LOSS 1.8463608741760253\n","EPOCH 7/200, LOSS 1.8161213556925455\n","EPOCH 8/200, LOSS 1.788509257634481\n","EPOCH 9/200, LOSS 1.7613321622212728\n","EPOCH 10/200, LOSS 1.7326075077056884\n","EPOCH 11/200, LOSS 1.7022112051645915\n","EPOCH 12/200, LOSS 1.6705374081929525\n","EPOCH 13/200, LOSS 1.6387920061747232\n","EPOCH 14/200, LOSS 1.6070402304331461\n","EPOCH 15/200, LOSS 1.5745100657145181\n","EPOCH 16/200, LOSS 1.5412081241607662\n","EPOCH 17/200, LOSS 1.5089199384053547\n","EPOCH 18/200, LOSS 1.4771472454071044\n","EPOCH 19/200, LOSS 1.4449333190917968\n","EPOCH 20/200, LOSS 1.4115772406260174\n","EPOCH 21/200, LOSS 1.3769299030303954\n","EPOCH 22/200, LOSS 1.3408918698628745\n","EPOCH 23/200, LOSS 1.3049866994222006\n","EPOCH 24/200, LOSS 1.2692285537719727\n","EPOCH 25/200, LOSS 1.2354562123616537\n","EPOCH 26/200, LOSS 1.2028584241867066\n","EPOCH 27/200, LOSS 1.170997969309489\n","EPOCH 28/200, LOSS 1.1405928134918213\n","EPOCH 29/200, LOSS 1.1113583326339722\n","EPOCH 30/200, LOSS 1.0831279118855794\n","EPOCH 31/200, LOSS 1.0560781160990398\n","EPOCH 32/200, LOSS 1.0296394983927408\n","EPOCH 33/200, LOSS 1.0043445746103923\n","EPOCH 34/200, LOSS 0.9805505673090618\n","EPOCH 35/200, LOSS 0.9579720815022786\n","EPOCH 36/200, LOSS 0.9363099575042724\n","EPOCH 37/200, LOSS 0.9156506776809692\n","EPOCH 38/200, LOSS 0.8953451712926229\n","EPOCH 39/200, LOSS 0.8765955607096354\n","EPOCH 40/200, LOSS 0.8589226722717284\n","EPOCH 41/200, LOSS 0.8421801090240479\n","EPOCH 42/200, LOSS 0.8266912539800008\n","EPOCH 43/200, LOSS 0.8117476304372152\n","EPOCH 44/200, LOSS 0.7966267108917237\n","EPOCH 45/200, LOSS 0.782835594813029\n","EPOCH 46/200, LOSS 0.769607170422872\n","EPOCH 47/200, LOSS 0.7568110942840577\n","EPOCH 48/200, LOSS 0.7449249267578124\n","EPOCH 49/200, LOSS 0.7348954677581787\n","EPOCH 50/200, LOSS 0.7234006722768148\n","EPOCH 51/200, LOSS 0.7123343785603842\n","EPOCH 52/200, LOSS 0.7013077020645142\n","EPOCH 53/200, LOSS 0.6917477687199911\n","EPOCH 54/200, LOSS 0.682193390528361\n","EPOCH 55/200, LOSS 0.6731327136357625\n","EPOCH 56/200, LOSS 0.6643525759379069\n","EPOCH 57/200, LOSS 0.6560391982396443\n","EPOCH 58/200, LOSS 0.6481552998224894\n","EPOCH 59/200, LOSS 0.6411879539489747\n","EPOCH 60/200, LOSS 0.6370291948318482\n","EPOCH 61/200, LOSS 0.634321649869283\n","EPOCH 62/200, LOSS 0.6214074929555257\n","EPOCH 63/200, LOSS 0.6134237289428711\n","EPOCH 64/200, LOSS 0.6061399141947428\n","EPOCH 65/200, LOSS 0.5993460496266683\n","EPOCH 66/200, LOSS 0.5933370272318523\n","EPOCH 67/200, LOSS 0.5872201919555664\n","EPOCH 68/200, LOSS 0.5814234773317972\n","EPOCH 69/200, LOSS 0.5759697914123535\n","EPOCH 70/200, LOSS 0.5705992937088014\n","EPOCH 71/200, LOSS 0.5653362115224203\n","EPOCH 72/200, LOSS 0.560222880045573\n","EPOCH 73/200, LOSS 0.5552744110425314\n","EPOCH 74/200, LOSS 0.5504895170529683\n","EPOCH 75/200, LOSS 0.545862340927124\n","EPOCH 76/200, LOSS 0.5413715044657389\n","EPOCH 77/200, LOSS 0.5369965036710103\n","EPOCH 78/200, LOSS 0.5327538927396138\n","EPOCH 79/200, LOSS 0.5286132256189982\n","EPOCH 80/200, LOSS 0.5245996157328288\n","EPOCH 81/200, LOSS 0.5206913908322652\n","EPOCH 82/200, LOSS 0.5169034520785014\n","EPOCH 83/200, LOSS 0.5132436712582906\n","EPOCH 84/200, LOSS 0.5097187121709187\n","EPOCH 85/200, LOSS 0.5063301841417949\n","EPOCH 86/200, LOSS 0.5031685471534728\n","EPOCH 87/200, LOSS 0.5004948417345683\n","EPOCH 88/200, LOSS 0.49911704858144124\n","EPOCH 89/200, LOSS 0.5022253632545471\n","EPOCH 90/200, LOSS 0.5016517082850138\n","EPOCH 91/200, LOSS 0.49129367669423424\n","EPOCH 92/200, LOSS 0.48574821949005126\n","EPOCH 93/200, LOSS 0.483111846446991\n","EPOCH 94/200, LOSS 0.47984124422073365\n","EPOCH 95/200, LOSS 0.4769318620363871\n","EPOCH 96/200, LOSS 0.47408901453018193\n","EPOCH 97/200, LOSS 0.47154792547225954\n","EPOCH 98/200, LOSS 0.468983260790507\n","EPOCH 99/200, LOSS 0.4664931138356527\n","EPOCH 100/200, LOSS 0.4641438285509745\n","EPOCH 101/200, LOSS 0.4618792812029521\n","EPOCH 102/200, LOSS 0.45964451233545944\n","EPOCH 103/200, LOSS 0.4574209968249003\n","EPOCH 104/200, LOSS 0.4552101731300354\n","EPOCH 105/200, LOSS 0.45305524667104086\n","EPOCH 106/200, LOSS 0.4510575215021769\n","EPOCH 107/200, LOSS 0.4494034846623738\n","EPOCH 108/200, LOSS 0.4485623121261597\n","EPOCH 109/200, LOSS 0.4490378936131796\n","EPOCH 110/200, LOSS 0.4501333793004354\n","EPOCH 111/200, LOSS 0.4443209012349446\n","EPOCH 112/200, LOSS 0.4419235269228617\n","EPOCH 113/200, LOSS 0.4392331878344218\n","EPOCH 114/200, LOSS 0.43769165277481076\n","EPOCH 115/200, LOSS 0.43596464792887374\n","EPOCH 116/200, LOSS 0.4341895302136739\n","EPOCH 117/200, LOSS 0.4328366319338481\n","EPOCH 118/200, LOSS 0.4315320014953613\n","EPOCH 119/200, LOSS 0.4300814270973205\n","EPOCH 120/200, LOSS 0.42859706083933513\n","EPOCH 121/200, LOSS 0.42733898957570393\n","EPOCH 122/200, LOSS 0.4260413805643717\n","EPOCH 123/200, LOSS 0.4246228893597921\n","EPOCH 124/200, LOSS 0.42263311545054116\n","EPOCH 125/200, LOSS 0.4210140506426493\n","EPOCH 126/200, LOSS 0.41952492396036795\n","EPOCH 127/200, LOSS 0.41779792308807373\n","EPOCH 128/200, LOSS 0.4154278953870138\n","EPOCH 129/200, LOSS 0.41404641469319653\n","EPOCH 130/200, LOSS 0.4127220034599304\n","EPOCH 131/200, LOSS 0.4112586895624797\n","EPOCH 132/200, LOSS 0.4094114343325297\n","EPOCH 133/200, LOSS 0.40821965535481763\n","EPOCH 134/200, LOSS 0.407267947991689\n","EPOCH 135/200, LOSS 0.4059908548990886\n","EPOCH 136/200, LOSS 0.4041156649589539\n","EPOCH 137/200, LOSS 0.4029521822929382\n","EPOCH 138/200, LOSS 0.402065893014272\n","EPOCH 139/200, LOSS 0.4006619135538737\n","EPOCH 140/200, LOSS 0.3987404028574626\n","EPOCH 141/200, LOSS 0.39792921940485637\n","EPOCH 142/200, LOSS 0.39719476302464807\n","EPOCH 143/200, LOSS 0.39602165222167973\n","EPOCH 144/200, LOSS 0.3940757393836975\n","EPOCH 145/200, LOSS 0.3935465733210246\n","EPOCH 146/200, LOSS 0.39292588631312053\n","EPOCH 147/200, LOSS 0.39158954222997033\n","EPOCH 148/200, LOSS 0.38978143533070886\n","EPOCH 149/200, LOSS 0.3894566059112549\n","EPOCH 150/200, LOSS 0.3886809587478638\n","EPOCH 151/200, LOSS 0.3870053092638652\n","EPOCH 152/200, LOSS 0.3859792669614156\n","EPOCH 153/200, LOSS 0.3857441107432047\n","EPOCH 154/200, LOSS 0.38531856139500936\n","EPOCH 155/200, LOSS 0.3834108432133992\n","EPOCH 156/200, LOSS 0.38294161160786955\n","EPOCH 157/200, LOSS 0.3832032362620036\n","EPOCH 158/200, LOSS 0.38218132257461546\n","EPOCH 159/200, LOSS 0.38107401132583624\n","EPOCH 160/200, LOSS 0.382116695245107\n","EPOCH 161/200, LOSS 0.38218736251195273\n","EPOCH 162/200, LOSS 0.3821359992027283\n","EPOCH 163/200, LOSS 0.38605018456776935\n","EPOCH 164/200, LOSS 0.3930925408999125\n","EPOCH 165/200, LOSS 0.40477215449015297\n","EPOCH 166/200, LOSS 0.3989786307017008\n","EPOCH 167/200, LOSS 0.3789208968480428\n","EPOCH 168/200, LOSS 0.3783860286076864\n","EPOCH 169/200, LOSS 0.3733404874801636\n","EPOCH 170/200, LOSS 0.37122649749120074\n","EPOCH 171/200, LOSS 0.37038638989130657\n","EPOCH 172/200, LOSS 0.36924087206522627\n","EPOCH 173/200, LOSS 0.36758917967478427\n","EPOCH 174/200, LOSS 0.3659200986226399\n","EPOCH 175/200, LOSS 0.36448411146799725\n","EPOCH 176/200, LOSS 0.3636321544647217\n","EPOCH 177/200, LOSS 0.36328795750935877\n","EPOCH 178/200, LOSS 0.3632786989212036\n","EPOCH 179/200, LOSS 0.36353405714035036\n","EPOCH 180/200, LOSS 0.3641859292984009\n","EPOCH 181/200, LOSS 0.36619174083073935\n","EPOCH 182/200, LOSS 0.37011826833089195\n","EPOCH 183/200, LOSS 0.3666561285654704\n","EPOCH 184/200, LOSS 0.3624434192975362\n","EPOCH 185/200, LOSS 0.35892995595932004\n","EPOCH 186/200, LOSS 0.3581649382909139\n","EPOCH 187/200, LOSS 0.35689597527186073\n","EPOCH 188/200, LOSS 0.35536399682362874\n","EPOCH 189/200, LOSS 0.35468200047810877\n","EPOCH 190/200, LOSS 0.3543053905169169\n","EPOCH 191/200, LOSS 0.35402138233184816\n","EPOCH 192/200, LOSS 0.35416971047719314\n","EPOCH 193/200, LOSS 0.35521443287531534\n","EPOCH 194/200, LOSS 0.3564482013384501\n","EPOCH 195/200, LOSS 0.35551069180170697\n","EPOCH 196/200, LOSS 0.3549297173817952\n","EPOCH 197/200, LOSS 0.35477979183197017\n","EPOCH 198/200, LOSS 0.35270750522613525\n","EPOCH 199/200, LOSS 0.3522701462109884\n","EPOCH 200/200, LOSS 0.3507433176040649\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AB7hW9IPS3gM"},"source":["# Base model + attention + deep encoder"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OFreqqb-S3gM","executionInfo":{"status":"ok","timestamp":1607034099086,"user_tz":360,"elapsed":652,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"20ddb31c-c5f8-4208-cb7c-5abbaa3ab792"},"source":["# Model\n","import copy\n","HIDDEN_DIM = 128\n","deep_encoder = EncoderRNN(input_lang.n_words, HIDDEN_DIM, 2)\n","deep_decoder = copy.deepcopy(attention_decoder) #AttentionDecoderRNN(output_lang.n_words, HIDDEN_DIM)\n","print(\"N Params: \", sum(p.numel() for p in deep_encoder.parameters()) + sum(p.numel() for p in deep_decoder.parameters()))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["N Params:  704774\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qwXlbKStS3gM","executionInfo":{"status":"ok","timestamp":1607034726830,"user_tz":360,"elapsed":624085,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"14c55531-f3c4-47e2-c7b0-b0f1856594ba"},"source":["deep_losses = train_model(deep_encoder, deep_decoder, output_lang, train, 200, 1e-3, len(train)//9)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["EPOCH 1/200, LOSS 2.2321849981943767\n","EPOCH 2/200, LOSS 1.6978626251220703\n","EPOCH 3/200, LOSS 1.5402049859364828\n","EPOCH 4/200, LOSS 1.4340380191802977\n","EPOCH 5/200, LOSS 1.3484118620554608\n","EPOCH 6/200, LOSS 1.2738835016886394\n","EPOCH 7/200, LOSS 1.2032069524129232\n","EPOCH 8/200, LOSS 1.1326571226119995\n","EPOCH 9/200, LOSS 1.0625780661900839\n","EPOCH 10/200, LOSS 0.9982682466506957\n","EPOCH 11/200, LOSS 0.9398912986119587\n","EPOCH 12/200, LOSS 0.8880402247111003\n","EPOCH 13/200, LOSS 0.8419578234354654\n","EPOCH 14/200, LOSS 0.8008792002995808\n","EPOCH 15/200, LOSS 0.7642747561136881\n","EPOCH 16/200, LOSS 0.7317228078842163\n","EPOCH 17/200, LOSS 0.7042086362838745\n","EPOCH 18/200, LOSS 0.6804332415262858\n","EPOCH 19/200, LOSS 0.6571684281031291\n","EPOCH 20/200, LOSS 0.6374407688776652\n","EPOCH 21/200, LOSS 0.6199136336644491\n","EPOCH 22/200, LOSS 0.6050577918688456\n","EPOCH 23/200, LOSS 0.5918320894241332\n","EPOCH 24/200, LOSS 0.5782084226608277\n","EPOCH 25/200, LOSS 0.5664737741152445\n","EPOCH 26/200, LOSS 0.5571903387705486\n","EPOCH 27/200, LOSS 0.5475370804468791\n","EPOCH 28/200, LOSS 0.5384677410125732\n","EPOCH 29/200, LOSS 0.530154303709666\n","EPOCH 30/200, LOSS 0.5231746157010396\n","EPOCH 31/200, LOSS 0.5164288600285848\n","EPOCH 32/200, LOSS 0.5101222435633341\n","EPOCH 33/200, LOSS 0.5049182494481405\n","EPOCH 34/200, LOSS 0.4996095975240072\n","EPOCH 35/200, LOSS 0.4941792527834574\n","EPOCH 36/200, LOSS 0.4884645541508993\n","EPOCH 37/200, LOSS 0.4838064034779867\n","EPOCH 38/200, LOSS 0.47938324610392247\n","EPOCH 39/200, LOSS 0.47517793575922657\n","EPOCH 40/200, LOSS 0.47127457459767663\n","EPOCH 41/200, LOSS 0.4674641013145447\n","EPOCH 42/200, LOSS 0.4637503107388814\n","EPOCH 43/200, LOSS 0.46510733763376866\n","EPOCH 44/200, LOSS 0.46043807665507\n","EPOCH 45/200, LOSS 0.45455565849939983\n","EPOCH 46/200, LOSS 0.4522289673487345\n","EPOCH 47/200, LOSS 0.44880937735239657\n","EPOCH 48/200, LOSS 0.44607401291529336\n","EPOCH 49/200, LOSS 0.4432432333628337\n","EPOCH 50/200, LOSS 0.44067700703938806\n","EPOCH 51/200, LOSS 0.4382622202237447\n","EPOCH 52/200, LOSS 0.4359839677810669\n","EPOCH 53/200, LOSS 0.43377708196640014\n","EPOCH 54/200, LOSS 0.43237187067667643\n","EPOCH 55/200, LOSS 0.4313490470250448\n","EPOCH 56/200, LOSS 0.42963666915893556\n","EPOCH 57/200, LOSS 0.4274511933326721\n","EPOCH 58/200, LOSS 0.42615495125452674\n","EPOCH 59/200, LOSS 0.422764519850413\n","EPOCH 60/200, LOSS 0.4207399924596151\n","EPOCH 61/200, LOSS 0.4183834592501322\n","EPOCH 62/200, LOSS 0.41612063646316527\n","EPOCH 63/200, LOSS 0.414182984828949\n","EPOCH 64/200, LOSS 0.4123093366622925\n","EPOCH 65/200, LOSS 0.4105317155520121\n","EPOCH 66/200, LOSS 0.40883124669392906\n","EPOCH 67/200, LOSS 0.4071868578592936\n","EPOCH 68/200, LOSS 0.4058274030685425\n","EPOCH 69/200, LOSS 0.4049997727076213\n","EPOCH 70/200, LOSS 0.4042978843053182\n","EPOCH 71/200, LOSS 0.40266465346018476\n","EPOCH 72/200, LOSS 0.4034811655680339\n","EPOCH 73/200, LOSS 0.405074143409729\n","EPOCH 74/200, LOSS 0.41402544975280764\n","EPOCH 75/200, LOSS 0.4022094011306763\n","EPOCH 76/200, LOSS 0.39730490843455\n","EPOCH 77/200, LOSS 0.3939284046490987\n","EPOCH 78/200, LOSS 0.3920183817545573\n","EPOCH 79/200, LOSS 0.39077526330947876\n","EPOCH 80/200, LOSS 0.3890185117721558\n","EPOCH 81/200, LOSS 0.38779364029566443\n","EPOCH 82/200, LOSS 0.3865225394566854\n","EPOCH 83/200, LOSS 0.3851108034451803\n","EPOCH 84/200, LOSS 0.3838300426801046\n","EPOCH 85/200, LOSS 0.38263914187749226\n","EPOCH 86/200, LOSS 0.38146987358729045\n","EPOCH 87/200, LOSS 0.38027595678965254\n","EPOCH 88/200, LOSS 0.37905744711558026\n","EPOCH 89/200, LOSS 0.37781916856765746\n","EPOCH 90/200, LOSS 0.3765705784161885\n","EPOCH 91/200, LOSS 0.3753187656402588\n","EPOCH 92/200, LOSS 0.3740719238917033\n","EPOCH 93/200, LOSS 0.37283988793691003\n","EPOCH 94/200, LOSS 0.3716343998908997\n","EPOCH 95/200, LOSS 0.37046222686767577\n","EPOCH 96/200, LOSS 0.36931949853897095\n","EPOCH 97/200, LOSS 0.368198823928833\n","EPOCH 98/200, LOSS 0.36709551016489667\n","EPOCH 99/200, LOSS 0.36600625117619834\n","EPOCH 100/200, LOSS 0.3649286945660909\n","EPOCH 101/200, LOSS 0.36386132637659707\n","EPOCH 102/200, LOSS 0.3628031452496847\n","EPOCH 103/200, LOSS 0.36175344785054525\n","EPOCH 104/200, LOSS 0.3607117374738057\n","EPOCH 105/200, LOSS 0.3596773306528727\n","EPOCH 106/200, LOSS 0.3586508433024088\n","EPOCH 107/200, LOSS 0.35762452681859336\n","EPOCH 108/200, LOSS 0.35693054199218754\n","EPOCH 109/200, LOSS 0.36069478193918864\n","EPOCH 110/200, LOSS 0.35730847915013625\n","EPOCH 111/200, LOSS 0.3560641050338744\n","EPOCH 112/200, LOSS 0.3542363921801249\n","EPOCH 113/200, LOSS 0.3533115029335021\n","EPOCH 114/200, LOSS 0.35300484100977575\n","EPOCH 115/200, LOSS 0.3523979584376017\n","EPOCH 116/200, LOSS 0.3534313639005025\n","EPOCH 117/200, LOSS 0.35776435931523637\n","EPOCH 118/200, LOSS 0.36501485904057823\n","EPOCH 119/200, LOSS 0.3547386328379313\n","EPOCH 120/200, LOSS 0.3521381855010986\n","EPOCH 121/200, LOSS 0.3482067584991455\n","EPOCH 122/200, LOSS 0.3468886931737264\n","EPOCH 123/200, LOSS 0.3456311265627543\n","EPOCH 124/200, LOSS 0.3447352925936381\n","EPOCH 125/200, LOSS 0.34356488386789963\n","EPOCH 126/200, LOSS 0.3426676313082377\n","EPOCH 127/200, LOSS 0.3418075362841288\n","EPOCH 128/200, LOSS 0.3409247756004333\n","EPOCH 129/200, LOSS 0.3399986465771993\n","EPOCH 130/200, LOSS 0.3390714287757873\n","EPOCH 131/200, LOSS 0.33816736141840614\n","EPOCH 132/200, LOSS 0.3373098572095235\n","EPOCH 133/200, LOSS 0.3365231434504191\n","EPOCH 134/200, LOSS 0.3358159065246582\n","EPOCH 135/200, LOSS 0.3352090716362\n","EPOCH 136/200, LOSS 0.33475787639617927\n","EPOCH 137/200, LOSS 0.33461633125940954\n","EPOCH 138/200, LOSS 0.3351535161336263\n","EPOCH 139/200, LOSS 0.3370958646138509\n","EPOCH 140/200, LOSS 0.3404423276583354\n","EPOCH 141/200, LOSS 0.33474768797556564\n","EPOCH 142/200, LOSS 0.3334538380304972\n","EPOCH 143/200, LOSS 0.3318032622337342\n","EPOCH 144/200, LOSS 0.3314084490140279\n","EPOCH 145/200, LOSS 0.33082308371861774\n","EPOCH 146/200, LOSS 0.3301690657933553\n","EPOCH 147/200, LOSS 0.32976272503534954\n","EPOCH 148/200, LOSS 0.32935791810353593\n","EPOCH 149/200, LOSS 0.32882743676503495\n","EPOCH 150/200, LOSS 0.3281979918479919\n","EPOCH 151/200, LOSS 0.3276064356168111\n","EPOCH 152/200, LOSS 0.32692993879318233\n","EPOCH 153/200, LOSS 0.3261391401290894\n","EPOCH 154/200, LOSS 0.32530901432037357\n","EPOCH 155/200, LOSS 0.32443511088689164\n","EPOCH 156/200, LOSS 0.3238390326499939\n","EPOCH 157/200, LOSS 0.3233474850654602\n","EPOCH 158/200, LOSS 0.32291991313298546\n","EPOCH 159/200, LOSS 0.3226724704106649\n","EPOCH 160/200, LOSS 0.321463398138682\n","EPOCH 161/200, LOSS 0.3208069086074829\n","EPOCH 162/200, LOSS 0.3205729365348816\n","EPOCH 163/200, LOSS 0.3204652229944865\n","EPOCH 164/200, LOSS 0.3193555434544881\n","EPOCH 165/200, LOSS 0.319010063012441\n","EPOCH 166/200, LOSS 0.31873462597529095\n","EPOCH 167/200, LOSS 0.31794650157292687\n","EPOCH 168/200, LOSS 0.3168316602706909\n","EPOCH 169/200, LOSS 0.3163041392962138\n","EPOCH 170/200, LOSS 0.31578109661738074\n","EPOCH 171/200, LOSS 0.3147247711817423\n","EPOCH 172/200, LOSS 0.31375668048858646\n","EPOCH 173/200, LOSS 0.313491427898407\n","EPOCH 174/200, LOSS 0.31359489361445114\n","EPOCH 175/200, LOSS 0.3133879939715067\n","EPOCH 176/200, LOSS 0.3143318295478821\n","EPOCH 177/200, LOSS 0.3178086241086324\n","EPOCH 178/200, LOSS 0.3205309430758158\n","EPOCH 179/200, LOSS 0.31634676456451416\n","EPOCH 180/200, LOSS 0.312914796670278\n","EPOCH 181/200, LOSS 0.31233562231063844\n","EPOCH 182/200, LOSS 0.3128781000773112\n","EPOCH 183/200, LOSS 0.3148370544115702\n","EPOCH 184/200, LOSS 0.3229970256487529\n","EPOCH 185/200, LOSS 0.3335017045338948\n","EPOCH 186/200, LOSS 0.33663924137751267\n","EPOCH 187/200, LOSS 0.31476562023162846\n","EPOCH 188/200, LOSS 0.3091899355252584\n","EPOCH 189/200, LOSS 0.3076143344243368\n","EPOCH 190/200, LOSS 0.3040089070796967\n","EPOCH 191/200, LOSS 0.30241208871205644\n","EPOCH 192/200, LOSS 0.3017774264017741\n","EPOCH 193/200, LOSS 0.3013006428877513\n","EPOCH 194/200, LOSS 0.30070221424102783\n","EPOCH 195/200, LOSS 0.30001851518948874\n","EPOCH 196/200, LOSS 0.29923801024754837\n","EPOCH 197/200, LOSS 0.2982631226380666\n","EPOCH 198/200, LOSS 0.29703701933224996\n","EPOCH 199/200, LOSS 0.29579551219940187\n","EPOCH 200/200, LOSS 0.29484637777010597\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r_2HbO1bUJ1H"},"source":["# Model comparisons"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"BlZrm8JKS3gN","executionInfo":{"status":"ok","timestamp":1607034727955,"user_tz":360,"elapsed":1107,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"070f7d3e-05d0-42db-d38d-e2b188bd8ca1"},"source":["import matplotlib.pyplot as plt\n","plt.plot(losses, '-', label = 'Base')\n","plt.plot(attention_losses, '-', label = 'Attention')\n","plt.plot(deep_losses, '-', label = 'Deep Attention')\n","plt.ylim((0, 5))\n","plt.xlabel('Epoch')\n","plt.ylabel('CE Loss')\n","plt.legend(loc='best')\n","plt.show()"],"execution_count":20,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7T3MMe-S3gN","executionInfo":{"status":"ok","timestamp":1607033529349,"user_tz":360,"elapsed":2797421,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"b03898c6-84d1-4dd7-e7a7-89089789e84e"},"source":["for e, d in zip(\n","        [encoder, attention_encoder, deep_encoder],\n","        [decoder, attention_decoder, deep_decoder],\n","    ):\n","    total_bleu1 = 0\n","    total_bleu2 = 0\n","    total_bleu3 = 0\n","    total_bleu4 = 0\n","    for ind in range(test.shape[0]):\n","        bleu1, bleu2, bleu3, bleu4, loss, r, t, c = translate_sentence(e,\n","                                                                      d, \n","                                                                      test.iloc[ind], \n","                                                                      input_lang, \n","                                                                      output_lang)\n","        total_bleu1 += bleu1 * 100 / test.shape[0]\n","        total_bleu2 += bleu2 * 100 / test.shape[0]\n","        total_bleu3 += bleu3 * 100 / test.shape[0]\n","        total_bleu4 += bleu4 * 100 / test.shape[0]\n","        if ind < 10: \n","            print(r, \"|\", t, \"|\", c)\n","    print(\"{:.3f}, {:.3f}, {:.3f}, {:.3f}\".format(total_bleu1, total_bleu2, total_bleu3, total_bleu4))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["<start> tom is working at a restaurant <end> | <start> tom esta trabalhando em um restaurante <end> | <start> tom esta trabalhando no onibus <end>\n","<start> i saw toms name on the <unk> <end> | <start> eu vi o nome de tom na <unk> <end> | <start> eu vi o nome do tom <unk> <end>\n","<start> tom only <unk> to <unk> <unk> <end> | <start> tom so <unk> <unk> <unk> <end> | <start> tom so <unk> <unk> para <unk> <end>\n","<start> thats <unk> <end> | <start> isso e <unk> <end> | <start> isso e <unk> <end>\n","<start> i was in a lot of pain <end> | <start> eu estava com muita dor <end> | <start> eu estava muito <unk> <end>\n","<start> dont let the children fight <end> | <start> nao deixe as criancas <unk> <end> | <start> nao deixe criancas <unk> <end>\n","<start> smoking is not allowed on this bus <end> | <start> e <unk> fumar neste onibus <end> | <start> e <unk> me <unk> hoje <end>\n","<start> tom doesnt need to do that right away <end> | <start> tom nao precisa fazer isso imediatamente <end> | <start> tom nao precisa fazer isso de novo <end>\n","<start> tom loves himself more than he loves anyone else <end> | <start> tom se ama mais do que qualquer outra pessoa <end> | <start> tom nao quer mais nada que eu <unk> <end>\n","<start> that was the <unk> movie ive ever seen <end> | <start> esse foi o pior filme que eu ja vi <end> | <start> isso foi o melhor que eu disse a <unk> <end>\n","64.503, 48.221, 36.353, 29.310\n","<start> tom is working at a restaurant <end> | <start> tom esta trabalhando em um restaurante <end> | <start> tom esta trabalhando em um restaurante <end>\n","<start> i saw toms name on the <unk> <end> | <start> eu vi o nome de tom na <unk> <end> | <start> eu vi o nome do tom na <unk> <end>\n","<start> tom only <unk> to <unk> <unk> <end> | <start> tom so <unk> <unk> <unk> <end> | <start> tom so <unk> a <unk> <unk> <end>\n","<start> thats <unk> <end> | <start> isso e <unk> <end> | <start> isso e <unk> <end>\n","<start> i was in a lot of pain <end> | <start> eu estava com muita dor <end> | <start> eu estava com muita dor <end>\n","<start> dont let the children fight <end> | <start> nao deixe as criancas <unk> <end> | <start> nao deixe as criancas <unk> <end>\n","<start> smoking is not allowed on this bus <end> | <start> e <unk> fumar neste onibus <end> | <start> fumar nao <unk> ir disso <end>\n","<start> tom doesnt need to do that right away <end> | <start> tom nao precisa fazer isso imediatamente <end> | <start> tom nao precisa fazer isso sem certo <end>\n","<start> tom loves himself more than he loves anyone else <end> | <start> tom se ama mais do que qualquer outra pessoa <end> | <start> tom ficou com mais do que ele precisava mais ninguem <end>\n","<start> that was the <unk> movie ive ever seen <end> | <start> esse foi o pior filme que eu ja vi <end> | <start> esse foi o filme <unk> que eu ja vi <end>\n","73.920, 61.357, 50.156, 42.165\n","<start> tom is working at a restaurant <end> | <start> tom esta trabalhando em um restaurante <end> | <start> tom esta trabalhando em um restaurante <end>\n","<start> i saw toms name on the <unk> <end> | <start> eu vi o nome de tom na <unk> <end> | <start> eu vi o nome de tom no <unk> <end>\n","<start> tom only <unk> to <unk> <unk> <end> | <start> tom so <unk> <unk> <unk> <end> | <start> tom so <unk> a <unk> <unk> <end>\n","<start> thats <unk> <end> | <start> isso e <unk> <end> | <start> isso e <unk> <end>\n","<start> i was in a lot of pain <end> | <start> eu estava com muita dor <end> | <start> eu estava muito <unk> <end>\n","<start> dont let the children fight <end> | <start> nao deixe as criancas <unk> <end> | <start> nao deixe a criancas <unk> <end>\n","<start> smoking is not allowed on this bus <end> | <start> e <unk> fumar neste onibus <end> | <start> <unk> nao <unk> no onibus <end>\n","<start> tom doesnt need to do that right away <end> | <start> tom nao precisa fazer isso imediatamente <end> | <start> tom nao precisa fazer isso aqui <end>\n","<start> tom loves himself more than he loves anyone else <end> | <start> tom se ama mais do que qualquer outra pessoa <end> | <start> tom <unk> se <unk> mais do que ele ama alguem <end>\n","<start> that was the <unk> movie ive ever seen <end> | <start> esse foi o pior filme que eu ja vi <end> | <start> esse foi o <unk> que ja vi <end>\n","73.443, 60.881, 49.697, 41.820\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QEFxWKH3sqef","executionInfo":{"status":"ok","timestamp":1607035204134,"user_tz":360,"elapsed":477282,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"d7ec606f-ac6b-4875-8c72-81a8bd7d4520"},"source":["for e, d in zip(\n","        [deep_encoder],\n","        [deep_decoder],\n","    ):\n","    total_bleu1 = 0\n","    total_bleu2 = 0\n","    total_bleu3 = 0\n","    total_bleu4 = 0\n","    for ind in range(test.shape[0]):\n","        bleu1, bleu2, bleu3, bleu4, loss, r, t, c = translate_sentence(e,\n","                                                                      d, \n","                                                                      test.iloc[ind], \n","                                                                      input_lang, \n","                                                                      output_lang)\n","        total_bleu1 += bleu1 * 100 / test.shape[0]\n","        total_bleu2 += bleu2 * 100 / test.shape[0]\n","        total_bleu3 += bleu3 * 100 / test.shape[0]\n","        total_bleu4 += bleu4 * 100 / test.shape[0]\n","        if ind < 10: \n","            print(r, \"|\", t, \"|\", c)\n","    print(\"{:.3f}, {:.3f}, {:.3f}, {:.3f}\".format(total_bleu1, total_bleu2, total_bleu3, total_bleu4))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["<start> tom is working at a restaurant <end> | <start> tom esta trabalhando em um restaurante <end> | <start> tom esta trabalhando num restaurante <end>\n","<start> i saw toms name on the <unk> <end> | <start> eu vi o nome de tom na <unk> <end> | <start> eu vi o nome do tom na <unk> <end>\n","<start> tom only <unk> to <unk> <unk> <end> | <start> tom so <unk> <unk> <unk> <end> | <start> tom so <unk> a <unk> <end>\n","<start> thats <unk> <end> | <start> isso e <unk> <end> | <start> isso e <unk> <end>\n","<start> i was in a lot of pain <end> | <start> eu estava com muita dor <end> | <start> eu estava com dor de dor <end>\n","<start> dont let the children fight <end> | <start> nao deixe as criancas <unk> <end> | <start> nao deixe que as criancas <unk> <end>\n","<start> smoking is not allowed on this bus <end> | <start> e <unk> fumar neste onibus <end> | <start> fumar nao e assim que onibus <end>\n","<start> tom doesnt need to do that right away <end> | <start> tom nao precisa fazer isso imediatamente <end> | <start> tom nao precisa fazer isso sem jeito <end>\n","<start> tom loves himself more than he loves anyone else <end> | <start> tom se ama mais do que qualquer outra pessoa <end> | <start> tom nao se pode mais do que ninguem mais <end>\n","<start> that was the <unk> movie ive ever seen <end> | <start> esse foi o pior filme que eu ja vi <end> | <start> esse foi o filme mais <unk> que eu deveria ter visto\n","74.541, 62.273, 51.303, 43.417\n"],"name":"stdout"}]}]}