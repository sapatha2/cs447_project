{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cs447project_english_italian.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyP8b8X9X1PGc56O7qJNwA9g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bnqyBxYPLyG6","executionInfo":{"status":"ok","timestamp":1607030764490,"user_tz":360,"elapsed":7366,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"ff4ad9ca-fc25-4cdb-eb69-115c22f52be0"},"source":["# PyTorch \n","!pip install --upgrade torch\n","!pip install --upgrade torchtext"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n","Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n","Collecting torchtext\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/23/8499af6d9c22b29b01f66a2c11d38ce71cd1cafa2655913c29818ed4a00f/torchtext-0.8.0-cp36-cp36m-manylinux1_x86_64.whl (6.9MB)\n","\u001b[K     |████████████████████████████████| 6.9MB 5.3MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.7.0+cu101)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n","Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n","Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.8)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.11.8)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n","Installing collected packages: torchtext\n","  Found existing installation: torchtext 0.3.1\n","    Uninstalling torchtext-0.3.1:\n","      Successfully uninstalled torchtext-0.3.1\n","Successfully installed torchtext-0.8.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bnDeYuA_LnXJ","executionInfo":{"status":"ok","timestamp":1607030768320,"user_tz":360,"elapsed":11188,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["from collections import defaultdict\n","import nltk\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction, corpus_bleu\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import re\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm.notebook import tqdm\n","import unicodedata\n","\n","if __name__ == '__main__':\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zQ4KX9-QTZcI"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"tUeHdn_uTfZ1","executionInfo":{"status":"ok","timestamp":1607030768322,"user_tz":360,"elapsed":11187,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Converts the unicode file to ascii\n","def unicode_to_ascii(s):\n","    \"\"\"Normalizes latin chars with accent to their canonical decomposition\"\"\"\n","    return ''.join(c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn')\n","\n","# Preprocessing the sentence to add the start, end tokens and make them lower-case\n","def preprocess_sentence(w):\n","    w = unicode_to_ascii(w.lower().strip())\n","    w = re.sub(r'([?.!,¿])', r' \\1 ', w)\n","    w = re.sub(r'[\" \"]+', ' ', w)\n","    w = re.sub(r'[^\\w\\s]', '', w) \n","\n","    w = re.sub(r'[^a-zA-Z?.!,¿]+', ' ', w)\n","    \n","    w = w.rstrip().strip()\n","    w = '<start> ' + w + ' <end>'\n","    return w\n","\n","def max_length(tensor):\n","    return max(len(t) for t in tensor)\n","\n","def pad_sequences(x, max_len):\n","    padded = np.zeros((max_len), dtype=np.int64)\n","    if len(x) > max_len:\n","        padded[:] = x[:max_len]\n","    else:\n","        padded[:len(x)] = x\n","    return padded\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2indexFull = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n","        self.word2countFull = {\"<start>\": 1e10, \"<end>\": 1e10, \"<unk>\": 1e10, \"<pad>\": 1e10}\n","        self.index2wordFull = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n","        self.word2index = {}\n","        self.index2word = {}\n","        self.n_wordsFull = 4  # Count SOS and EOS\n","        self.n_words = 0\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2indexFull:\n","            self.word2indexFull[word] = self.n_wordsFull\n","            self.word2countFull[word] = 1\n","            self.index2wordFull[self.n_wordsFull] = word\n","            self.n_wordsFull += 1\n","        else:\n","            self.word2countFull[word] += 1\n","\n","    def reduceDictionary(self, threshold = 50):\n","        n_words = 0\n","        for word in self.word2indexFull.keys():\n","            if self.word2countFull[word] >= threshold:\n","                self.word2index[word] = n_words\n","                self.index2word[n_words] = word\n","                n_words += 1\n","        self.n_words = n_words\n","    \n","    def sentence2Index(self, sentence):\n","        output = []\n","        for word in sentence.split(' '):\n","            if word in self.word2index.keys():\n","                output.append(self.word2index[word])\n","            else:\n","                output.append(self.word2index[\"<unk>\"])\n","        return output\n","\n","def build_dataset(target_language, threshold):\n","    # Load in and process sentences\n","    lines = open(target_language+'.txt', encoding='UTF-8').read().strip().split('\\n')\n","    original_word_pairs = [[w for w in l.split('\\t')][:2] for l in lines]\n","\n","    data = pd.DataFrame(original_word_pairs, columns=['eng', target_language])\n","    data['eng'] = data.eng.apply(lambda w: preprocess_sentence(w))\n","    data[target_language] = data[target_language].apply(lambda w: preprocess_sentence(w))\n","\n","    # Remove all sentences with length longer than 10 (+ 2 for start/end)\n","    data['len_eng'] = data.eng.apply(lambda w: len(w.split(\" \")))\n","    data['len_'+target_language] = data[target_language].apply(lambda w: len(w.split(\" \")))\n","    data = data[(data['len_eng'] <= MAX_LEN + 2)*(data['len_'+target_language] <= MAX_LEN + 2)]\n","    data = data[['eng',target_language]]\n","\n","    # Build language dictionaries \n","    input_lang = Lang('eng')\n","    output_lang = Lang(target_language)\n","    for sentence in data['eng']:\n","      input_lang.addSentence(sentence)\n","\n","    for sentence in data[target_language]:\n","      output_lang.addSentence(sentence)\n","    input_lang.reduceDictionary(threshold)\n","    output_lang.reduceDictionary(threshold)\n","\n","    data['eng'] = data.eng.apply(lambda w: input_lang.sentence2Index(w))\n","    data[target_language] = data[target_language].apply(lambda w: output_lang.sentence2Index(w))\n","    data['eng'] = data.eng.apply(lambda w: pad_sequences(w, MAX_LEN + 2))\n","    data[target_language] = data[target_language].apply(lambda w: pad_sequences(w, MAX_LEN + 2))\n","\n","    # Filter out sentences that have more than 1 (10%) UNK\n","    eng_filter = data['eng'].apply(lambda w: np.sum(w == input_lang.word2index['<unk>']))\n","    target_filter = data[target_language].apply(lambda w: np.sum(w == output_lang.word2index['<unk>']))\n","    data = data[(eng_filter <= MAX_UNK) * (target_filter <= MAX_UNK)]\n","\n","    return input_lang, output_lang, data"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bJBt9wUfT3Jo","executionInfo":{"status":"ok","timestamp":1607030770878,"user_tz":360,"elapsed":13740,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"a9724295-aa2b-434f-c0a2-caf6c258e598"},"source":["!wget http://www.manythings.org/anki/ita-eng.zip\n","!unzip -o ita-eng.zip"],"execution_count":4,"outputs":[{"output_type":"stream","text":["--2020-12-03 21:26:07--  http://www.manythings.org/anki/ita-eng.zip\n","Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 172.67.173.198, ...\n","Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 7521114 (7.2M) [application/zip]\n","Saving to: ‘ita-eng.zip’\n","\n","ita-eng.zip         100%[===================>]   7.17M  5.06MB/s    in 1.4s    \n","\n","2020-12-03 21:26:09 (5.06 MB/s) - ‘ita-eng.zip’ saved [7521114/7521114]\n","\n","Archive:  ita-eng.zip\n","  inflating: ita.txt                 \n","  inflating: _about.txt              \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zkGty2WyUFNT","executionInfo":{"status":"ok","timestamp":1607030792658,"user_tz":360,"elapsed":35518,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"855af3ce-e0c5-4987-e64e-0eab6362d2c2"},"source":["MAX_LEN = 10 # (+2 for <start>, <end>)\n","MAX_UNK = 1000 \n","THRESHOLD = 100\n","input_lang, output_lang, data = build_dataset('ita', THRESHOLD) #\n","print(\"Input words {}, Output words {}, N sentences {}\".format(input_lang.n_words, output_lang.n_words, data.shape[0]))\n","print(data)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/computation/expressions.py:204: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n","  f\"evaluating in Python space because the {repr(op_str)} \"\n"],"name":"stderr"},{"output_type":"stream","text":["Input words 1294, Output words 1562, N sentences 334008\n","                                                      eng                                                ita\n","0                    [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]               [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","1                    [1, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]               [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","2                    [1, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]               [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","3                    [1, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]               [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","4                    [1, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]               [1, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","...                                                   ...                                                ...\n","342420  [1, 450, 3, 31, 3, 1230, 3, 990, 450, 1213, 3, 2]        [1, 3, 670, 3, 45, 863, 9, 3, 3, 692, 3, 2]\n","342423               [1, 3, 3, 3, 3, 3, 3, 3, 2, 0, 0, 0]              [1, 3, 3, 3, 3, 45, 3, 3, 3, 2, 0, 0]\n","342483           [1, 3, 3, 215, 3, 3, 3, 596, 3, 2, 0, 0]    [1, 14, 3, 3, 134, 1290, 824, 225, 3, 65, 3, 2]\n","342739  [1, 595, 381, 240, 213, 3, 215, 411, 1034, 106...  [1, 588, 496, 101, 313, 38, 3, 65, 225, 1331, ...\n","342968           [1, 3, 3, 3, 596, 3, 326, 3, 3, 3, 2, 0]           [1, 3, 3, 3, 65, 3, 21, 3, 3, 156, 3, 2]\n","\n","[334008 rows x 2 columns]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/computation/expressions.py:204: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n","  f\"evaluating in Python space because the {repr(op_str)} \"\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"MRYEK7LNLUgE"},"source":["# Models"]},{"cell_type":"code","metadata":{"id":"-fFrQ7qnLSNd","executionInfo":{"status":"ok","timestamp":1607030792660,"user_tz":360,"elapsed":35518,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Encoder (Takes a sentence seq_len -> returns output, hidden)\n","class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers=1):\n","        super(EncoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, num_layers = num_layers)\n","\n","    def forward(self, input_sentence):\n","        embedded = self.embedding(input_sentence)\n","        output, hidden = self.gru(embedded)  \n","\n","        # For deep\n","        hidden = hidden[-1].unsqueeze(0)         \n","        return output, hidden"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"RqjwRA2jRAcr","executionInfo":{"status":"ok","timestamp":1607030792661,"user_tz":360,"elapsed":35516,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Decoder (Takes a sentence seq_len -> returns output)\n","class DecoderRNN(nn.Module):\n","    def __init__(self, output_size, hidden_size):\n","        super(DecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim = 2)\n","\n","    def forward(self, input_sentence, hidden, encoder_output):\n","        embedded = self.embedding(input_sentence)              \n","        output, decoder_hidden = self.gru(embedded, hidden)                    \n","        output = self.out(output).squeeze(0)                              \n","        return output, decoder_hidden"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jex7T-VWurHx","executionInfo":{"status":"ok","timestamp":1607030792662,"user_tz":360,"elapsed":35515,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Decoder with attention (Takes a sentence seq_len -> returns output)\n","class AttentionDecoderRNN(nn.Module):\n","    def __init__(self, output_size, hidden_size):\n","        super(AttentionDecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","\n","        self.score = nn.Sequential(\n","            nn.Linear(2 * hidden_size, hidden_size),\n","            nn.Tanh(),\n","            nn.Linear(hidden_size, 1)\n","        )\n","\n","        self.attention = nn.Sequential(\n","            nn.Linear(2 * hidden_size, hidden_size),\n","            nn.Tanh()\n","        )\n","\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim = 2)\n","\n","    def forward(self, input_sentence, hidden, encoder_output):\n","        embedded = self.embedding(input_sentence) # [1, batch_size, ]\n","\n","        # Compute score vector\n","        score_vector = self.score(\n","            torch.cat([torch.cat((MAX_LEN + 2)*[hidden]), encoder_output], dim = 2)\n","        ).squeeze(-1) # [seq_len, batch_size] \n","\n","        # Compute attention weights\n","        attention_weights = F.softmax(score_vector, dim = 0) # [seq_len, batch_size]\n","\n","        # Compute context vector\n","        context_vector = torch.einsum('sb, sbh -> bh', attention_weights, encoder_output) # [batch_size, hidden_size]\n","\n","        # Compute attention vector\n","        attention_vector = self.attention(torch.cat([context_vector.unsqueeze(0), embedded], dim = 2))\n","\n","        # Pass into decoder\n","        output, decoder_hidden = self.gru(attention_vector, hidden)                    \n","        output = self.out(output).squeeze(0)                              \n","        return output, decoder_hidden"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zOxsduH8c-Yt"},"source":["#  Training"]},{"cell_type":"code","metadata":{"id":"nWq4Mfj6dB7C","executionInfo":{"status":"ok","timestamp":1607030792663,"user_tz":360,"elapsed":35514,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["def translate_sentence(encoder, decoder, pair, ref_lang, targ_lang):\n","    \"\"\"\n","    Translate single sentence, returns\n","\n","    reference\n","    target\n","    candidate\n","    \"\"\"\n","    test_loss = 0\n","    candidate = []\n","    with torch.no_grad():\n","        reference = torch.tensor(pair[0]).unsqueeze(1).to(device)\n","        target =    torch.tensor(pair[1]).unsqueeze(1).to(device)\n","\n","        # Encoder pass\n","        encoder_output, encoder_hidden = encoder(reference) \n","  \n","        # Decoder pass\n","        decoder_input = target[0].unsqueeze(0)\n","        decoder_hidden = encoder_hidden\n","        candidate.append(decoder_input)\n","        for j in range(1, len(target)):\n","            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output) \n","            test_loss += loss_function(target[j], decoder_output) / len(target)\n","            decoder_input = F.log_softmax(decoder_output.unsqueeze(0), dim=-1).argmax(dim = -1)\n","            candidate.append(decoder_input)\n","            if decoder_input == targ_lang.word2index['<end>']:\n","                break\n","    \n","    reference = reference[reference > 0]\n","    target = target[target > 0]\n","\n","    reference = [ref_lang.index2word[int(s)] for s in reference]\n","    target =    [targ_lang.index2word[int(s)] for s in target]\n","    candidate = [targ_lang.index2word[int(s)] for s in candidate]\n","\n","    smoother = SmoothingFunction()\n","    bleu1 = sentence_bleu([target[1:]], candidate[1:], weights=(1,), smoothing_function=smoother.method1)\n","    bleu2 = sentence_bleu([target[1:]], candidate[1:], weights=(1/2, 1/2), smoothing_function=smoother.method1)\n","    bleu3 = sentence_bleu([target[1:]], candidate[1:], weights=(1/3, 1/3, 1/3), smoothing_function=smoother.method1)\n","    bleu4 = sentence_bleu([target[1:]], candidate[1:], weights=(1/4, 1/4, 1/4, 1/4), smoothing_function=smoother.method1)\n","    return bleu1, bleu2, bleu3, bleu4, test_loss, \" \".join(reference), \" \".join(target), \" \".join(candidate)\n","\n","def loss_function(real, pred):\n","    \"\"\" Only consider non-pad inputs in the loss; mask needed \"\"\"\n","    mask = real.ge(1).float()\n","    \n","    loss_ = F.cross_entropy(pred, real) * mask \n","    return torch.mean(loss_)\n","\n","def train_model(encoder, decoder, targ_lang, train, num_epochs, learning_rate, batch_size, breakp = 1e10):\n","    # Return training losses\n","    losses = []\n","    \n","    # Model, optimizer, criterion\n","    encoder = encoder.to(device)\n","    decoder = decoder.to(device)\n","    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n","\n","    # Build batches\n","    batches = [df for g, df in train.groupby(np.arange(len(train)) // batch_size)]\n","\n","    # Train\n","    for i in range(num_epochs):\n","        epoch_loss = 0\n","        for b in range(len(batches)):\n","            batch = batches[b]\n","            if len(batch) == batch_size: # Discard partial batches\n","                target = torch.tensor([s for s in batch[list(batch)[1]]]).T.to(device)\n","                reference = torch.tensor([s for s in batch[list(batch)[0]]]).T.to(device)\n","\n","                # Encoder pass: [max_len, batch_size, hidden_size], [1, batch_size, hidden_size]\n","                encoder_output, encoder_hidden = encoder(reference) \n","\n","                # Decoder pass: teacher forcing\n","                loss = 0\n","                decoder_input = target[0].unsqueeze(0) # [1, batch_size]\n","                decoder_hidden = encoder_hidden\n","                for j in range(1, len(target)):\n","                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output) # [batch_size, output_size], [1, batch_size, hidden_size]\n","                    loss += loss_function(target[j], decoder_output)\n","                    decoder_input = target[j].unsqueeze(0)\n","\n","                # Step\n","                loss.backward()\n","                encoder_optimizer.step()\n","                decoder_optimizer.step()\n","                encoder_optimizer.zero_grad()\n","                decoder_optimizer.zero_grad()\n","\n","                # Prints\n","                epoch_loss += loss.item() / (len(target) * len(batches))\n","\n","        # Training losses\n","        print(\"EPOCH {}/{}, LOSS {}\".format(i + 1, num_epochs, epoch_loss))\n","        losses.append(epoch_loss)\n","    return losses "],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9R9Gj0aKS7lw"},"source":["# Base model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XeFcWwFpqpLW","executionInfo":{"status":"ok","timestamp":1607030792902,"user_tz":360,"elapsed":35751,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"cfb0f3b9-3c13-4626-b67e-77535eb67de0"},"source":["# Train test split\n","data = data.sample(frac = 1, replace = False)\n","train = data.iloc[:data.shape[0]//4 * 3]\n","test = data.iloc[data.shape[0]//4 * 3:]\n","\n","# Model\n","HIDDEN_DIM = 128\n","encoder = EncoderRNN(input_lang.n_words, HIDDEN_DIM)\n","decoder = DecoderRNN(output_lang.n_words, HIDDEN_DIM)\n","print(\"N Params: \", sum(p.numel() for p in encoder.parameters()) + sum(p.numel() for p in decoder.parameters()))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["N Params:  765210\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KLv65YtkgxlA","executionInfo":{"status":"ok","timestamp":1607031267777,"user_tz":360,"elapsed":510623,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"c4a64f76-b00f-4d27-9937-c1240cdd04ff"},"source":["losses = train_model(encoder, decoder, output_lang, train, 200, 1e-3, len(train)//9)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["EPOCH 1/200, LOSS 3.5341563048186124\n","EPOCH 2/200, LOSS 2.831196184511538\n","EPOCH 3/200, LOSS 2.3171232364795826\n","EPOCH 4/200, LOSS 2.1531163144994667\n","EPOCH 5/200, LOSS 2.0996774390891746\n","EPOCH 6/200, LOSS 2.055738272490325\n","EPOCH 7/200, LOSS 2.0224209184999817\n","EPOCH 8/200, LOSS 1.9905086799904155\n","EPOCH 9/200, LOSS 1.9596715679875127\n","EPOCH 10/200, LOSS 1.9296002211394132\n","EPOCH 11/200, LOSS 1.8999793617813676\n","EPOCH 12/200, LOSS 1.8707238656503185\n","EPOCH 13/200, LOSS 1.8422642637182163\n","EPOCH 14/200, LOSS 1.814834471102114\n","EPOCH 15/200, LOSS 1.7886596785651312\n","EPOCH 16/200, LOSS 1.7633232187341759\n","EPOCH 17/200, LOSS 1.7390289659853335\n","EPOCH 18/200, LOSS 1.715617956938567\n","EPOCH 19/200, LOSS 1.692524203547725\n","EPOCH 20/200, LOSS 1.6695998862937647\n","EPOCH 21/200, LOSS 1.646677370424624\n","EPOCH 22/200, LOSS 1.6233970324198403\n","EPOCH 23/200, LOSS 1.5999349311546043\n","EPOCH 24/200, LOSS 1.5769514154504847\n","EPOCH 25/200, LOSS 1.555000481782136\n","EPOCH 26/200, LOSS 1.5339855088127985\n","EPOCH 27/200, LOSS 1.5136710802714028\n","EPOCH 28/200, LOSS 1.4940636952718098\n","EPOCH 29/200, LOSS 1.474257204267714\n","EPOCH 30/200, LOSS 1.4541672070821128\n","EPOCH 31/200, LOSS 1.43440255412349\n","EPOCH 32/200, LOSS 1.4145129168475115\n","EPOCH 33/200, LOSS 1.3940362577085144\n","EPOCH 34/200, LOSS 1.3745738488656505\n","EPOCH 35/200, LOSS 1.3562004124676739\n","EPOCH 36/200, LOSS 1.338148523260046\n","EPOCH 37/200, LOSS 1.3210998464513708\n","EPOCH 38/200, LOSS 1.304285755863896\n","EPOCH 39/200, LOSS 1.2883425641942907\n","EPOCH 40/200, LOSS 1.2732047858061613\n","EPOCH 41/200, LOSS 1.2582736103623002\n","EPOCH 42/200, LOSS 1.2438656753963893\n","EPOCH 43/200, LOSS 1.2302041142075149\n","EPOCH 44/200, LOSS 1.2171902921464708\n","EPOCH 45/200, LOSS 1.204630869406241\n","EPOCH 46/200, LOSS 1.1919541712160464\n","EPOCH 47/200, LOSS 1.1801287244867398\n","EPOCH 48/200, LOSS 1.1684983129854556\n","EPOCH 49/200, LOSS 1.1571872764163549\n","EPOCH 50/200, LOSS 1.1468416584862602\n","EPOCH 51/200, LOSS 1.135900055920636\n","EPOCH 52/200, LOSS 1.1247622348644115\n","EPOCH 53/200, LOSS 1.114138197015833\n","EPOCH 54/200, LOSS 1.1038045000146937\n","EPOCH 55/200, LOSS 1.0937194559309218\n","EPOCH 56/200, LOSS 1.0839913332903826\n","EPOCH 57/200, LOSS 1.074083372398659\n","EPOCH 58/200, LOSS 1.064603849693581\n","EPOCH 59/200, LOSS 1.0552921030256484\n","EPOCH 60/200, LOSS 1.0460769953551114\n","EPOCH 61/200, LOSS 1.036710226977313\n","EPOCH 62/200, LOSS 1.027648095731382\n","EPOCH 63/200, LOSS 1.018915017445882\n","EPOCH 64/200, LOSS 1.0103633845293964\n","EPOCH 65/200, LOSS 1.001999961005317\n","EPOCH 66/200, LOSS 0.9938298331366645\n","EPOCH 67/200, LOSS 0.9858256004474781\n","EPOCH 68/200, LOSS 0.9780281649695501\n","EPOCH 69/200, LOSS 0.9704511695437961\n","EPOCH 70/200, LOSS 0.9631724887424045\n","EPOCH 71/200, LOSS 0.9549515335648149\n","EPOCH 72/200, LOSS 0.9476131156638817\n","EPOCH 73/200, LOSS 0.9401499606944898\n","EPOCH 74/200, LOSS 0.9329855530350297\n","EPOCH 75/200, LOSS 0.926033170134933\n","EPOCH 76/200, LOSS 0.919269738373933\n","EPOCH 77/200, LOSS 0.9125715891520182\n","EPOCH 78/200, LOSS 0.9056703337916622\n","EPOCH 79/200, LOSS 0.8988879433384648\n","EPOCH 80/200, LOSS 0.8922069425936099\n","EPOCH 81/200, LOSS 0.8858146049358226\n","EPOCH 82/200, LOSS 0.8794301615820991\n","EPOCH 83/200, LOSS 0.8731272750430636\n","EPOCH 84/200, LOSS 0.8668852558842411\n","EPOCH 85/200, LOSS 0.8607061792303015\n","EPOCH 86/200, LOSS 0.8548694275043628\n","EPOCH 87/200, LOSS 0.8500457693029332\n","EPOCH 88/200, LOSS 0.8435254980016639\n","EPOCH 89/200, LOSS 0.8374663017414233\n","EPOCH 90/200, LOSS 0.8317356816044559\n","EPOCH 91/200, LOSS 0.8261107956921612\n","EPOCH 92/200, LOSS 0.8205859219586409\n","EPOCH 93/200, LOSS 0.8151811846980342\n","EPOCH 94/200, LOSS 0.8098455446737785\n","EPOCH 95/200, LOSS 0.8045805825127494\n","EPOCH 96/200, LOSS 0.7993813708976463\n","EPOCH 97/200, LOSS 0.7942925294240316\n","EPOCH 98/200, LOSS 0.7906563811832005\n","EPOCH 99/200, LOSS 0.784703219378436\n","EPOCH 100/200, LOSS 0.7798556839978253\n","EPOCH 101/200, LOSS 0.774754974577162\n","EPOCH 102/200, LOSS 0.7700122992197672\n","EPOCH 103/200, LOSS 0.7653103139665391\n","EPOCH 104/200, LOSS 0.7606753331643565\n","EPOCH 105/200, LOSS 0.7560842302110462\n","EPOCH 106/200, LOSS 0.751582004405834\n","EPOCH 107/200, LOSS 0.7471703864909984\n","EPOCH 108/200, LOSS 0.7428250754321063\n","EPOCH 109/200, LOSS 0.7386129520557545\n","EPOCH 110/200, LOSS 0.7348232887409352\n","EPOCH 111/200, LOSS 0.7306514934257223\n","EPOCH 112/200, LOSS 0.7257762838293005\n","EPOCH 113/200, LOSS 0.7215205033620198\n","EPOCH 114/200, LOSS 0.7172053478382252\n","EPOCH 115/200, LOSS 0.7131456798977321\n","EPOCH 116/200, LOSS 0.7090332419783981\n","EPOCH 117/200, LOSS 0.7050009656835484\n","EPOCH 118/200, LOSS 0.701049460305108\n","EPOCH 119/200, LOSS 0.6971407996283637\n","EPOCH 120/200, LOSS 0.6932597955067953\n","EPOCH 121/200, LOSS 0.6894113575970685\n","EPOCH 122/200, LOSS 0.6856051904183849\n","EPOCH 123/200, LOSS 0.6818453294259531\n","EPOCH 124/200, LOSS 0.6781384768309417\n","EPOCH 125/200, LOSS 0.6744829372123435\n","EPOCH 126/200, LOSS 0.6708752667462383\n","EPOCH 127/200, LOSS 0.6674798506277579\n","EPOCH 128/200, LOSS 0.6653428695819996\n","EPOCH 129/200, LOSS 0.6613855670999597\n","EPOCH 130/200, LOSS 0.6574356732545076\n","EPOCH 131/200, LOSS 0.6541635857688055\n","EPOCH 132/200, LOSS 0.6507804570374667\n","EPOCH 133/200, LOSS 0.6474884880913628\n","EPOCH 134/200, LOSS 0.6442980766296387\n","EPOCH 135/200, LOSS 0.6410262761292633\n","EPOCH 136/200, LOSS 0.637874201492027\n","EPOCH 137/200, LOSS 0.6347338358561198\n","EPOCH 138/200, LOSS 0.6316191929358024\n","EPOCH 139/200, LOSS 0.6285498495455141\n","EPOCH 140/200, LOSS 0.6255248608412566\n","EPOCH 141/200, LOSS 0.6225368623380307\n","EPOCH 142/200, LOSS 0.6195986712420429\n","EPOCH 143/200, LOSS 0.6167097224129571\n","EPOCH 144/200, LOSS 0.6138720556541725\n","EPOCH 145/200, LOSS 0.611077692773607\n","EPOCH 146/200, LOSS 0.608330421977573\n","EPOCH 147/200, LOSS 0.6056181369004425\n","EPOCH 148/200, LOSS 0.6029811788488317\n","EPOCH 149/200, LOSS 0.6003046874646787\n","EPOCH 150/200, LOSS 0.5984794254656192\n","EPOCH 151/200, LOSS 0.5960271755854288\n","EPOCH 152/200, LOSS 0.593570762210422\n","EPOCH 153/200, LOSS 0.5909573678617124\n","EPOCH 154/200, LOSS 0.5879774049476341\n","EPOCH 155/200, LOSS 0.5856745817043163\n","EPOCH 156/200, LOSS 0.5832683863463226\n","EPOCH 157/200, LOSS 0.5808490823816369\n","EPOCH 158/200, LOSS 0.5784541147726553\n","EPOCH 159/200, LOSS 0.57622523661013\n","EPOCH 160/200, LOSS 0.5739810201856825\n","EPOCH 161/200, LOSS 0.5718076935520878\n","EPOCH 162/200, LOSS 0.5698573059505887\n","EPOCH 163/200, LOSS 0.5682906530521534\n","EPOCH 164/200, LOSS 0.5671102250063861\n","EPOCH 165/200, LOSS 0.5676496779477155\n","EPOCH 166/200, LOSS 0.5630212669019345\n","EPOCH 167/200, LOSS 0.5595816506279839\n","EPOCH 168/200, LOSS 0.5571448317280522\n","EPOCH 169/200, LOSS 0.5547389498463383\n","EPOCH 170/200, LOSS 0.5528809008774934\n","EPOCH 171/200, LOSS 0.5506579522733335\n","EPOCH 172/200, LOSS 0.5486127579653706\n","EPOCH 173/200, LOSS 0.546746986883658\n","EPOCH 174/200, LOSS 0.5448634977693911\n","EPOCH 175/200, LOSS 0.542930316042017\n","EPOCH 176/200, LOSS 0.5410503502245303\n","EPOCH 177/200, LOSS 0.5392160327346237\n","EPOCH 178/200, LOSS 0.5374224759914257\n","EPOCH 179/200, LOSS 0.5356465887140345\n","EPOCH 180/200, LOSS 0.5338775802541663\n","EPOCH 181/200, LOSS 0.5321209033330282\n","EPOCH 182/200, LOSS 0.5303729507658216\n","EPOCH 183/200, LOSS 0.528636614481608\n","EPOCH 184/200, LOSS 0.5269170513859501\n","EPOCH 185/200, LOSS 0.5252161202607332\n","EPOCH 186/200, LOSS 0.5235348851592452\n","EPOCH 187/200, LOSS 0.5218760658193518\n","EPOCH 188/200, LOSS 0.5202363023051509\n","EPOCH 189/200, LOSS 0.518616588027389\n","EPOCH 190/200, LOSS 0.5170121016325774\n","EPOCH 191/200, LOSS 0.5154284459573251\n","EPOCH 192/200, LOSS 0.5138549848839089\n","EPOCH 193/200, LOSS 0.5122995244132148\n","EPOCH 194/200, LOSS 0.5107521966651634\n","EPOCH 195/200, LOSS 0.5092241675765427\n","EPOCH 196/200, LOSS 0.5077077856770268\n","EPOCH 197/200, LOSS 0.5062015189064873\n","EPOCH 198/200, LOSS 0.5047100561636465\n","EPOCH 199/200, LOSS 0.5032340906284474\n","EPOCH 200/200, LOSS 0.5017701387405396\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NkNIfoBzDXKq"},"source":["# Base model + attention"]},{"cell_type":"code","metadata":{"id":"H6oXqCrmDXKq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607031267780,"user_tz":360,"elapsed":510624,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"264e20e8-f123-4e31-babe-c94b28e44076"},"source":["# Model\n","HIDDEN_DIM = 128\n","attention_encoder = EncoderRNN(input_lang.n_words, HIDDEN_DIM)\n","attention_decoder = AttentionDecoderRNN(output_lang.n_words, HIDDEN_DIM)\n","print(\"N Params: \", sum(p.numel() for p in attention_encoder.parameters()) + sum(p.numel() for p in attention_decoder.parameters()))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["N Params:  831131\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g-JUGP1nDXKr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607031990154,"user_tz":360,"elapsed":1232996,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"f1d2a7c9-3d20-4f40-8dce-3c2af7a0af05"},"source":["attention_losses = train_model(attention_encoder, attention_decoder, output_lang, train, 200, 1e-3, len(train)//9)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["EPOCH 1/200, LOSS 3.5239083325421374\n","EPOCH 2/200, LOSS 2.7236636832908347\n","EPOCH 3/200, LOSS 2.353002636520951\n","EPOCH 4/200, LOSS 2.2079506450229225\n","EPOCH 5/200, LOSS 2.134017644105134\n","EPOCH 6/200, LOSS 2.088229461952492\n","EPOCH 7/200, LOSS 2.054693275027805\n","EPOCH 8/200, LOSS 2.0250569272924355\n","EPOCH 9/200, LOSS 1.9949215076587816\n","EPOCH 10/200, LOSS 1.9628923204210071\n","EPOCH 11/200, LOSS 1.9294827072708693\n","EPOCH 12/200, LOSS 1.8958590472186054\n","EPOCH 13/200, LOSS 1.8625972006056044\n","EPOCH 14/200, LOSS 1.8291681430957933\n","EPOCH 15/200, LOSS 1.795118667461254\n","EPOCH 16/200, LOSS 1.7613195489954065\n","EPOCH 17/200, LOSS 1.7277746553774231\n","EPOCH 18/200, LOSS 1.6959525744120278\n","EPOCH 19/200, LOSS 1.6645339330037432\n","EPOCH 20/200, LOSS 1.6353577507866754\n","EPOCH 21/200, LOSS 1.6041141969186288\n","EPOCH 22/200, LOSS 1.5731503345348214\n","EPOCH 23/200, LOSS 1.541598461292408\n","EPOCH 24/200, LOSS 1.5088033852753817\n","EPOCH 25/200, LOSS 1.4764575958251953\n","EPOCH 26/200, LOSS 1.4432355562845867\n","EPOCH 27/200, LOSS 1.4106269589176885\n","EPOCH 28/200, LOSS 1.3791775703430176\n","EPOCH 29/200, LOSS 1.3482742927692555\n","EPOCH 30/200, LOSS 1.317744837866889\n","EPOCH 31/200, LOSS 1.2895758858433475\n","EPOCH 32/200, LOSS 1.259582766780147\n","EPOCH 33/200, LOSS 1.2308163996096009\n","EPOCH 34/200, LOSS 1.202990249351219\n","EPOCH 35/200, LOSS 1.1752576651396573\n","EPOCH 36/200, LOSS 1.148528673030712\n","EPOCH 37/200, LOSS 1.1221847710786041\n","EPOCH 38/200, LOSS 1.0963885607542814\n","EPOCH 39/200, LOSS 1.070763111114502\n","EPOCH 40/200, LOSS 1.046365155114068\n","EPOCH 41/200, LOSS 1.0230838457743328\n","EPOCH 42/200, LOSS 1.0005820592244465\n","EPOCH 43/200, LOSS 0.9780123851917408\n","EPOCH 44/200, LOSS 0.9567284142529523\n","EPOCH 45/200, LOSS 0.9368714933042173\n","EPOCH 46/200, LOSS 0.9176931469528764\n","EPOCH 47/200, LOSS 0.8987715950718632\n","EPOCH 48/200, LOSS 0.8813142953095613\n","EPOCH 49/200, LOSS 0.8636965045222531\n","EPOCH 50/200, LOSS 0.8474090629153781\n","EPOCH 51/200, LOSS 0.8312717190495242\n","EPOCH 52/200, LOSS 0.8161980046166314\n","EPOCH 53/200, LOSS 0.8018314661803069\n","EPOCH 54/200, LOSS 0.7883111900753444\n","EPOCH 55/200, LOSS 0.7765129142337376\n","EPOCH 56/200, LOSS 0.7620915130332664\n","EPOCH 57/200, LOSS 0.7490597124452945\n","EPOCH 58/200, LOSS 0.736838146492287\n","EPOCH 59/200, LOSS 0.7253597930625634\n","EPOCH 60/200, LOSS 0.7140681655318648\n","EPOCH 61/200, LOSS 0.7032175770512334\n","EPOCH 62/200, LOSS 0.6933274534013535\n","EPOCH 63/200, LOSS 0.6826272982138173\n","EPOCH 64/200, LOSS 0.6770096178408023\n","EPOCH 65/200, LOSS 0.6657137120211566\n","EPOCH 66/200, LOSS 0.6548334006909972\n","EPOCH 67/200, LOSS 0.6460440247147171\n","EPOCH 68/200, LOSS 0.6375965365657099\n","EPOCH 69/200, LOSS 0.6292844966605857\n","EPOCH 70/200, LOSS 0.6213293384622645\n","EPOCH 71/200, LOSS 0.6135838075920388\n","EPOCH 72/200, LOSS 0.6061415274937949\n","EPOCH 73/200, LOSS 0.5989752389766551\n","EPOCH 74/200, LOSS 0.5920452409320407\n","EPOCH 75/200, LOSS 0.5853724656281649\n","EPOCH 76/200, LOSS 0.5790300369262696\n","EPOCH 77/200, LOSS 0.574618834036368\n","EPOCH 78/200, LOSS 0.5694178916789867\n","EPOCH 79/200, LOSS 0.5605249360755638\n","EPOCH 80/200, LOSS 0.5551950401730007\n","EPOCH 81/200, LOSS 0.5490951273176405\n","EPOCH 82/200, LOSS 0.5435603592130873\n","EPOCH 83/200, LOSS 0.5383034238108882\n","EPOCH 84/200, LOSS 0.5332132136380231\n","EPOCH 85/200, LOSS 0.5282386011547512\n","EPOCH 86/200, LOSS 0.523447765244378\n","EPOCH 87/200, LOSS 0.5188347763485379\n","EPOCH 88/200, LOSS 0.5143819208498354\n","EPOCH 89/200, LOSS 0.5102637962058738\n","EPOCH 90/200, LOSS 0.5060758855607774\n","EPOCH 91/200, LOSS 0.5063371349264074\n","EPOCH 92/200, LOSS 0.501845262668751\n","EPOCH 93/200, LOSS 0.4958075858928539\n","EPOCH 94/200, LOSS 0.4919826631192808\n","EPOCH 95/200, LOSS 0.48879780151225904\n","EPOCH 96/200, LOSS 0.4860009749730428\n","EPOCH 97/200, LOSS 0.48291018715611206\n","EPOCH 98/200, LOSS 0.47828929071073184\n","EPOCH 99/200, LOSS 0.4739667662867793\n","EPOCH 100/200, LOSS 0.4701714824747156\n","EPOCH 101/200, LOSS 0.4669502443737454\n","EPOCH 102/200, LOSS 0.4638439505188553\n","EPOCH 103/200, LOSS 0.46091027613039365\n","EPOCH 104/200, LOSS 0.4580289037139328\n","EPOCH 105/200, LOSS 0.4551698278497767\n","EPOCH 106/200, LOSS 0.4524190690782335\n","EPOCH 107/200, LOSS 0.449800067477756\n","EPOCH 108/200, LOSS 0.44728367416946974\n","EPOCH 109/200, LOSS 0.44487201725995107\n","EPOCH 110/200, LOSS 0.4426926639344957\n","EPOCH 111/200, LOSS 0.441212049237004\n","EPOCH 112/200, LOSS 0.44048490347685637\n","EPOCH 113/200, LOSS 0.43868260030393247\n","EPOCH 114/200, LOSS 0.4350252107337669\n","EPOCH 115/200, LOSS 0.43056034158777307\n","EPOCH 116/200, LOSS 0.4283873681668882\n","EPOCH 117/200, LOSS 0.42575188036318173\n","EPOCH 118/200, LOSS 0.4233874524081195\n","EPOCH 119/200, LOSS 0.42135950371071146\n","EPOCH 120/200, LOSS 0.41943251203607634\n","EPOCH 121/200, LOSS 0.4175530672073365\n","EPOCH 122/200, LOSS 0.4156129227744209\n","EPOCH 123/200, LOSS 0.4136694934633043\n","EPOCH 124/200, LOSS 0.4118631106835824\n","EPOCH 125/200, LOSS 0.4101795576236866\n","EPOCH 126/200, LOSS 0.4087784908435963\n","EPOCH 127/200, LOSS 0.40806129243638783\n","EPOCH 128/200, LOSS 0.4067877531051636\n","EPOCH 129/200, LOSS 0.40625622978916875\n","EPOCH 130/200, LOSS 0.40251762778670697\n","EPOCH 131/200, LOSS 0.39965796912157975\n","EPOCH 132/200, LOSS 0.39785144064161515\n","EPOCH 133/200, LOSS 0.395912523622866\n","EPOCH 134/200, LOSS 0.3941065028861717\n","EPOCH 135/200, LOSS 0.3925446801715427\n","EPOCH 136/200, LOSS 0.3910518354839749\n","EPOCH 137/200, LOSS 0.3896141096397683\n","EPOCH 138/200, LOSS 0.38827836513519287\n","EPOCH 139/200, LOSS 0.3869525812290333\n","EPOCH 140/200, LOSS 0.38579162844905146\n","EPOCH 141/200, LOSS 0.38487228640803584\n","EPOCH 142/200, LOSS 0.3841528274394848\n","EPOCH 143/200, LOSS 0.3839991622500949\n","EPOCH 144/200, LOSS 0.38205706190179894\n","EPOCH 145/200, LOSS 0.3796968592537774\n","EPOCH 146/200, LOSS 0.3772267059043602\n","EPOCH 147/200, LOSS 0.37560147267800786\n","EPOCH 148/200, LOSS 0.37421023404156717\n","EPOCH 149/200, LOSS 0.37270683270913585\n","EPOCH 150/200, LOSS 0.37138642205132383\n","EPOCH 151/200, LOSS 0.3701948236536097\n","EPOCH 152/200, LOSS 0.36909505173012064\n","EPOCH 153/200, LOSS 0.36821511056688094\n","EPOCH 154/200, LOSS 0.367566810713874\n","EPOCH 155/200, LOSS 0.36696550581190324\n","EPOCH 156/200, LOSS 0.3663413789537218\n","EPOCH 157/200, LOSS 0.36591214603847927\n","EPOCH 158/200, LOSS 0.36371009438126173\n","EPOCH 159/200, LOSS 0.361762872448674\n","EPOCH 160/200, LOSS 0.359896668681392\n","EPOCH 161/200, LOSS 0.3585551491490117\n","EPOCH 162/200, LOSS 0.3574444188012017\n","EPOCH 163/200, LOSS 0.3562444845835368\n","EPOCH 164/200, LOSS 0.35513395733303493\n","EPOCH 165/200, LOSS 0.35414673663951723\n","EPOCH 166/200, LOSS 0.35329027970631915\n","EPOCH 167/200, LOSS 0.35250353813171387\n","EPOCH 168/200, LOSS 0.3519126044379341\n","EPOCH 169/200, LOSS 0.35173503999356864\n","EPOCH 170/200, LOSS 0.35188563664754235\n","EPOCH 171/200, LOSS 0.3517982032563951\n","EPOCH 172/200, LOSS 0.3503594663408067\n","EPOCH 173/200, LOSS 0.3481248219807942\n","EPOCH 174/200, LOSS 0.34613927205403644\n","EPOCH 175/200, LOSS 0.34485013396651654\n","EPOCH 176/200, LOSS 0.34369492530822754\n","EPOCH 177/200, LOSS 0.3426981502109104\n","EPOCH 178/200, LOSS 0.3416806282820525\n","EPOCH 179/200, LOSS 0.3407454093297323\n","EPOCH 180/200, LOSS 0.33989843174263284\n","EPOCH 181/200, LOSS 0.33920603328280974\n","EPOCH 182/200, LOSS 0.3388917843500773\n","EPOCH 183/200, LOSS 0.33883244461483425\n","EPOCH 184/200, LOSS 0.3390280493983516\n","EPOCH 185/200, LOSS 0.3390116603286178\n","EPOCH 186/200, LOSS 0.3369975156254239\n","EPOCH 187/200, LOSS 0.3352554263891997\n","EPOCH 188/200, LOSS 0.3337502545780606\n","EPOCH 189/200, LOSS 0.33277328146828544\n","EPOCH 190/200, LOSS 0.33200684079417475\n","EPOCH 191/200, LOSS 0.33114750738497134\n","EPOCH 192/200, LOSS 0.33046883123892323\n","EPOCH 193/200, LOSS 0.32977259600604025\n","EPOCH 194/200, LOSS 0.3294000294473436\n","EPOCH 195/200, LOSS 0.3291041762740523\n","EPOCH 196/200, LOSS 0.328822116057078\n","EPOCH 197/200, LOSS 0.32880012635831474\n","EPOCH 198/200, LOSS 0.3275314635700649\n","EPOCH 199/200, LOSS 0.326830111168049\n","EPOCH 200/200, LOSS 0.32528568417937664\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AB7hW9IPS3gM"},"source":["# Base model + attention + deep encoder"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OFreqqb-S3gM","executionInfo":{"status":"ok","timestamp":1607034257141,"user_tz":360,"elapsed":1277,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"7080bb50-a4af-4430-cc34-6c3692801ce6"},"source":["# Model\n","import copy\n","HIDDEN_DIM = 128\n","deep_encoder = EncoderRNN(input_lang.n_words, HIDDEN_DIM, 2)\n","deep_decoder = copy.deepcopy(attention_decoder) #AttentionDecoderRNN(output_lang.n_words, HIDDEN_DIM)\n","print(\"N Params: \", sum(p.numel() for p in deep_encoder.parameters()) + sum(p.numel() for p in deep_decoder.parameters()))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["N Params:  930203\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qwXlbKStS3gM","executionInfo":{"status":"ok","timestamp":1607035037368,"user_tz":360,"elapsed":780587,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"d8b15155-b5f2-4ec0-c11a-a898ccd4560f"},"source":["deep_losses = train_model(deep_encoder, deep_decoder, output_lang, train, 200, 1e-3, len(train)//9)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["EPOCH 1/200, LOSS 2.2898401860837585\n","EPOCH 2/200, LOSS 1.8302043632224754\n","EPOCH 3/200, LOSS 1.6566055968955713\n","EPOCH 4/200, LOSS 1.5311514713146068\n","EPOCH 5/200, LOSS 1.4279169683103208\n","EPOCH 6/200, LOSS 1.3342545297410753\n","EPOCH 7/200, LOSS 1.2468385608107955\n","EPOCH 8/200, LOSS 1.1660723333005554\n","EPOCH 9/200, LOSS 1.0921678631393998\n","EPOCH 10/200, LOSS 1.0251803751345034\n","EPOCH 11/200, LOSS 0.9652078504915591\n","EPOCH 12/200, LOSS 0.9126148047270598\n","EPOCH 13/200, LOSS 0.8663982108787254\n","EPOCH 14/200, LOSS 0.8257484877551043\n","EPOCH 15/200, LOSS 0.7879760353653519\n","EPOCH 16/200, LOSS 0.7559416117491545\n","EPOCH 17/200, LOSS 0.7280279883631954\n","EPOCH 18/200, LOSS 0.7021603495986373\n","EPOCH 19/200, LOSS 0.6788783559092768\n","EPOCH 20/200, LOSS 0.6575847599241469\n","EPOCH 21/200, LOSS 0.6375270154741075\n","EPOCH 22/200, LOSS 0.6200023801238448\n","EPOCH 23/200, LOSS 0.6231642917350486\n","EPOCH 24/200, LOSS 0.6012079230061285\n","EPOCH 25/200, LOSS 0.5822458134757147\n","EPOCH 26/200, LOSS 0.5688601555647673\n","EPOCH 27/200, LOSS 0.5572816839924565\n","EPOCH 28/200, LOSS 0.5468371135217173\n","EPOCH 29/200, LOSS 0.5373875697453817\n","EPOCH 30/200, LOSS 0.528577036327786\n","EPOCH 31/200, LOSS 0.5204470334229645\n","EPOCH 32/200, LOSS 0.5127574117095383\n","EPOCH 33/200, LOSS 0.5055367240199337\n","EPOCH 34/200, LOSS 0.498806463347541\n","EPOCH 35/200, LOSS 0.49242234671557394\n","EPOCH 36/200, LOSS 0.4872704656035811\n","EPOCH 37/200, LOSS 0.4807720625842059\n","EPOCH 38/200, LOSS 0.4845036224082664\n","EPOCH 39/200, LOSS 0.4753743410110474\n","EPOCH 40/200, LOSS 0.46791532746067743\n","EPOCH 41/200, LOSS 0.4624709773946692\n","EPOCH 42/200, LOSS 0.45769904277942797\n","EPOCH 43/200, LOSS 0.45352267777478256\n","EPOCH 44/200, LOSS 0.4494493228417856\n","EPOCH 45/200, LOSS 0.44556523693932426\n","EPOCH 46/200, LOSS 0.4419602906262433\n","EPOCH 47/200, LOSS 0.438804339479517\n","EPOCH 48/200, LOSS 0.4355513254801432\n","EPOCH 49/200, LOSS 0.4323995289979157\n","EPOCH 50/200, LOSS 0.4302561238959984\n","EPOCH 51/200, LOSS 0.4275710052914089\n","EPOCH 52/200, LOSS 0.4300471720872102\n","EPOCH 53/200, LOSS 0.4231632153193156\n","EPOCH 54/200, LOSS 0.41771345668368864\n","EPOCH 55/200, LOSS 0.41469728505169906\n","EPOCH 56/200, LOSS 0.4119585443426062\n","EPOCH 57/200, LOSS 0.4092693946979664\n","EPOCH 58/200, LOSS 0.40690329781285045\n","EPOCH 59/200, LOSS 0.4044914996182477\n","EPOCH 60/200, LOSS 0.40216000433321347\n","EPOCH 61/200, LOSS 0.40030735951882823\n","EPOCH 62/200, LOSS 0.4055971172120836\n","EPOCH 63/200, LOSS 0.39798983379646585\n","EPOCH 64/200, LOSS 0.3940049233259978\n","EPOCH 65/200, LOSS 0.39188619896217636\n","EPOCH 66/200, LOSS 0.38985373797240086\n","EPOCH 67/200, LOSS 0.3877087654890838\n","EPOCH 68/200, LOSS 0.38577992827804\n","EPOCH 69/200, LOSS 0.3839287802025124\n","EPOCH 70/200, LOSS 0.3821551755622581\n","EPOCH 71/200, LOSS 0.38044343612812187\n","EPOCH 72/200, LOSS 0.37871693681787566\n","EPOCH 73/200, LOSS 0.4137387761363277\n","EPOCH 74/200, LOSS 0.4025214557294492\n","EPOCH 75/200, LOSS 0.38629136703632494\n","EPOCH 76/200, LOSS 0.37991019973048457\n","EPOCH 77/200, LOSS 0.3751167412157412\n","EPOCH 78/200, LOSS 0.3721940914789835\n","EPOCH 79/200, LOSS 0.37007536270000313\n","EPOCH 80/200, LOSS 0.3682978859654179\n","EPOCH 81/200, LOSS 0.36660711853592487\n","EPOCH 82/200, LOSS 0.3649686310026381\n","EPOCH 83/200, LOSS 0.36340598706845884\n","EPOCH 84/200, LOSS 0.3619118708151358\n","EPOCH 85/200, LOSS 0.3604776285312794\n","EPOCH 86/200, LOSS 0.359083374341329\n","EPOCH 87/200, LOSS 0.35772165987226695\n","EPOCH 88/200, LOSS 0.3563889529969957\n","EPOCH 89/200, LOSS 0.3550785029375994\n","EPOCH 90/200, LOSS 0.353789316283332\n","EPOCH 91/200, LOSS 0.352519741764775\n","EPOCH 92/200, LOSS 0.35126901555944373\n","EPOCH 93/200, LOSS 0.3500372657069454\n","EPOCH 94/200, LOSS 0.3488239182366265\n","EPOCH 95/200, LOSS 0.34762741459740537\n","EPOCH 96/200, LOSS 0.3464444036836977\n","EPOCH 97/200, LOSS 0.3452715917869851\n","EPOCH 98/200, LOSS 0.3441078883630258\n","EPOCH 99/200, LOSS 0.3429559248465079\n","EPOCH 100/200, LOSS 0.34181917155230485\n","EPOCH 101/200, LOSS 0.34069704126428674\n","EPOCH 102/200, LOSS 0.3395882270954273\n","EPOCH 103/200, LOSS 0.3384932809405857\n","EPOCH 104/200, LOSS 0.33741134625894054\n","EPOCH 105/200, LOSS 0.3363427232812952\n","EPOCH 106/200, LOSS 0.33528721111792104\n","EPOCH 107/200, LOSS 0.334244657445837\n","EPOCH 108/200, LOSS 0.3332153757413228\n","EPOCH 109/200, LOSS 0.33219969272613525\n","EPOCH 110/200, LOSS 0.3311973103770503\n","EPOCH 111/200, LOSS 0.3302056105048568\n","EPOCH 112/200, LOSS 0.3292215775560449\n","EPOCH 113/200, LOSS 0.32824872158191815\n","EPOCH 114/200, LOSS 0.32727800033710625\n","EPOCH 115/200, LOSS 0.33235625425974524\n","EPOCH 116/200, LOSS 0.35193272431691486\n","EPOCH 117/200, LOSS 0.3325659588531211\n","EPOCH 118/200, LOSS 0.3271064912831341\n","EPOCH 119/200, LOSS 0.3249990012910631\n","EPOCH 120/200, LOSS 0.3230176435576544\n","EPOCH 121/200, LOSS 0.32162045107947457\n","EPOCH 122/200, LOSS 0.3206381488729406\n","EPOCH 123/200, LOSS 0.319651981194814\n","EPOCH 124/200, LOSS 0.3187264535162184\n","EPOCH 125/200, LOSS 0.3178285029199388\n","EPOCH 126/200, LOSS 0.31696400156727544\n","EPOCH 127/200, LOSS 0.3161061185377615\n","EPOCH 128/200, LOSS 0.31524521995473787\n","EPOCH 129/200, LOSS 0.3143820630179511\n","EPOCH 130/200, LOSS 0.31352174944347805\n","EPOCH 131/200, LOSS 0.3126699681635256\n","EPOCH 132/200, LOSS 0.3118273615837097\n","EPOCH 133/200, LOSS 0.31099394736466585\n","EPOCH 134/200, LOSS 0.31016948488023544\n","EPOCH 135/200, LOSS 0.30935406464117543\n","EPOCH 136/200, LOSS 0.3085470729404025\n","EPOCH 137/200, LOSS 0.307747812182815\n","EPOCH 138/200, LOSS 0.30695595785423563\n","EPOCH 139/200, LOSS 0.3061711854404874\n","EPOCH 140/200, LOSS 0.3053935192249439\n","EPOCH 141/200, LOSS 0.30462310270026877\n","EPOCH 142/200, LOSS 0.30386016766230267\n","EPOCH 143/200, LOSS 0.3031045551653261\n","EPOCH 144/200, LOSS 0.30235693410590847\n","EPOCH 145/200, LOSS 0.3016135052398399\n","EPOCH 146/200, LOSS 0.30106929717240505\n","EPOCH 147/200, LOSS 0.3079170849588182\n","EPOCH 148/200, LOSS 0.30414838040316544\n","EPOCH 149/200, LOSS 0.3002115841265078\n","EPOCH 150/200, LOSS 0.29958065792366306\n","EPOCH 151/200, LOSS 0.2993766886216623\n","EPOCH 152/200, LOSS 0.29834668062351366\n","EPOCH 153/200, LOSS 0.29734408634680287\n","EPOCH 154/200, LOSS 0.296332198160666\n","EPOCH 155/200, LOSS 0.2953823407491048\n","EPOCH 156/200, LOSS 0.2946149221173039\n","EPOCH 157/200, LOSS 0.2939865434611285\n","EPOCH 158/200, LOSS 0.29333290126588607\n","EPOCH 159/200, LOSS 0.2925572991371155\n","EPOCH 160/200, LOSS 0.29213062039128057\n","EPOCH 161/200, LOSS 0.2919816507233514\n","EPOCH 162/200, LOSS 0.29228524367014563\n","EPOCH 163/200, LOSS 0.2920628896466008\n","EPOCH 164/200, LOSS 0.2905062790270205\n","EPOCH 165/200, LOSS 0.2915827521571407\n","EPOCH 166/200, LOSS 0.29343170148354986\n","EPOCH 167/200, LOSS 0.2979315519332886\n","EPOCH 168/200, LOSS 0.2999915767599035\n","EPOCH 169/200, LOSS 0.2946319580078125\n","EPOCH 170/200, LOSS 0.2945482951623422\n","EPOCH 171/200, LOSS 0.29177832824212535\n","EPOCH 172/200, LOSS 0.28768636341448184\n","EPOCH 173/200, LOSS 0.2865799334314134\n","EPOCH 174/200, LOSS 0.28483934314162646\n","EPOCH 175/200, LOSS 0.2840105096499126\n","EPOCH 176/200, LOSS 0.2835912881074128\n","EPOCH 177/200, LOSS 0.28299396788632425\n","EPOCH 178/200, LOSS 0.2822065794909442\n","EPOCH 179/200, LOSS 0.28136055999332005\n","EPOCH 180/200, LOSS 0.2805754409896003\n","EPOCH 181/200, LOSS 0.2798824067468996\n","EPOCH 182/200, LOSS 0.27930049543027524\n","EPOCH 183/200, LOSS 0.2788296319820263\n","EPOCH 184/200, LOSS 0.27847167959919683\n","EPOCH 185/200, LOSS 0.2782750063472324\n","EPOCH 186/200, LOSS 0.2783588722900108\n","EPOCH 187/200, LOSS 0.2791474020039594\n","EPOCH 188/200, LOSS 0.2812840011384752\n","EPOCH 189/200, LOSS 0.281340323112629\n","EPOCH 190/200, LOSS 0.2779648988335221\n","EPOCH 191/200, LOSS 0.2759632777284693\n","EPOCH 192/200, LOSS 0.2746758659680685\n","EPOCH 193/200, LOSS 0.27368963206255875\n","EPOCH 194/200, LOSS 0.2730185455746121\n","EPOCH 195/200, LOSS 0.27234276356520476\n","EPOCH 196/200, LOSS 0.2717092633247376\n","EPOCH 197/200, LOSS 0.2711349262131585\n","EPOCH 198/200, LOSS 0.2706453888504593\n","EPOCH 199/200, LOSS 0.270234328729135\n","EPOCH 200/200, LOSS 0.2698655702449657\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r_2HbO1bUJ1H"},"source":["# Model comparisons"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"BlZrm8JKS3gN","executionInfo":{"status":"ok","timestamp":1607035039058,"user_tz":360,"elapsed":1662,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"d2fcec82-d793-448a-accc-14059e169539"},"source":["import matplotlib.pyplot as plt\n","plt.plot(losses, '-', label = 'Base')\n","plt.plot(attention_losses, '-', label = 'Attention')\n","plt.plot(deep_losses, '-', label = 'Deep Attention')\n","plt.ylim((0, 5))\n","plt.xlabel('Epoch')\n","plt.ylabel('CE Loss')\n","plt.legend(loc='best')\n","plt.show()"],"execution_count":22,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":955},"id":"y7T3MMe-S3gN","executionInfo":{"status":"error","timestamp":1607034132053,"user_tz":360,"elapsed":255181,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"09ab7e46-17a1-4476-d3d1-bb8e873264d9"},"source":["for e, d in zip(\n","        [encoder, attention_encoder, deep_encoder],\n","        [decoder, attention_decoder, deep_decoder],\n","    ):\n","    total_bleu1 = 0\n","    total_bleu2 = 0\n","    total_bleu3 = 0\n","    total_bleu4 = 0\n","    for ind in range(test.shape[0]):\n","        bleu1, bleu2, bleu3, bleu4, loss, r, t, c = translate_sentence(e,\n","                                                                      d, \n","                                                                      test.iloc[ind], \n","                                                                      input_lang, \n","                                                                      output_lang)\n","        total_bleu1 += bleu1 * 100 / test.shape[0]\n","        total_bleu2 += bleu2 * 100 / test.shape[0]\n","        total_bleu3 += bleu3 * 100 / test.shape[0]\n","        total_bleu4 += bleu4 * 100 / test.shape[0]\n","        if ind < 10: \n","            print(r, \"|\", t, \"|\", c)\n","    print(\"{:.3f}, {:.3f}, {:.3f}, {:.3f}\".format(total_bleu1, total_bleu2, total_bleu3, total_bleu4))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["<start> would you like black tea or coffee <end> | <start> vorresti del te nero o del caffe <end> | <start> vorresti <unk> il libro <end>\n","<start> how often do you visit your family <end> | <start> quanto spesso vai a trovare la tua famiglia <end> | <start> quanto spesso <unk> la mia famiglia <end>\n","<start> im just being polite <end> | <start> io sto soltanto <unk> <unk> <end> | <start> sto solo <unk> <unk> <end>\n","<start> tom must be really worried <end> | <start> tom deve essere veramente preoccupato <end> | <start> tom deve essere veramente preoccupato <end>\n","<start> he cant see <unk> hear <end> | <start> non puo ne vedere ne sentire <end> | <start> non riesce a vedere la <unk> <end>\n","<start> you need to eat breakfast <end> | <start> devi mangiare la colazione <end> | <start> deve mangiare da solo <end>\n","<start> i never worried about you <end> | <start> non mi sono mai preoccupato per lei <end> | <start> non ho mai avuto una <unk> <end>\n","<start> tom is usually very <unk> isnt he <end> | <start> tom solitamente e molto <unk> vero <end> | <start> tom di solito e molto <unk> vero <end>\n","<start> we have to make up for lost time <end> | <start> noi dobbiamo <unk> il tempo perso <end> | <start> dobbiamo <unk> di <unk> ogni giorno <end>\n","<start> she <unk> a <unk> by <unk> <unk> <end> | <start> <unk> una fortuna <unk> <unk> <end> | <start> lei <unk> una <unk> <unk> <unk> <end>\n","62.758, 46.850, 36.379, 29.542\n","<start> would you like black tea or coffee <end> | <start> vorresti del te nero o del caffe <end> | <start> vorresti <unk> il te o caffe <end>\n","<start> how often do you visit your family <end> | <start> quanto spesso vai a trovare la tua famiglia <end> | <start> quanto spesso <unk> a chiave della sua famiglia <end>\n","<start> im just being polite <end> | <start> io sto soltanto <unk> <unk> <end> | <start> io sto solo <unk> <unk> <end>\n","<start> tom must be really worried <end> | <start> tom deve essere veramente preoccupato <end> | <start> tom deve essere davvero preoccupato <end>\n","<start> he cant see <unk> hear <end> | <start> non puo ne vedere ne sentire <end> | <start> lui non riesce a vedere <unk> sentire <end>\n","<start> you need to eat breakfast <end> | <start> devi mangiare la colazione <end> | <start> deve fare colazione <end>\n","<start> i never worried about you <end> | <start> non mi sono mai preoccupato per lei <end> | <start> non ho mai preoccupato per voi <end>\n","<start> tom is usually very <unk> isnt he <end> | <start> tom solitamente e molto <unk> vero <end> | <start> tom di solito e molto <unk> vero <end>\n","<start> we have to make up for lost time <end> | <start> noi dobbiamo <unk> il tempo perso <end> | <start> dobbiamo <unk> <unk> e il tempo <end>\n","<start> she <unk> a <unk> by <unk> <unk> <end> | <start> <unk> una fortuna <unk> <unk> <end> | <start> lei <unk> una <unk> <unk> <unk> <end>\n","72.786, 60.997, 51.073, 43.221\n","<start> would you like black tea or coffee <end> | <start> vorresti del te nero o del caffe <end> | <start> vorreste del te o una tv <end>\n","<start> how often do you visit your family <end> | <start> quanto spesso vai a trovare la tua famiglia <end> | <start> quanto spesso <unk> la sua famiglia <end>\n","<start> im just being polite <end> | <start> io sto soltanto <unk> <unk> <end> | <start> sto solo <unk> <unk> <end>\n","<start> tom must be really worried <end> | <start> tom deve essere veramente preoccupato <end> | <start> tom deve davvero davvero molto <end>\n","<start> he cant see <unk> hear <end> | <start> non puo ne vedere ne sentire <end> | <start> non riesce a vedere <unk> <unk> <end>\n","<start> you need to eat breakfast <end> | <start> devi mangiare la colazione <end> | <start> devi mangiare la colazione <end>\n","<start> i never worried about you <end> | <start> non mi sono mai preoccupato per lei <end> | <start> io non sono mai preoccupato per voi <end>\n","<start> tom is usually very <unk> isnt he <end> | <start> tom solitamente e molto <unk> vero <end> | <start> tom di solito e molto <unk> vero <end>\n","<start> we have to make up for lost time <end> | <start> noi dobbiamo <unk> il tempo perso <end> | <start> dobbiamo <unk> <unk> il tempo <end>\n","<start> she <unk> a <unk> by <unk> <unk> <end> | <start> <unk> una fortuna <unk> <unk> <end> | <start> <unk> un <unk> da <unk> <unk> <end>\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-b90f5667eaa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                                                                       \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                                                       \u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                                                       output_lang)\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtotal_bleu1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbleu1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtotal_bleu2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbleu2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-2b2974ad7ef7>\u001b[0m in \u001b[0;36mtranslate_sentence\u001b[0;34m(encoder, decoder, pair, ref_lang, targ_lang)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mcandidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-328d295c01e9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_sentence, hidden, encoder_output)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Pass into decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 740\u001b[0;31m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KM2pH3nFsvjB","executionInfo":{"status":"ok","timestamp":1607035727554,"user_tz":360,"elapsed":690152,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"8e678e8d-d655-4456-a4f7-72a809188b42"},"source":["for e, d in zip(\n","        [deep_encoder],\n","        [deep_decoder],\n","    ):\n","    total_bleu1 = 0\n","    total_bleu2 = 0\n","    total_bleu3 = 0\n","    total_bleu4 = 0\n","    for ind in range(test.shape[0]):\n","        bleu1, bleu2, bleu3, bleu4, loss, r, t, c = translate_sentence(e,\n","                                                                      d, \n","                                                                      test.iloc[ind], \n","                                                                      input_lang, \n","                                                                      output_lang)\n","        total_bleu1 += bleu1 * 100 / test.shape[0]\n","        total_bleu2 += bleu2 * 100 / test.shape[0]\n","        total_bleu3 += bleu3 * 100 / test.shape[0]\n","        total_bleu4 += bleu4 * 100 / test.shape[0]\n","        if ind < 10: \n","            print(r, \"|\", t, \"|\", c)\n","    print(\"{:.3f}, {:.3f}, {:.3f}, {:.3f}\".format(total_bleu1, total_bleu2, total_bleu3, total_bleu4))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["<start> would you like black tea or coffee <end> | <start> vorresti del te nero o del caffe <end> | <start> vorreste del te nero o il caffe <end>\n","<start> how often do you visit your family <end> | <start> quanto spesso vai a trovare la tua famiglia <end> | <start> quanto spesso <unk> a letto la tua famiglia <end>\n","<start> im just being polite <end> | <start> io sto soltanto <unk> <unk> <end> | <start> sto solo <unk> <unk> <end>\n","<start> tom must be really worried <end> | <start> tom deve essere veramente preoccupato <end> | <start> tom deve essere veramente preoccupato <end>\n","<start> he cant see <unk> hear <end> | <start> non puo ne vedere ne sentire <end> | <start> non riesce a vedere <unk> sentire <end>\n","<start> you need to eat breakfast <end> | <start> devi mangiare la colazione <end> | <start> deve mangiare la colazione <end>\n","<start> i never worried about you <end> | <start> non mi sono mai preoccupato per lei <end> | <start> non mi sono mai preoccupata per voi <end>\n","<start> tom is usually very <unk> isnt he <end> | <start> tom solitamente e molto <unk> vero <end> | <start> tom di solito e molto <unk> vero <end>\n","<start> we have to make up for lost time <end> | <start> noi dobbiamo <unk> il tempo perso <end> | <start> dobbiamo <unk> <unk> <unk> <end>\n","<start> she <unk> a <unk> by <unk> <unk> <end> | <start> <unk> una fortuna <unk> <unk> <end> | <start> lei ha <unk> un <unk> <unk> <unk> <end>\n","74.061, 62.757, 53.096, 45.296\n"],"name":"stdout"}]}]}