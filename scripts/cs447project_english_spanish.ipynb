{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cs447project_english_spanish.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNqlyYsjFEP6pOCw285BS0i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bnqyBxYPLyG6","executionInfo":{"status":"ok","timestamp":1607034642740,"user_tz":360,"elapsed":7761,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"fdb204ee-3858-4f78-cd93-96ae30017e33"},"source":["# PyTorch \n","!pip install --upgrade torch\n","!pip install --upgrade torchtext"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n","Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n","Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n","Collecting torchtext\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/23/8499af6d9c22b29b01f66a2c11d38ce71cd1cafa2655913c29818ed4a00f/torchtext-0.8.0-cp36-cp36m-manylinux1_x86_64.whl (6.9MB)\n","\u001b[K     |████████████████████████████████| 6.9MB 5.4MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.7.0+cu101)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n","Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n","Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.8)\n","Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.11.8)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n","Installing collected packages: torchtext\n","  Found existing installation: torchtext 0.3.1\n","    Uninstalling torchtext-0.3.1:\n","      Successfully uninstalled torchtext-0.3.1\n","Successfully installed torchtext-0.8.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bnDeYuA_LnXJ","executionInfo":{"status":"ok","timestamp":1607034646206,"user_tz":360,"elapsed":11220,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["from collections import defaultdict\n","import nltk\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction, corpus_bleu\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import re\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm.notebook import tqdm\n","import unicodedata\n","\n","if __name__ == '__main__':\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zQ4KX9-QTZcI"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"tUeHdn_uTfZ1","executionInfo":{"status":"ok","timestamp":1607034646207,"user_tz":360,"elapsed":11217,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Converts the unicode file to ascii\n","def unicode_to_ascii(s):\n","    \"\"\"Normalizes latin chars with accent to their canonical decomposition\"\"\"\n","    return ''.join(c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn')\n","\n","# Preprocessing the sentence to add the start, end tokens and make them lower-case\n","def preprocess_sentence(w):\n","    w = unicode_to_ascii(w.lower().strip())\n","    w = re.sub(r'([?.!,¿])', r' \\1 ', w)\n","    w = re.sub(r'[\" \"]+', ' ', w)\n","    w = re.sub(r'[^\\w\\s]', '', w) \n","\n","    w = re.sub(r'[^a-zA-Z?.!,¿]+', ' ', w)\n","    \n","    w = w.rstrip().strip()\n","    w = '<start> ' + w + ' <end>'\n","    return w\n","\n","def max_length(tensor):\n","    return max(len(t) for t in tensor)\n","\n","def pad_sequences(x, max_len):\n","    padded = np.zeros((max_len), dtype=np.int64)\n","    if len(x) > max_len:\n","        padded[:] = x[:max_len]\n","    else:\n","        padded[:len(x)] = x\n","    return padded\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2indexFull = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n","        self.word2countFull = {\"<start>\": 1e10, \"<end>\": 1e10, \"<unk>\": 1e10, \"<pad>\": 1e10}\n","        self.index2wordFull = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n","        self.word2index = {}\n","        self.index2word = {}\n","        self.n_wordsFull = 4  # Count SOS and EOS\n","        self.n_words = 0\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2indexFull:\n","            self.word2indexFull[word] = self.n_wordsFull\n","            self.word2countFull[word] = 1\n","            self.index2wordFull[self.n_wordsFull] = word\n","            self.n_wordsFull += 1\n","        else:\n","            self.word2countFull[word] += 1\n","\n","    def reduceDictionary(self, threshold = 50):\n","        n_words = 0\n","        for word in self.word2indexFull.keys():\n","            if self.word2countFull[word] >= threshold:\n","                self.word2index[word] = n_words\n","                self.index2word[n_words] = word\n","                n_words += 1\n","        self.n_words = n_words\n","    \n","    def sentence2Index(self, sentence):\n","        output = []\n","        for word in sentence.split(' '):\n","            if word in self.word2index.keys():\n","                output.append(self.word2index[word])\n","            else:\n","                output.append(self.word2index[\"<unk>\"])\n","        return output\n","\n","def build_dataset(target_language, threshold):\n","    # Load in and process sentences\n","    lines = open(target_language+'.txt', encoding='UTF-8').read().strip().split('\\n')\n","    original_word_pairs = [[w for w in l.split('\\t')][:2] for l in lines]\n","\n","    data = pd.DataFrame(original_word_pairs, columns=['eng', target_language])\n","    data['eng'] = data.eng.apply(lambda w: preprocess_sentence(w))\n","    data[target_language] = data[target_language].apply(lambda w: preprocess_sentence(w))\n","\n","    # Remove all sentences with length longer than 10 (+ 2 for start/end)\n","    data['len_eng'] = data.eng.apply(lambda w: len(w.split(\" \")))\n","    data['len_'+target_language] = data[target_language].apply(lambda w: len(w.split(\" \")))\n","    data = data[(data['len_eng'] <= MAX_LEN + 2)*(data['len_'+target_language] <= MAX_LEN + 2)]\n","    data = data[['eng',target_language]]\n","\n","    # Build language dictionaries \n","    input_lang = Lang('eng')\n","    output_lang = Lang(target_language)\n","    for sentence in data['eng']:\n","      input_lang.addSentence(sentence)\n","\n","    for sentence in data[target_language]:\n","      output_lang.addSentence(sentence)\n","    input_lang.reduceDictionary(threshold)\n","    output_lang.reduceDictionary(threshold)\n","\n","    data['eng'] = data.eng.apply(lambda w: input_lang.sentence2Index(w))\n","    data[target_language] = data[target_language].apply(lambda w: output_lang.sentence2Index(w))\n","    data['eng'] = data.eng.apply(lambda w: pad_sequences(w, MAX_LEN + 2))\n","    data[target_language] = data[target_language].apply(lambda w: pad_sequences(w, MAX_LEN + 2))\n","\n","    # Filter out sentences that have more than 1 (10%) UNK\n","    eng_filter = data['eng'].apply(lambda w: np.sum(w == input_lang.word2index['<unk>']))\n","    target_filter = data[target_language].apply(lambda w: np.sum(w == output_lang.word2index['<unk>']))\n","    data = data[(eng_filter <= MAX_UNK) * (target_filter <= MAX_UNK)]\n","\n","    return input_lang, output_lang, data"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bJBt9wUfT3Jo","executionInfo":{"status":"ok","timestamp":1607034650740,"user_tz":360,"elapsed":15745,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"62f21bb3-0425-40a4-9232-6a958f43d873"},"source":["!wget http://www.manythings.org/anki/spa-eng.zip\n","!unzip -o spa-eng.zip"],"execution_count":4,"outputs":[{"output_type":"stream","text":["--2020-12-03 22:30:45--  http://www.manythings.org/anki/spa-eng.zip\n","Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 172.67.173.198, ...\n","Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4845402 (4.6M) [application/zip]\n","Saving to: ‘spa-eng.zip’\n","\n","spa-eng.zip         100%[===================>]   4.62M  1.26MB/s    in 4.0s    \n","\n","2020-12-03 22:30:50 (1.15 MB/s) - ‘spa-eng.zip’ saved [4845402/4845402]\n","\n","Archive:  spa-eng.zip\n","  inflating: _about.txt              \n","  inflating: spa.txt                 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zkGty2WyUFNT","executionInfo":{"status":"ok","timestamp":1607034660057,"user_tz":360,"elapsed":25057,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"875a9698-0e2a-4f21-8aac-e3b7a7025993"},"source":["MAX_LEN = 10 # (+2 for <start>, <end>)\n","MAX_UNK = 1000 \n","THRESHOLD = 100\n","input_lang, output_lang, data = build_dataset('spa', THRESHOLD) #\n","print(\"Input words {}, Output words {}, N sentences {}\".format(input_lang.n_words, output_lang.n_words, data.shape[0]))\n","print(data)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/computation/expressions.py:204: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n","  f\"evaluating in Python space because the {repr(op_str)} \"\n"],"name":"stderr"},{"output_type":"stream","text":["Input words 746, Output words 720, N sentences 114550\n","                                                      eng                                              spa\n","0                    [1, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]             [1, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","1                    [1, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]             [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","2                    [1, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]             [1, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","3                    [1, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]             [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","4                    [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]             [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","...                                                   ...                                              ...\n","123981         [1, 3, 162, 362, 3, 3, 202, 3, 3, 2, 0, 0]        [1, 39, 3, 115, 39, 3, 3, 31, 3, 3, 2, 0]\n","123987  [1, 144, 139, 476, 3, 162, 278, 3, 639, 689, 3...   [1, 9, 457, 39, 3, 115, 127, 3, 43, 576, 3, 2]\n","124081    [1, 38, 221, 3, 3, 379, 553, 3, 583, 707, 3, 2]      [1, 141, 3, 3, 204, 374, 3, 38, 3, 3, 2, 0]\n","124099       [1, 3, 553, 3, 3, 379, 553, 276, 3, 3, 2, 0]     [1, 3, 478, 3, 195, 204, 478, 3, 3, 3, 2, 0]\n","124731   [1, 626, 3, 71, 3, 162, 362, 3, 530, 24, 473, 2]  [1, 16, 3, 3, 115, 39, 475, 127, 3, 38, 493, 2]\n","\n","[114550 rows x 2 columns]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/computation/expressions.py:204: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n","  f\"evaluating in Python space because the {repr(op_str)} \"\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"MRYEK7LNLUgE"},"source":["# Models"]},{"cell_type":"code","metadata":{"id":"-fFrQ7qnLSNd","executionInfo":{"status":"ok","timestamp":1607034660059,"user_tz":360,"elapsed":25052,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Encoder (Takes a sentence seq_len -> returns output, hidden)\n","class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers=1):\n","        super(EncoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, num_layers = num_layers)\n","\n","    def forward(self, input_sentence):\n","        embedded = self.embedding(input_sentence)\n","        output, hidden = self.gru(embedded)  \n","\n","        # For deep\n","        hidden = hidden[-1].unsqueeze(0)         \n","        return output, hidden"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"RqjwRA2jRAcr","executionInfo":{"status":"ok","timestamp":1607034660059,"user_tz":360,"elapsed":25047,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Decoder (Takes a sentence seq_len -> returns output)\n","class DecoderRNN(nn.Module):\n","    def __init__(self, output_size, hidden_size):\n","        super(DecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim = 2)\n","\n","    def forward(self, input_sentence, hidden, encoder_output):\n","        embedded = self.embedding(input_sentence)              \n","        output, decoder_hidden = self.gru(embedded, hidden)                    \n","        output = self.out(output).squeeze(0)                              \n","        return output, decoder_hidden"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jex7T-VWurHx","executionInfo":{"status":"ok","timestamp":1607034660060,"user_tz":360,"elapsed":25045,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Decoder with attention (Takes a sentence seq_len -> returns output)\n","class AttentionDecoderRNN(nn.Module):\n","    def __init__(self, output_size, hidden_size):\n","        super(AttentionDecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","\n","        self.score = nn.Sequential(\n","            nn.Linear(2 * hidden_size, hidden_size),\n","            nn.Tanh(),\n","            nn.Linear(hidden_size, 1)\n","        )\n","\n","        self.attention = nn.Sequential(\n","            nn.Linear(2 * hidden_size, hidden_size),\n","            nn.Tanh()\n","        )\n","\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim = 2)\n","\n","    def forward(self, input_sentence, hidden, encoder_output):\n","        embedded = self.embedding(input_sentence) # [1, batch_size, ]\n","\n","        # Compute score vector\n","        score_vector = self.score(\n","            torch.cat([torch.cat((MAX_LEN + 2)*[hidden]), encoder_output], dim = 2)\n","        ).squeeze(-1) # [seq_len, batch_size] \n","\n","        # Compute attention weights\n","        attention_weights = F.softmax(score_vector, dim = 0) # [seq_len, batch_size]\n","\n","        # Compute context vector\n","        context_vector = torch.einsum('sb, sbh -> bh', attention_weights, encoder_output) # [batch_size, hidden_size]\n","\n","        # Compute attention vector\n","        attention_vector = self.attention(torch.cat([context_vector.unsqueeze(0), embedded], dim = 2))\n","\n","        # Pass into decoder\n","        output, decoder_hidden = self.gru(attention_vector, hidden)                    \n","        output = self.out(output).squeeze(0)                              \n","        return output, decoder_hidden"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zOxsduH8c-Yt"},"source":["#  Training"]},{"cell_type":"code","metadata":{"id":"nWq4Mfj6dB7C","executionInfo":{"status":"ok","timestamp":1607034660061,"user_tz":360,"elapsed":25043,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["def translate_sentence(encoder, decoder, pair, ref_lang, targ_lang):\n","    \"\"\"\n","    Translate single sentence, returns\n","\n","    reference\n","    target\n","    candidate\n","    \"\"\"\n","    test_loss = 0\n","    candidate = []\n","    with torch.no_grad():\n","        reference = torch.tensor(pair[0]).unsqueeze(1).to(device)\n","        target =    torch.tensor(pair[1]).unsqueeze(1).to(device)\n","\n","        # Encoder pass\n","        encoder_output, encoder_hidden = encoder(reference) \n","  \n","        # Decoder pass\n","        decoder_input = target[0].unsqueeze(0)\n","        decoder_hidden = encoder_hidden\n","        candidate.append(decoder_input)\n","        for j in range(1, len(target)):\n","            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output) \n","            test_loss += loss_function(target[j], decoder_output) / len(target)\n","            decoder_input = F.log_softmax(decoder_output.unsqueeze(0), dim=-1).argmax(dim = -1)\n","            candidate.append(decoder_input)\n","            if decoder_input == targ_lang.word2index['<end>']:\n","                break\n","    \n","    reference = reference[reference > 0]\n","    target = target[target > 0]\n","\n","    reference = [ref_lang.index2word[int(s)] for s in reference]\n","    target =    [targ_lang.index2word[int(s)] for s in target]\n","    candidate = [targ_lang.index2word[int(s)] for s in candidate]\n","\n","    smoother = SmoothingFunction()\n","    bleu1 = sentence_bleu([target[1:]], candidate[1:], weights=(1,), smoothing_function=smoother.method1)\n","    bleu2 = sentence_bleu([target[1:]], candidate[1:], weights=(1/2, 1/2), smoothing_function=smoother.method1)\n","    bleu3 = sentence_bleu([target[1:]], candidate[1:], weights=(1/3, 1/3, 1/3), smoothing_function=smoother.method1)\n","    bleu4 = sentence_bleu([target[1:]], candidate[1:], weights=(1/4, 1/4, 1/4, 1/4), smoothing_function=smoother.method1)\n","    return bleu1, bleu2, bleu3, bleu4, test_loss, \" \".join(reference), \" \".join(target), \" \".join(candidate)\n","\n","def loss_function(real, pred):\n","    \"\"\" Only consider non-pad inputs in the loss; mask needed \"\"\"\n","    mask = real.ge(1).float()\n","    \n","    loss_ = F.cross_entropy(pred, real) * mask \n","    return torch.mean(loss_)\n","\n","def train_model(encoder, decoder, targ_lang, train, num_epochs, learning_rate, batch_size, breakp = 1e10):\n","    # Return training losses\n","    losses = []\n","    \n","    # Model, optimizer, criterion\n","    encoder = encoder.to(device)\n","    decoder = decoder.to(device)\n","    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n","\n","    # Build batches\n","    batches = [df for g, df in train.groupby(np.arange(len(train)) // batch_size)]\n","\n","    # Train\n","    for i in range(num_epochs):\n","        epoch_loss = 0\n","        for b in range(len(batches)):\n","            batch = batches[b]\n","            if len(batch) == batch_size: # Discard partial batches\n","                target = torch.tensor([s for s in batch[list(batch)[1]]]).T.to(device)\n","                reference = torch.tensor([s for s in batch[list(batch)[0]]]).T.to(device)\n","\n","                # Encoder pass: [max_len, batch_size, hidden_size], [1, batch_size, hidden_size]\n","                encoder_output, encoder_hidden = encoder(reference) \n","\n","                # Decoder pass: teacher forcing\n","                loss = 0\n","                decoder_input = target[0].unsqueeze(0) # [1, batch_size]\n","                decoder_hidden = encoder_hidden\n","                for j in range(1, len(target)):\n","                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output) # [batch_size, output_size], [1, batch_size, hidden_size]\n","                    loss += loss_function(target[j], decoder_output)\n","                    decoder_input = target[j].unsqueeze(0)\n","\n","                # Step\n","                loss.backward()\n","                encoder_optimizer.step()\n","                decoder_optimizer.step()\n","                encoder_optimizer.zero_grad()\n","                decoder_optimizer.zero_grad()\n","\n","                # Prints\n","                epoch_loss += loss.item() / (len(target) * len(batches))\n","\n","        # Training losses\n","        print(\"EPOCH {}/{}, LOSS {}\".format(i + 1, num_epochs, epoch_loss))\n","        losses.append(epoch_loss)\n","    return losses "],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9R9Gj0aKS7lw"},"source":["# Base model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XeFcWwFpqpLW","executionInfo":{"status":"ok","timestamp":1607034660773,"user_tz":360,"elapsed":25751,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"234f3d81-27d5-4c1d-e82e-b423f6bd7417"},"source":["# Train test split\n","data = data.sample(frac = 1, replace = False)\n","train = data.iloc[:data.shape[0]//4 * 3]\n","test = data.iloc[data.shape[0]//4 * 3:]\n","\n","# Model\n","HIDDEN_DIM = 128\n","encoder = EncoderRNN(input_lang.n_words, HIDDEN_DIM)\n","decoder = DecoderRNN(output_lang.n_words, HIDDEN_DIM)\n","print(\"N Params: \", sum(p.numel() for p in encoder.parameters()) + sum(p.numel() for p in decoder.parameters()))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["N Params:  478672\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KLv65YtkgxlA","executionInfo":{"status":"ok","timestamp":1607034826066,"user_tz":360,"elapsed":191037,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"7e1e69a5-4b65-4753-c74b-43f75dbd8fdf"},"source":["losses = train_model(encoder, decoder, output_lang, train, 200, 1e-3, len(train)//9)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["EPOCH 1/200, LOSS 2.9617909749348956\n","EPOCH 2/200, LOSS 2.2873456478118896\n","EPOCH 3/200, LOSS 1.8537216345469154\n","EPOCH 4/200, LOSS 1.7518339951833088\n","EPOCH 5/200, LOSS 1.7120381832122804\n","EPOCH 6/200, LOSS 1.6852162043253578\n","EPOCH 7/200, LOSS 1.6632439613342285\n","EPOCH 8/200, LOSS 1.6411399523417156\n","EPOCH 9/200, LOSS 1.6184736410776774\n","EPOCH 10/200, LOSS 1.5949224789937335\n","EPOCH 11/200, LOSS 1.5701891422271728\n","EPOCH 12/200, LOSS 1.5443236986796063\n","EPOCH 13/200, LOSS 1.5176546732584635\n","EPOCH 14/200, LOSS 1.4906649112701418\n","EPOCH 15/200, LOSS 1.4639108975728352\n","EPOCH 16/200, LOSS 1.4378707408905027\n","EPOCH 17/200, LOSS 1.4126558621724448\n","EPOCH 18/200, LOSS 1.3882708072662355\n","EPOCH 19/200, LOSS 1.3645991166432698\n","EPOCH 20/200, LOSS 1.3416650295257568\n","EPOCH 21/200, LOSS 1.3194685300191245\n","EPOCH 22/200, LOSS 1.297998015085856\n","EPOCH 23/200, LOSS 1.2770101070404054\n","EPOCH 24/200, LOSS 1.2564536889394124\n","EPOCH 25/200, LOSS 1.2364607652028403\n","EPOCH 26/200, LOSS 1.2168942928314208\n","EPOCH 27/200, LOSS 1.1976396719614664\n","EPOCH 28/200, LOSS 1.178843156496684\n","EPOCH 29/200, LOSS 1.1605069478352863\n","EPOCH 30/200, LOSS 1.1426661332448322\n","EPOCH 31/200, LOSS 1.125384267171224\n","EPOCH 32/200, LOSS 1.1086735328038535\n","EPOCH 33/200, LOSS 1.0925631125768025\n","EPOCH 34/200, LOSS 1.0769758224487305\n","EPOCH 35/200, LOSS 1.0618777036666869\n","EPOCH 36/200, LOSS 1.0471992015838623\n","EPOCH 37/200, LOSS 1.03296111424764\n","EPOCH 38/200, LOSS 1.01921439965566\n","EPOCH 39/200, LOSS 1.0059584856033328\n","EPOCH 40/200, LOSS 0.9931607087453207\n","EPOCH 41/200, LOSS 0.9808203220367431\n","EPOCH 42/200, LOSS 0.9689494768778483\n","EPOCH 43/200, LOSS 0.9575095653533935\n","EPOCH 44/200, LOSS 0.9464910984039306\n","EPOCH 45/200, LOSS 0.9358793735504151\n","EPOCH 46/200, LOSS 0.9256484031677246\n","EPOCH 47/200, LOSS 0.9157384792963662\n","EPOCH 48/200, LOSS 0.9061492045720417\n","EPOCH 49/200, LOSS 0.8968859036763509\n","EPOCH 50/200, LOSS 0.8879364331563313\n","EPOCH 51/200, LOSS 0.8793042580286661\n","EPOCH 52/200, LOSS 0.8710670312245687\n","EPOCH 53/200, LOSS 0.8631455739339193\n","EPOCH 54/200, LOSS 0.8551374276479087\n","EPOCH 55/200, LOSS 0.8471670230229695\n","EPOCH 56/200, LOSS 0.8396959940592447\n","EPOCH 57/200, LOSS 0.8324878454208375\n","EPOCH 58/200, LOSS 0.8254185835520426\n","EPOCH 59/200, LOSS 0.8185771703720093\n","EPOCH 60/200, LOSS 0.811944095293681\n","EPOCH 61/200, LOSS 0.8054719050725302\n","EPOCH 62/200, LOSS 0.7991497119267783\n","EPOCH 63/200, LOSS 0.7929701805114746\n","EPOCH 64/200, LOSS 0.7869263728459676\n","EPOCH 65/200, LOSS 0.7810131072998046\n","EPOCH 66/200, LOSS 0.7752274115880331\n","EPOCH 67/200, LOSS 0.7695649464925131\n","EPOCH 68/200, LOSS 0.7640179634094239\n","EPOCH 69/200, LOSS 0.7585786819458007\n","EPOCH 70/200, LOSS 0.7532425800959269\n","EPOCH 71/200, LOSS 0.7480140288670858\n","EPOCH 72/200, LOSS 0.7429091374079386\n","EPOCH 73/200, LOSS 0.737936226526896\n","EPOCH 74/200, LOSS 0.7330562114715576\n","EPOCH 75/200, LOSS 0.7282208601633707\n","EPOCH 76/200, LOSS 0.7235823472340901\n","EPOCH 77/200, LOSS 0.7190439462661744\n","EPOCH 78/200, LOSS 0.7143142461776734\n","EPOCH 79/200, LOSS 0.7096910794576009\n","EPOCH 80/200, LOSS 0.7052000681559245\n","EPOCH 81/200, LOSS 0.7008731762568157\n","EPOCH 82/200, LOSS 0.6966618537902831\n","EPOCH 83/200, LOSS 0.692499319712321\n","EPOCH 84/200, LOSS 0.6884000142415365\n","EPOCH 85/200, LOSS 0.68437020778656\n","EPOCH 86/200, LOSS 0.6804139137268067\n","EPOCH 87/200, LOSS 0.6765344301859537\n","EPOCH 88/200, LOSS 0.6727361996968587\n","EPOCH 89/200, LOSS 0.669025206565857\n","EPOCH 90/200, LOSS 0.6653710047403971\n","EPOCH 91/200, LOSS 0.6618650754292806\n","EPOCH 92/200, LOSS 0.6583818435668946\n","EPOCH 93/200, LOSS 0.6549263715744019\n","EPOCH 94/200, LOSS 0.6515702009201049\n","EPOCH 95/200, LOSS 0.6483442068099976\n","EPOCH 96/200, LOSS 0.6453418890635173\n","EPOCH 97/200, LOSS 0.6422863562901815\n","EPOCH 98/200, LOSS 0.6392572402954101\n","EPOCH 99/200, LOSS 0.6362031698226929\n","EPOCH 100/200, LOSS 0.6328399419784545\n","EPOCH 101/200, LOSS 0.6295186281204223\n","EPOCH 102/200, LOSS 0.6262358029683431\n","EPOCH 103/200, LOSS 0.6232491493225096\n","EPOCH 104/200, LOSS 0.6203661839167277\n","EPOCH 105/200, LOSS 0.6174825668334961\n","EPOCH 106/200, LOSS 0.6146550337473551\n","EPOCH 107/200, LOSS 0.6118810733159382\n","EPOCH 108/200, LOSS 0.6091509580612183\n","EPOCH 109/200, LOSS 0.6064752022425334\n","EPOCH 110/200, LOSS 0.6038575649261475\n","EPOCH 111/200, LOSS 0.6012869914372763\n","EPOCH 112/200, LOSS 0.5987495104471844\n","EPOCH 113/200, LOSS 0.5962403337160747\n","EPOCH 114/200, LOSS 0.5937546571095784\n","EPOCH 115/200, LOSS 0.5912956873575846\n","EPOCH 116/200, LOSS 0.5888834635416668\n","EPOCH 117/200, LOSS 0.5865287224451702\n","EPOCH 118/200, LOSS 0.5842294494311014\n","EPOCH 119/200, LOSS 0.581980307896932\n","EPOCH 120/200, LOSS 0.5797818104426066\n","EPOCH 121/200, LOSS 0.5776419878005982\n","EPOCH 122/200, LOSS 0.5755532026290893\n","EPOCH 123/200, LOSS 0.5736588478088378\n","EPOCH 124/200, LOSS 0.5717097322146099\n","EPOCH 125/200, LOSS 0.5694828629493713\n","EPOCH 126/200, LOSS 0.5673265894254049\n","EPOCH 127/200, LOSS 0.5652242382367452\n","EPOCH 128/200, LOSS 0.5631893237431844\n","EPOCH 129/200, LOSS 0.561271568139394\n","EPOCH 130/200, LOSS 0.5593059817949931\n","EPOCH 131/200, LOSS 0.5574406743049621\n","EPOCH 132/200, LOSS 0.555920418103536\n","EPOCH 133/200, LOSS 0.5542664647102356\n","EPOCH 134/200, LOSS 0.5526412526766459\n","EPOCH 135/200, LOSS 0.5507609049479166\n","EPOCH 136/200, LOSS 0.5490395863850912\n","EPOCH 137/200, LOSS 0.5473502874374389\n","EPOCH 138/200, LOSS 0.5456560333569845\n","EPOCH 139/200, LOSS 0.5440083543459574\n","EPOCH 140/200, LOSS 0.5424492239952087\n","EPOCH 141/200, LOSS 0.5409941832224529\n","EPOCH 142/200, LOSS 0.5395577708880106\n","EPOCH 143/200, LOSS 0.537883977095286\n","EPOCH 144/200, LOSS 0.5360955317815145\n","EPOCH 145/200, LOSS 0.5344030658404033\n","EPOCH 146/200, LOSS 0.5328758200009663\n","EPOCH 147/200, LOSS 0.5314666112263998\n","EPOCH 148/200, LOSS 0.5300386945406595\n","EPOCH 149/200, LOSS 0.5286144057909647\n","EPOCH 150/200, LOSS 0.5273484150568645\n","EPOCH 151/200, LOSS 0.5264679789543151\n","EPOCH 152/200, LOSS 0.5263316591580709\n","EPOCH 153/200, LOSS 0.5277328093846638\n","EPOCH 154/200, LOSS 0.5287012855211893\n","EPOCH 155/200, LOSS 0.5249767065048218\n","EPOCH 156/200, LOSS 0.5209059794743855\n","EPOCH 157/200, LOSS 0.5198564052581787\n","EPOCH 158/200, LOSS 0.5178232431411742\n","EPOCH 159/200, LOSS 0.515827723344167\n","EPOCH 160/200, LOSS 0.5142353773117065\n","EPOCH 161/200, LOSS 0.5124930620193482\n","EPOCH 162/200, LOSS 0.5107053756713866\n","EPOCH 163/200, LOSS 0.5090847253799438\n","EPOCH 164/200, LOSS 0.5077663262685139\n","EPOCH 165/200, LOSS 0.5066643317540486\n","EPOCH 166/200, LOSS 0.5056556701660156\n","EPOCH 167/200, LOSS 0.5048124313354492\n","EPOCH 168/200, LOSS 0.5043534437815348\n","EPOCH 169/200, LOSS 0.5047607183456422\n","EPOCH 170/200, LOSS 0.5056913455327352\n","EPOCH 171/200, LOSS 0.5043560822804769\n","EPOCH 172/200, LOSS 0.5007615605990092\n","EPOCH 173/200, LOSS 0.4973310708999634\n","EPOCH 174/200, LOSS 0.49626593192418417\n","EPOCH 175/200, LOSS 0.49467670520146684\n","EPOCH 176/200, LOSS 0.49279614686965945\n","EPOCH 177/200, LOSS 0.4915817260742188\n","EPOCH 178/200, LOSS 0.4907413283983867\n","EPOCH 179/200, LOSS 0.4897274096806844\n","EPOCH 180/200, LOSS 0.4884807944297791\n","EPOCH 181/200, LOSS 0.48715412616729736\n","EPOCH 182/200, LOSS 0.4860722462336223\n","EPOCH 183/200, LOSS 0.48521534999211624\n","EPOCH 184/200, LOSS 0.48447010119756073\n","EPOCH 185/200, LOSS 0.4836685498555501\n","EPOCH 186/200, LOSS 0.48280313412348436\n","EPOCH 187/200, LOSS 0.48194734255472815\n","EPOCH 188/200, LOSS 0.48126109441121423\n","EPOCH 189/200, LOSS 0.4808254996935527\n","EPOCH 190/200, LOSS 0.4805944681167603\n","EPOCH 191/200, LOSS 0.4804513335227967\n","EPOCH 192/200, LOSS 0.48039455413818355\n","EPOCH 193/200, LOSS 0.48045213222503663\n","EPOCH 194/200, LOSS 0.4806058168411255\n","EPOCH 195/200, LOSS 0.4810795505841573\n","EPOCH 196/200, LOSS 0.4812598307927449\n","EPOCH 197/200, LOSS 0.48002844254175825\n","EPOCH 198/200, LOSS 0.47747449477513637\n","EPOCH 199/200, LOSS 0.47460174560546875\n","EPOCH 200/200, LOSS 0.4719196796417236\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NkNIfoBzDXKq"},"source":["# Base model + attention"]},{"cell_type":"code","metadata":{"id":"H6oXqCrmDXKq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607034826068,"user_tz":360,"elapsed":191032,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"3e97e020-facd-4f4c-90ab-d2d70d8364ea"},"source":["# Model\n","HIDDEN_DIM = 128\n","attention_encoder = EncoderRNN(input_lang.n_words, HIDDEN_DIM)\n","attention_decoder = AttentionDecoderRNN(output_lang.n_words, HIDDEN_DIM)\n","print(\"N Params: \", sum(p.numel() for p in attention_encoder.parameters()) + sum(p.numel() for p in attention_decoder.parameters()))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["N Params:  544593\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g-JUGP1nDXKr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607035078177,"user_tz":360,"elapsed":443136,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"a3521f7c-7d89-4fe3-e4df-f3126007b3d3"},"source":["attention_losses = train_model(attention_encoder, attention_decoder, output_lang, train, 200, 1e-3, len(train)//9)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["EPOCH 1/200, LOSS 2.9041900952657063\n","EPOCH 2/200, LOSS 2.145966084798177\n","EPOCH 3/200, LOSS 1.8234176635742188\n","EPOCH 4/200, LOSS 1.7534342924753827\n","EPOCH 5/200, LOSS 1.7220098177591958\n","EPOCH 6/200, LOSS 1.6968788146972655\n","EPOCH 7/200, LOSS 1.6738709608713787\n","EPOCH 8/200, LOSS 1.6533292134602866\n","EPOCH 9/200, LOSS 1.6333069801330569\n","EPOCH 10/200, LOSS 1.6132283846537274\n","EPOCH 11/200, LOSS 1.592954874038696\n","EPOCH 12/200, LOSS 1.5722449779510497\n","EPOCH 13/200, LOSS 1.5514033953348796\n","EPOCH 14/200, LOSS 1.5306230703989665\n","EPOCH 15/200, LOSS 1.5101505597432454\n","EPOCH 16/200, LOSS 1.4898837407430015\n","EPOCH 17/200, LOSS 1.469597880045573\n","EPOCH 18/200, LOSS 1.449359655380249\n","EPOCH 19/200, LOSS 1.428659232457479\n","EPOCH 20/200, LOSS 1.4071661790211996\n","EPOCH 21/200, LOSS 1.3844947497049966\n","EPOCH 22/200, LOSS 1.3612536748250326\n","EPOCH 23/200, LOSS 1.3374680042266844\n","EPOCH 24/200, LOSS 1.3131687005360921\n","EPOCH 25/200, LOSS 1.289059003194173\n","EPOCH 26/200, LOSS 1.2653108914693196\n","EPOCH 27/200, LOSS 1.2408349990844727\n","EPOCH 28/200, LOSS 1.2164732933044435\n","EPOCH 29/200, LOSS 1.1928227027257283\n","EPOCH 30/200, LOSS 1.169244392712911\n","EPOCH 31/200, LOSS 1.1468587954839071\n","EPOCH 32/200, LOSS 1.1248092651367188\n","EPOCH 33/200, LOSS 1.1030232111612956\n","EPOCH 34/200, LOSS 1.082684048016866\n","EPOCH 35/200, LOSS 1.0628397464752197\n","EPOCH 36/200, LOSS 1.043329366048177\n","EPOCH 37/200, LOSS 1.0254384438196817\n","EPOCH 38/200, LOSS 1.0078929901123046\n","EPOCH 39/200, LOSS 0.9901181856791179\n","EPOCH 40/200, LOSS 0.9738846381505329\n","EPOCH 41/200, LOSS 0.9577089707056682\n","EPOCH 42/200, LOSS 0.9425319910049438\n","EPOCH 43/200, LOSS 0.9275632222493491\n","EPOCH 44/200, LOSS 0.9128784179687498\n","EPOCH 45/200, LOSS 0.8990772485733032\n","EPOCH 46/200, LOSS 0.8856066226959229\n","EPOCH 47/200, LOSS 0.8720794439315797\n","EPOCH 48/200, LOSS 0.8593099196751912\n","EPOCH 49/200, LOSS 0.8471787850062052\n","EPOCH 50/200, LOSS 0.834933876991272\n","EPOCH 51/200, LOSS 0.8229360103607177\n","EPOCH 52/200, LOSS 0.8117889563242594\n","EPOCH 53/200, LOSS 0.8009121417999268\n","EPOCH 54/200, LOSS 0.7897681872049968\n","EPOCH 55/200, LOSS 0.7792130072911579\n","EPOCH 56/200, LOSS 0.7693397045135498\n","EPOCH 57/200, LOSS 0.7590550104777017\n","EPOCH 58/200, LOSS 0.7488207419713339\n","EPOCH 59/200, LOSS 0.739758570988973\n","EPOCH 60/200, LOSS 0.730635126431783\n","EPOCH 61/200, LOSS 0.722114324569702\n","EPOCH 62/200, LOSS 0.7145012617111206\n","EPOCH 63/200, LOSS 0.7100084225336711\n","EPOCH 64/200, LOSS 0.7024967670440674\n","EPOCH 65/200, LOSS 0.6918281396230062\n","EPOCH 66/200, LOSS 0.683952244122823\n","EPOCH 67/200, LOSS 0.6765504678090414\n","EPOCH 68/200, LOSS 0.6697901248931885\n","EPOCH 69/200, LOSS 0.6631461858749389\n","EPOCH 70/200, LOSS 0.6566689491271973\n","EPOCH 71/200, LOSS 0.6503436962763469\n","EPOCH 72/200, LOSS 0.6443752368291219\n","EPOCH 73/200, LOSS 0.6386365254720052\n","EPOCH 74/200, LOSS 0.6331086079279582\n","EPOCH 75/200, LOSS 0.6278447389602662\n","EPOCH 76/200, LOSS 0.622876000404358\n","EPOCH 77/200, LOSS 0.6180947462717692\n","EPOCH 78/200, LOSS 0.6131345669428507\n","EPOCH 79/200, LOSS 0.6078047990798949\n","EPOCH 80/200, LOSS 0.6023849924405417\n","EPOCH 81/200, LOSS 0.5972227096557617\n","EPOCH 82/200, LOSS 0.5922942678133646\n","EPOCH 83/200, LOSS 0.5874810218811035\n","EPOCH 84/200, LOSS 0.582972784837087\n","EPOCH 85/200, LOSS 0.5787042578061421\n","EPOCH 86/200, LOSS 0.574591620763143\n","EPOCH 87/200, LOSS 0.5705656607945759\n","EPOCH 88/200, LOSS 0.5666055003801982\n","EPOCH 89/200, LOSS 0.5627181212107341\n","EPOCH 90/200, LOSS 0.5589232683181763\n","EPOCH 91/200, LOSS 0.5552399873733521\n","EPOCH 92/200, LOSS 0.5516876935958862\n","EPOCH 93/200, LOSS 0.548344898223877\n","EPOCH 94/200, LOSS 0.5455089330673218\n","EPOCH 95/200, LOSS 0.5428515911102294\n","EPOCH 96/200, LOSS 0.5405906518300374\n","EPOCH 97/200, LOSS 0.5401376684506733\n","EPOCH 98/200, LOSS 0.54104030529658\n","EPOCH 99/200, LOSS 0.532036264737447\n","EPOCH 100/200, LOSS 0.5277262250582377\n","EPOCH 101/200, LOSS 0.5241105953852335\n","EPOCH 102/200, LOSS 0.5208946585655212\n","EPOCH 103/200, LOSS 0.5179572939872742\n","EPOCH 104/200, LOSS 0.5152210553487142\n","EPOCH 105/200, LOSS 0.5123987436294556\n","EPOCH 106/200, LOSS 0.5097765564918518\n","EPOCH 107/200, LOSS 0.507282292842865\n","EPOCH 108/200, LOSS 0.5048241138458252\n","EPOCH 109/200, LOSS 0.5024026075998942\n","EPOCH 110/200, LOSS 0.5000251094500224\n","EPOCH 111/200, LOSS 0.4976807594299316\n","EPOCH 112/200, LOSS 0.4954334417978923\n","EPOCH 113/200, LOSS 0.4934860984484355\n","EPOCH 114/200, LOSS 0.49210081100463865\n","EPOCH 115/200, LOSS 0.491327945391337\n","EPOCH 116/200, LOSS 0.49281707604726155\n","EPOCH 117/200, LOSS 0.4885485013326009\n","EPOCH 118/200, LOSS 0.484563668568929\n","EPOCH 119/200, LOSS 0.48225362300872804\n","EPOCH 120/200, LOSS 0.48013049364089966\n","EPOCH 121/200, LOSS 0.4781651417414347\n","EPOCH 122/200, LOSS 0.47624405225118005\n","EPOCH 123/200, LOSS 0.47437659899393714\n","EPOCH 124/200, LOSS 0.4726157029469808\n","EPOCH 125/200, LOSS 0.4708971977233887\n","EPOCH 126/200, LOSS 0.46918835639953616\n","EPOCH 127/200, LOSS 0.46753640174865724\n","EPOCH 128/200, LOSS 0.4660444617271423\n","EPOCH 129/200, LOSS 0.46464734077453607\n","EPOCH 130/200, LOSS 0.4631729165712993\n","EPOCH 131/200, LOSS 0.4616136074066162\n","EPOCH 132/200, LOSS 0.45989982684453334\n","EPOCH 133/200, LOSS 0.4583908796310424\n","EPOCH 134/200, LOSS 0.45637646118799846\n","EPOCH 135/200, LOSS 0.4542421062787374\n","EPOCH 136/200, LOSS 0.4519104798634847\n","EPOCH 137/200, LOSS 0.45022320350011197\n","EPOCH 138/200, LOSS 0.448582657178243\n","EPOCH 139/200, LOSS 0.4470013340314229\n","EPOCH 140/200, LOSS 0.4454810460408529\n","EPOCH 141/200, LOSS 0.44436668157577514\n","EPOCH 142/200, LOSS 0.44319560527801516\n","EPOCH 143/200, LOSS 0.44176116784413655\n","EPOCH 144/200, LOSS 0.4399212876955668\n","EPOCH 145/200, LOSS 0.43886079390843713\n","EPOCH 146/200, LOSS 0.43761807680130005\n","EPOCH 147/200, LOSS 0.4357179323832194\n","EPOCH 148/200, LOSS 0.4342302004496257\n","EPOCH 149/200, LOSS 0.4335405349731445\n","EPOCH 150/200, LOSS 0.4324735562006633\n","EPOCH 151/200, LOSS 0.4308537483215332\n","EPOCH 152/200, LOSS 0.42940054734547933\n","EPOCH 153/200, LOSS 0.42908343076705935\n","EPOCH 154/200, LOSS 0.42793577114741005\n","EPOCH 155/200, LOSS 0.42605995337168373\n","EPOCH 156/200, LOSS 0.4252986709276836\n","EPOCH 157/200, LOSS 0.4257241169611613\n","EPOCH 158/200, LOSS 0.4248064160346985\n","EPOCH 159/200, LOSS 0.42441460688908894\n","EPOCH 160/200, LOSS 0.4250870704650879\n","EPOCH 161/200, LOSS 0.4256078243255615\n","EPOCH 162/200, LOSS 0.43044384320576984\n","EPOCH 163/200, LOSS 0.44255344470342\n","EPOCH 164/200, LOSS 0.4461044232050578\n","EPOCH 165/200, LOSS 0.42499940395355223\n","EPOCH 166/200, LOSS 0.4212117910385132\n","EPOCH 167/200, LOSS 0.41744586626688646\n","EPOCH 168/200, LOSS 0.41348847945531214\n","EPOCH 169/200, LOSS 0.41175432205200196\n","EPOCH 170/200, LOSS 0.41125414371490476\n","EPOCH 171/200, LOSS 0.41031126578648885\n","EPOCH 172/200, LOSS 0.4090052962303162\n","EPOCH 173/200, LOSS 0.40732270081837973\n","EPOCH 174/200, LOSS 0.4055164535840353\n","EPOCH 175/200, LOSS 0.4039501786231995\n","EPOCH 176/200, LOSS 0.40300316810607906\n","EPOCH 177/200, LOSS 0.40282242298126214\n","EPOCH 178/200, LOSS 0.40336835781733194\n","EPOCH 179/200, LOSS 0.4051342884699505\n","EPOCH 180/200, LOSS 0.40866129795710243\n","EPOCH 181/200, LOSS 0.40958460569381716\n","EPOCH 182/200, LOSS 0.407361356417338\n","EPOCH 183/200, LOSS 0.3997763911883036\n","EPOCH 184/200, LOSS 0.3984718720118205\n","EPOCH 185/200, LOSS 0.3975706299146017\n","EPOCH 186/200, LOSS 0.39604951540629074\n","EPOCH 187/200, LOSS 0.3942327817281087\n","EPOCH 188/200, LOSS 0.3929925163586934\n","EPOCH 189/200, LOSS 0.392549975713094\n","EPOCH 190/200, LOSS 0.39256252050399776\n","EPOCH 191/200, LOSS 0.39266233046849575\n","EPOCH 192/200, LOSS 0.3925827463467916\n","EPOCH 193/200, LOSS 0.39245411554972326\n","EPOCH 194/200, LOSS 0.39200361967086783\n","EPOCH 195/200, LOSS 0.3904006203015645\n","EPOCH 196/200, LOSS 0.39004717270533246\n","EPOCH 197/200, LOSS 0.3900404334068298\n","EPOCH 198/200, LOSS 0.3880903005599975\n","EPOCH 199/200, LOSS 0.3865277846654256\n","EPOCH 200/200, LOSS 0.3848928213119507\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AB7hW9IPS3gM"},"source":["# Base model + attention + deep encoder"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OFreqqb-S3gM","executionInfo":{"status":"ok","timestamp":1607035078180,"user_tz":360,"elapsed":443133,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"6e315466-1f94-4ba1-dbe6-db0f480c2c65"},"source":["# Model\n","import copy\n","HIDDEN_DIM = 128\n","deep_encoder = EncoderRNN(input_lang.n_words, HIDDEN_DIM, 2)\n","deep_decoder = copy.deepcopy(attention_decoder) #AttentionDecoderRNN(output_lang.n_words, HIDDEN_DIM)\n","print(\"N Params: \", sum(p.numel() for p in deep_encoder.parameters()) + sum(p.numel() for p in deep_decoder.parameters()))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["N Params:  643665\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qwXlbKStS3gM","executionInfo":{"status":"ok","timestamp":1607035342984,"user_tz":360,"elapsed":707931,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"10f39157-89a6-4b65-aa7f-4cec019ebeee"},"source":["deep_losses = train_model(deep_encoder, deep_decoder, output_lang, train, 200, 1e-3, len(train)//9)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["EPOCH 1/200, LOSS 1.8795223236083984\n","EPOCH 2/200, LOSS 1.5058482487996419\n","EPOCH 3/200, LOSS 1.3758157253265382\n","EPOCH 4/200, LOSS 1.2781602700551349\n","EPOCH 5/200, LOSS 1.1991435607274374\n","EPOCH 6/200, LOSS 1.1288502057393393\n","EPOCH 7/200, LOSS 1.0657607237497966\n","EPOCH 8/200, LOSS 1.006949249903361\n","EPOCH 9/200, LOSS 0.9521990299224854\n","EPOCH 10/200, LOSS 0.9021675348281861\n","EPOCH 11/200, LOSS 0.857794745763143\n","EPOCH 12/200, LOSS 0.8191307147343954\n","EPOCH 13/200, LOSS 0.7855723857879638\n","EPOCH 14/200, LOSS 0.7562782287597658\n","EPOCH 15/200, LOSS 0.7300562222798666\n","EPOCH 16/200, LOSS 0.7065982023874919\n","EPOCH 17/200, LOSS 0.6857331116994223\n","EPOCH 18/200, LOSS 0.6671441713968912\n","EPOCH 19/200, LOSS 0.6508107900619508\n","EPOCH 20/200, LOSS 0.6360037803649903\n","EPOCH 21/200, LOSS 0.6223763147989907\n","EPOCH 22/200, LOSS 0.6103448788324992\n","EPOCH 23/200, LOSS 0.6003695726394654\n","EPOCH 24/200, LOSS 0.5902888774871826\n","EPOCH 25/200, LOSS 0.5810768564542135\n","EPOCH 26/200, LOSS 0.5719119230906169\n","EPOCH 27/200, LOSS 0.5641229867935181\n","EPOCH 28/200, LOSS 0.5569957097371419\n","EPOCH 29/200, LOSS 0.5503674507141113\n","EPOCH 30/200, LOSS 0.5471200068791707\n","EPOCH 31/200, LOSS 0.5398639599482218\n","EPOCH 32/200, LOSS 0.5345778981844584\n","EPOCH 33/200, LOSS 0.5287858764330546\n","EPOCH 34/200, LOSS 0.5239698330561321\n","EPOCH 35/200, LOSS 0.5195724447568257\n","EPOCH 36/200, LOSS 0.5155382593472799\n","EPOCH 37/200, LOSS 0.5119192997614542\n","EPOCH 38/200, LOSS 0.5093736211458841\n","EPOCH 39/200, LOSS 0.5058196544647217\n","EPOCH 40/200, LOSS 0.5022296667098999\n","EPOCH 41/200, LOSS 0.49917977650960293\n","EPOCH 42/200, LOSS 0.49545708497365315\n","EPOCH 43/200, LOSS 0.4927547772725423\n","EPOCH 44/200, LOSS 0.4905225356419881\n","EPOCH 45/200, LOSS 0.4861738999684652\n","EPOCH 46/200, LOSS 0.48237492640813195\n","EPOCH 47/200, LOSS 0.47975910504659014\n","EPOCH 48/200, LOSS 0.47693494160970057\n","EPOCH 49/200, LOSS 0.4748796542485555\n","EPOCH 50/200, LOSS 0.47367409070332844\n","EPOCH 51/200, LOSS 0.47059425910313923\n","EPOCH 52/200, LOSS 0.4705499172210693\n","EPOCH 53/200, LOSS 0.4663272658983866\n","EPOCH 54/200, LOSS 0.4659379402796427\n","EPOCH 55/200, LOSS 0.4659146547317505\n","EPOCH 56/200, LOSS 0.46808532079060877\n","EPOCH 57/200, LOSS 0.45931257804234826\n","EPOCH 58/200, LOSS 0.457428240776062\n","EPOCH 59/200, LOSS 0.45441348950068156\n","EPOCH 60/200, LOSS 0.4532665848731995\n","EPOCH 61/200, LOSS 0.4514654715855917\n","EPOCH 62/200, LOSS 0.44989624818166096\n","EPOCH 63/200, LOSS 0.44789690176645913\n","EPOCH 64/200, LOSS 0.44561725457509355\n","EPOCH 65/200, LOSS 0.4432395418485007\n","EPOCH 66/200, LOSS 0.44141099055608113\n","EPOCH 67/200, LOSS 0.43983046611150106\n","EPOCH 68/200, LOSS 0.4382496515909831\n","EPOCH 69/200, LOSS 0.43663687308629356\n","EPOCH 70/200, LOSS 0.4350557645161946\n","EPOCH 71/200, LOSS 0.43354708751042687\n","EPOCH 72/200, LOSS 0.4321147282918294\n","EPOCH 73/200, LOSS 0.4307817260424296\n","EPOCH 74/200, LOSS 0.42961370944976807\n","EPOCH 75/200, LOSS 0.4287367343902588\n","EPOCH 76/200, LOSS 0.42835896809895835\n","EPOCH 77/200, LOSS 0.42886970043182376\n","EPOCH 78/200, LOSS 0.4326274474461873\n","EPOCH 79/200, LOSS 0.43435308933258054\n","EPOCH 80/200, LOSS 0.42632913986841836\n","EPOCH 81/200, LOSS 0.421525772412618\n","EPOCH 82/200, LOSS 0.41876709858576455\n","EPOCH 83/200, LOSS 0.4165519873301188\n","EPOCH 84/200, LOSS 0.41520754893620804\n","EPOCH 85/200, LOSS 0.4136128028233846\n","EPOCH 86/200, LOSS 0.4123124321301778\n","EPOCH 87/200, LOSS 0.41092563470204674\n","EPOCH 88/200, LOSS 0.4095824480056763\n","EPOCH 89/200, LOSS 0.4082619746526083\n","EPOCH 90/200, LOSS 0.40693193674087524\n","EPOCH 91/200, LOSS 0.4056256135304769\n","EPOCH 92/200, LOSS 0.404357365767161\n","EPOCH 93/200, LOSS 0.40312312841415404\n","EPOCH 94/200, LOSS 0.40191662311553955\n","EPOCH 95/200, LOSS 0.4007329305013021\n","EPOCH 96/200, LOSS 0.3995769103368123\n","EPOCH 97/200, LOSS 0.39845443964004523\n","EPOCH 98/200, LOSS 0.39737891753514604\n","EPOCH 99/200, LOSS 0.3963748733202616\n","EPOCH 100/200, LOSS 0.39549930095672603\n","EPOCH 101/200, LOSS 0.3949046373367309\n","EPOCH 102/200, LOSS 0.3951696674029032\n","EPOCH 103/200, LOSS 0.3992385307947795\n","EPOCH 104/200, LOSS 0.4042060573895772\n","EPOCH 105/200, LOSS 0.3955492933591207\n","EPOCH 106/200, LOSS 0.392701264222463\n","EPOCH 107/200, LOSS 0.38894442717234295\n","EPOCH 108/200, LOSS 0.3881182273228963\n","EPOCH 109/200, LOSS 0.3865184386571249\n","EPOCH 110/200, LOSS 0.3853413184483846\n","EPOCH 111/200, LOSS 0.3841343998908997\n","EPOCH 112/200, LOSS 0.38310185273488356\n","EPOCH 113/200, LOSS 0.38204592068990073\n","EPOCH 114/200, LOSS 0.38104270696640014\n","EPOCH 115/200, LOSS 0.38002785046895343\n","EPOCH 116/200, LOSS 0.37901435693105057\n","EPOCH 117/200, LOSS 0.3780366142590841\n","EPOCH 118/200, LOSS 0.3770808974901835\n","EPOCH 119/200, LOSS 0.3762186050415039\n","EPOCH 120/200, LOSS 0.37546894947687787\n","EPOCH 121/200, LOSS 0.3748662312825521\n","EPOCH 122/200, LOSS 0.37442830006281536\n","EPOCH 123/200, LOSS 0.3744112571080526\n","EPOCH 124/200, LOSS 0.3757397095362346\n","EPOCH 125/200, LOSS 0.3763744632403056\n","EPOCH 126/200, LOSS 0.37704007625579833\n","EPOCH 127/200, LOSS 0.37200017770131427\n","EPOCH 128/200, LOSS 0.3703262448310852\n","EPOCH 129/200, LOSS 0.3691912571589152\n","EPOCH 130/200, LOSS 0.36779492696126304\n","EPOCH 131/200, LOSS 0.3674764394760132\n","EPOCH 132/200, LOSS 0.3679702003796896\n","EPOCH 133/200, LOSS 0.36883715391159055\n","EPOCH 134/200, LOSS 0.37094573179880785\n","EPOCH 135/200, LOSS 0.37712905804316205\n","EPOCH 136/200, LOSS 0.378554371992747\n","EPOCH 137/200, LOSS 0.37150873343149826\n","EPOCH 138/200, LOSS 0.3650433262189229\n","EPOCH 139/200, LOSS 0.36300694942474365\n","EPOCH 140/200, LOSS 0.3622319539388021\n","EPOCH 141/200, LOSS 0.3609915335973104\n","EPOCH 142/200, LOSS 0.35992552439371744\n","EPOCH 143/200, LOSS 0.35894235769907634\n","EPOCH 144/200, LOSS 0.35879045724868774\n","EPOCH 145/200, LOSS 0.3587989091873169\n","EPOCH 146/200, LOSS 0.3594359993934631\n","EPOCH 147/200, LOSS 0.36094503402709965\n","EPOCH 148/200, LOSS 0.36586267153422036\n","EPOCH 149/200, LOSS 0.3764697790145874\n","EPOCH 150/200, LOSS 0.38114669322967526\n","EPOCH 151/200, LOSS 0.3586463371912639\n","EPOCH 152/200, LOSS 0.3552042563756307\n","EPOCH 153/200, LOSS 0.35196516116460164\n","EPOCH 154/200, LOSS 0.3473713556925455\n","EPOCH 155/200, LOSS 0.34723416169484456\n","EPOCH 156/200, LOSS 0.34641208648681643\n","EPOCH 157/200, LOSS 0.34424924850463867\n","EPOCH 158/200, LOSS 0.34214926163355514\n","EPOCH 159/200, LOSS 0.34081106980641684\n","EPOCH 160/200, LOSS 0.34009691079457605\n","EPOCH 161/200, LOSS 0.3396268685658773\n","EPOCH 162/200, LOSS 0.33921085993448896\n","EPOCH 163/200, LOSS 0.3388509591420492\n","EPOCH 164/200, LOSS 0.338629428545634\n","EPOCH 165/200, LOSS 0.33840265671412145\n","EPOCH 166/200, LOSS 0.3379155874252319\n","EPOCH 167/200, LOSS 0.3369914094607035\n","EPOCH 168/200, LOSS 0.3364697893460592\n","EPOCH 169/200, LOSS 0.3360989054044088\n","EPOCH 170/200, LOSS 0.3347490231196086\n","EPOCH 171/200, LOSS 0.3333048224449158\n","EPOCH 172/200, LOSS 0.33126032749811807\n","EPOCH 173/200, LOSS 0.3299967805544536\n","EPOCH 174/200, LOSS 0.3293525218963623\n","EPOCH 175/200, LOSS 0.3288775205612182\n","EPOCH 176/200, LOSS 0.3284180521965027\n","EPOCH 177/200, LOSS 0.3280030250549317\n","EPOCH 178/200, LOSS 0.3276985645294189\n","EPOCH 179/200, LOSS 0.3275013526280721\n","EPOCH 180/200, LOSS 0.3277093688646952\n","EPOCH 181/200, LOSS 0.32891828616460167\n","EPOCH 182/200, LOSS 0.33174564043680826\n","EPOCH 183/200, LOSS 0.33818932374318444\n","EPOCH 184/200, LOSS 0.3455556273460388\n","EPOCH 185/200, LOSS 0.3470933675765991\n","EPOCH 186/200, LOSS 0.34051494201024374\n","EPOCH 187/200, LOSS 0.3329017718633016\n","EPOCH 188/200, LOSS 0.32865062554677327\n","EPOCH 189/200, LOSS 0.326125705242157\n","EPOCH 190/200, LOSS 0.3253817558288574\n","EPOCH 191/200, LOSS 0.32473728656768797\n","EPOCH 192/200, LOSS 0.3242500821749369\n","EPOCH 193/200, LOSS 0.32377319335937504\n","EPOCH 194/200, LOSS 0.32351295550664266\n","EPOCH 195/200, LOSS 0.32302202781041467\n","EPOCH 196/200, LOSS 0.3220327377319335\n","EPOCH 197/200, LOSS 0.3210684418678284\n","EPOCH 198/200, LOSS 0.3211100180943807\n","EPOCH 199/200, LOSS 0.3228305657704672\n","EPOCH 200/200, LOSS 0.3240133802096049\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r_2HbO1bUJ1H"},"source":["# Model comparisons"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"BlZrm8JKS3gN","executionInfo":{"status":"ok","timestamp":1607035342987,"user_tz":360,"elapsed":707913,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"148f0cd3-5b29-4254-915d-c75657b43a93"},"source":["import matplotlib.pyplot as plt\n","plt.plot(losses, '-', label = 'Base')\n","plt.plot(attention_losses, '-', label = 'Attention')\n","plt.plot(deep_losses, '-', label = 'Deep Attention')\n","plt.ylim((0, 5))\n","plt.xlabel('Epoch')\n","plt.ylabel('CE Loss')\n","plt.legend(loc='best')\n","plt.show()"],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7T3MMe-S3gN","executionInfo":{"status":"ok","timestamp":1607036031287,"user_tz":360,"elapsed":1396209,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"ca64a611-b8bb-48ec-c10e-489f0cba9f04"},"source":["for e, d in zip(\n","        [encoder, attention_encoder, deep_encoder],\n","        [decoder, attention_decoder, deep_decoder],\n","    ):\n","    total_bleu1 = 0\n","    total_bleu2 = 0\n","    total_bleu3 = 0\n","    total_bleu4 = 0\n","    for ind in range(test.shape[0]):\n","        bleu1, bleu2, bleu3, bleu4, loss, r, t, c = translate_sentence(e,\n","                                                                      d, \n","                                                                      test.iloc[ind], \n","                                                                      input_lang, \n","                                                                      output_lang)\n","        total_bleu1 += bleu1 * 100 / test.shape[0]\n","        total_bleu2 += bleu2 * 100 / test.shape[0]\n","        total_bleu3 += bleu3 * 100 / test.shape[0]\n","        total_bleu4 += bleu4 * 100 / test.shape[0]\n","        if ind < 10: \n","            print(r, \"|\", t, \"|\", c)\n","    print(\"{:.3f}, {:.3f}, {:.3f}, {:.3f}\".format(total_bleu1, total_bleu2, total_bleu3, total_bleu4))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["<start> i cant <unk> it <end> | <start> no puedo <unk> <end> | <start> no puedo <unk> <end>\n","<start> now be <unk> <end> | <start> ahora <unk> <unk> <end> | <start> ahora mismo <end>\n","<start> let go of my arm <end> | <start> <unk> el <unk> <end> | <start> <unk> <unk> <end>\n","<start> by the time you get back <unk> have left <end> | <start> para cuando <unk> ella ya se <unk> ido <end> | <start> por favor <unk> cuando <unk> <end>\n","<start> you know what tom wants <end> | <start> tu sabes que quiere tom <end> | <start> sabes lo que tom quiere <end>\n","<start> she is <unk> for the job <end> | <start> ella es <unk> para ese trabajo <end> | <start> ella esta <unk> para el trabajo <end>\n","<start> i know you feel <unk> <end> | <start> se que te <unk> solo <end> | <start> se que <unk> <unk> <end>\n","<start> the building is one <unk> <unk> high <end> | <start> el edificio tiene una <unk> de <unk> <unk> <end> | <start> el edificio <unk> <unk> <unk> de <unk> <end>\n","<start> i cant stand the cold <end> | <start> no <unk> el frio <end> | <start> no puedo <unk> el <unk> <end>\n","<start> that tall boy <unk> the <unk> child <end> | <start> ese chico alto <unk> al nino que se estaba <unk> <end> | <start> ese chico <unk> ver los <unk> <unk> a tom <end>\n","65.336, 48.556, 35.846, 28.290\n","<start> i cant <unk> it <end> | <start> no puedo <unk> <end> | <start> no lo puedo <unk> <end>\n","<start> now be <unk> <end> | <start> ahora <unk> <unk> <end> | <start> ahora <unk> <end>\n","<start> let go of my arm <end> | <start> <unk> el <unk> <end> | <start> <unk> a mi <unk> <end>\n","<start> by the time you get back <unk> have left <end> | <start> para cuando <unk> ella ya se <unk> ido <end> | <start> a que hora <unk> <unk> <unk> <end>\n","<start> you know what tom wants <end> | <start> tu sabes que quiere tom <end> | <start> sabes lo que tom <end>\n","<start> she is <unk> for the job <end> | <start> ella es <unk> para ese trabajo <end> | <start> ella esta <unk> para el trabajo <end>\n","<start> i know you feel <unk> <end> | <start> se que te <unk> solo <end> | <start> se que te <unk> <unk> <end>\n","<start> the building is one <unk> <unk> high <end> | <start> el edificio tiene una <unk> de <unk> <unk> <end> | <start> el edificio es <unk> <unk> <unk> <end>\n","<start> i cant stand the cold <end> | <start> no <unk> el frio <end> | <start> no <unk> la <unk> <end>\n","<start> that tall boy <unk> the <unk> child <end> | <start> ese chico alto <unk> al nino que se estaba <unk> <end> | <start> ese chico <unk> <unk> el nino <end>\n","71.209, 57.435, 45.270, 36.956\n","<start> i cant <unk> it <end> | <start> no puedo <unk> <end> | <start> no puedo <unk> <end>\n","<start> now be <unk> <end> | <start> ahora <unk> <unk> <end> | <start> ahora <unk> <unk> <end>\n","<start> let go of my arm <end> | <start> <unk> el <unk> <end> | <start> <unk> a mi <unk> <end>\n","<start> by the time you get back <unk> have left <end> | <start> para cuando <unk> ella ya se <unk> ido <end> | <start> por hora de <unk> <unk> <unk> <end>\n","<start> you know what tom wants <end> | <start> tu sabes que quiere tom <end> | <start> sabes lo que tom quiere <end>\n","<start> she is <unk> for the job <end> | <start> ella es <unk> para ese trabajo <end> | <start> ella es <unk> para el trabajo <end>\n","<start> i know you feel <unk> <end> | <start> se que te <unk> solo <end> | <start> se que ustedes <unk> <unk> <end>\n","<start> the building is one <unk> <unk> high <end> | <start> el edificio tiene una <unk> de <unk> <unk> <end> | <start> el edificio es uno <unk> <unk> <unk> <end>\n","<start> i cant stand the cold <end> | <start> no <unk> el frio <end> | <start> no <unk> el frio <end>\n","<start> that tall boy <unk> the <unk> child <end> | <start> ese chico alto <unk> al nino que se estaba <unk> <end> | <start> ese nino se <unk> al <unk> <unk> <end>\n","72.299, 58.727, 46.777, 38.362\n"],"name":"stdout"}]}]}