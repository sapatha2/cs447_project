{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cs447project_english_french.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNL+9byiVBzDvyT899KJ0+n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bnqyBxYPLyG6","executionInfo":{"status":"ok","timestamp":1607030685175,"user_tz":360,"elapsed":5663,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"d4d1a0c6-f32c-42a4-d710-14588b2cde38"},"source":["# PyTorch \n","!pip install --upgrade torch\n","!pip install --upgrade torchtext"],"execution_count":44,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n","Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n","Requirement already up-to-date: torchtext in /usr/local/lib/python3.6/dist-packages (0.8.0)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n","Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n","Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.7.0+cu101)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n","Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.8)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.11.8)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bnDeYuA_LnXJ","executionInfo":{"status":"ok","timestamp":1607030685175,"user_tz":360,"elapsed":5658,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["from collections import defaultdict\n","import nltk\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction, corpus_bleu\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import re\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm.notebook import tqdm\n","import unicodedata\n","\n","if __name__ == '__main__':\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":45,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zQ4KX9-QTZcI"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"tUeHdn_uTfZ1","executionInfo":{"status":"ok","timestamp":1607030685176,"user_tz":360,"elapsed":5657,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Converts the unicode file to ascii\n","def unicode_to_ascii(s):\n","    \"\"\"Normalizes latin chars with accent to their canonical decomposition\"\"\"\n","    return ''.join(c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn')\n","\n","# Preprocessing the sentence to add the start, end tokens and make them lower-case\n","def preprocess_sentence(w):\n","    w = unicode_to_ascii(w.lower().strip())\n","    w = re.sub(r'([?.!,¿])', r' \\1 ', w)\n","    w = re.sub(r'[\" \"]+', ' ', w)\n","    w = re.sub(r'[^\\w\\s]', '', w) \n","\n","    w = re.sub(r'[^a-zA-Z?.!,¿]+', ' ', w)\n","    \n","    w = w.rstrip().strip()\n","    w = '<start> ' + w + ' <end>'\n","    return w\n","\n","def max_length(tensor):\n","    return max(len(t) for t in tensor)\n","\n","def pad_sequences(x, max_len):\n","    padded = np.zeros((max_len), dtype=np.int64)\n","    if len(x) > max_len:\n","        padded[:] = x[:max_len]\n","    else:\n","        padded[:len(x)] = x\n","    return padded\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2indexFull = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n","        self.word2countFull = {\"<start>\": 1e10, \"<end>\": 1e10, \"<unk>\": 1e10, \"<pad>\": 1e10}\n","        self.index2wordFull = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n","        self.word2index = {}\n","        self.index2word = {}\n","        self.n_wordsFull = 4  # Count SOS and EOS\n","        self.n_words = 0\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2indexFull:\n","            self.word2indexFull[word] = self.n_wordsFull\n","            self.word2countFull[word] = 1\n","            self.index2wordFull[self.n_wordsFull] = word\n","            self.n_wordsFull += 1\n","        else:\n","            self.word2countFull[word] += 1\n","\n","    def reduceDictionary(self, threshold = 50):\n","        n_words = 0\n","        for word in self.word2indexFull.keys():\n","            if self.word2countFull[word] >= threshold:\n","                self.word2index[word] = n_words\n","                self.index2word[n_words] = word\n","                n_words += 1\n","        self.n_words = n_words\n","    \n","    def sentence2Index(self, sentence):\n","        output = []\n","        for word in sentence.split(' '):\n","            if word in self.word2index.keys():\n","                output.append(self.word2index[word])\n","            else:\n","                output.append(self.word2index[\"<unk>\"])\n","        return output\n","\n","def build_dataset(target_language, threshold):\n","    # Load in and process sentences\n","    lines = open(target_language+'.txt', encoding='UTF-8').read().strip().split('\\n')\n","    original_word_pairs = [[w for w in l.split('\\t')][:2] for l in lines]\n","\n","    data = pd.DataFrame(original_word_pairs, columns=['eng', target_language])\n","    data['eng'] = data.eng.apply(lambda w: preprocess_sentence(w))\n","    data[target_language] = data[target_language].apply(lambda w: preprocess_sentence(w))\n","\n","    # Remove all sentences with length longer than 10 (+ 2 for start/end)\n","    data['len_eng'] = data.eng.apply(lambda w: len(w.split(\" \")))\n","    data['len_'+target_language] = data[target_language].apply(lambda w: len(w.split(\" \")))\n","    data = data[(data['len_eng'] <= MAX_LEN + 2)*(data['len_'+target_language] <= MAX_LEN + 2)]\n","    data = data[['eng',target_language]]\n","\n","    # Build language dictionaries \n","    input_lang = Lang('eng')\n","    output_lang = Lang(target_language)\n","    for sentence in data['eng']:\n","      input_lang.addSentence(sentence)\n","\n","    for sentence in data[target_language]:\n","      output_lang.addSentence(sentence)\n","    input_lang.reduceDictionary(threshold)\n","    output_lang.reduceDictionary(threshold)\n","\n","    data['eng'] = data.eng.apply(lambda w: input_lang.sentence2Index(w))\n","    data[target_language] = data[target_language].apply(lambda w: output_lang.sentence2Index(w))\n","    data['eng'] = data.eng.apply(lambda w: pad_sequences(w, MAX_LEN + 2))\n","    data[target_language] = data[target_language].apply(lambda w: pad_sequences(w, MAX_LEN + 2))\n","\n","    # Filter out sentences that have more than 1 (10%) UNK\n","    eng_filter = data['eng'].apply(lambda w: np.sum(w == input_lang.word2index['<unk>']))\n","    target_filter = data[target_language].apply(lambda w: np.sum(w == output_lang.word2index['<unk>']))\n","    data = data[(eng_filter <= MAX_UNK) * (target_filter <= MAX_UNK)]\n","\n","    return input_lang, output_lang, data"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bJBt9wUfT3Jo","executionInfo":{"status":"ok","timestamp":1607030688441,"user_tz":360,"elapsed":8919,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"200394d2-de93-43bb-ab78-2faedb7ec2fd"},"source":["!wget http://www.manythings.org/anki/fra-eng.zip\n","!unzip -o fra-eng.zip"],"execution_count":47,"outputs":[{"output_type":"stream","text":["--2020-12-03 21:24:44--  http://www.manythings.org/anki/fra-eng.zip\n","Resolving www.manythings.org (www.manythings.org)... 104.24.108.196, 172.67.173.198, 104.24.109.196, ...\n","Connecting to www.manythings.org (www.manythings.org)|104.24.108.196|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6129887 (5.8M) [application/zip]\n","Saving to: ‘fra-eng.zip.2’\n","\n","fra-eng.zip.2       100%[===================>]   5.85M  1.91MB/s    in 3.1s    \n","\n","2020-12-03 21:24:47 (1.91 MB/s) - ‘fra-eng.zip.2’ saved [6129887/6129887]\n","\n","Archive:  fra-eng.zip\n","  inflating: _about.txt              \n","  inflating: fra.txt                 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zkGty2WyUFNT","executionInfo":{"status":"ok","timestamp":1607030700942,"user_tz":360,"elapsed":21417,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"240fc466-d708-442e-e265-2bafeb1c9c49"},"source":["MAX_LEN = 10 # (+2 for <start>, <end>)\n","MAX_UNK = 1000 \n","THRESHOLD = 100\n","input_lang, output_lang, data = build_dataset('fra', THRESHOLD) #\n","print(\"Input words {}, Output words {}, N sentences {}\".format(input_lang.n_words, output_lang.n_words, data.shape[0]))\n","print(data)"],"execution_count":48,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/computation/expressions.py:204: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n","  f\"evaluating in Python space because the {repr(op_str)} \"\n"],"name":"stderr"},{"output_type":"stream","text":["Input words 889, Output words 967, N sentences 162842\n","                                                     eng                                               fra\n","0                   [1, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]              [1, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","1                   [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]              [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","2                   [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]              [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","3                   [1, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]              [1, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","4                   [1, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]              [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","...                                                  ...                                               ...\n","177819            [1, 3, 3, 3, 3, 695, 3, 3, 2, 0, 0, 0]          [1, 140, 3, 3, 3, 3, 693, 3, 3, 2, 0, 0]\n","177836    [1, 3, 175, 355, 815, 3, 355, 3, 180, 3, 3, 2]            [1, 3, 80, 3, 76, 3, 3, 3, 2, 0, 0, 0]\n","177939  [1, 695, 3, 339, 677, 3, 3, 887, 242, 3, 841, 2]       [1, 785, 3, 11, 3, 3, 11, 661, 43, 3, 3, 2]\n","178470         [1, 3, 3, 3, 424, 695, 3, 3, 22, 3, 2, 0]       [1, 140, 3, 3, 3, 3, 693, 3, 3, 45, 762, 2]\n","179020  [1, 671, 3, 72, 3, 180, 355, 3, 655, 22, 406, 2]  [1, 411, 3, 3, 50, 76, 674, 506, 76, 90, 505, 2]\n","\n","[162842 rows x 2 columns]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/computation/expressions.py:204: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n","  f\"evaluating in Python space because the {repr(op_str)} \"\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"MRYEK7LNLUgE"},"source":["# Models"]},{"cell_type":"code","metadata":{"id":"-fFrQ7qnLSNd","executionInfo":{"status":"ok","timestamp":1607030700943,"user_tz":360,"elapsed":21416,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Encoder (Takes a sentence seq_len -> returns output, hidden)\n","class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers=1):\n","        super(EncoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, num_layers = num_layers)\n","\n","    def forward(self, input_sentence):\n","        embedded = self.embedding(input_sentence)\n","        output, hidden = self.gru(embedded)  \n","\n","        # For deep\n","        hidden = hidden[-1].unsqueeze(0)         \n","        return output, hidden"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"RqjwRA2jRAcr","executionInfo":{"status":"ok","timestamp":1607030700944,"user_tz":360,"elapsed":21414,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Decoder (Takes a sentence seq_len -> returns output)\n","class DecoderRNN(nn.Module):\n","    def __init__(self, output_size, hidden_size):\n","        super(DecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim = 2)\n","\n","    def forward(self, input_sentence, hidden, encoder_output):\n","        embedded = self.embedding(input_sentence)              \n","        output, decoder_hidden = self.gru(embedded, hidden)                    \n","        output = self.out(output).squeeze(0)                              \n","        return output, decoder_hidden"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jex7T-VWurHx","executionInfo":{"status":"ok","timestamp":1607030700945,"user_tz":360,"elapsed":21414,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Decoder with attention (Takes a sentence seq_len -> returns output)\n","class AttentionDecoderRNN(nn.Module):\n","    def __init__(self, output_size, hidden_size):\n","        super(AttentionDecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","\n","        self.score = nn.Sequential(\n","            nn.Linear(2 * hidden_size, hidden_size),\n","            nn.Tanh(),\n","            nn.Linear(hidden_size, 1)\n","        )\n","\n","        self.attention = nn.Sequential(\n","            nn.Linear(2 * hidden_size, hidden_size),\n","            nn.Tanh()\n","        )\n","\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim = 2)\n","\n","    def forward(self, input_sentence, hidden, encoder_output):\n","        embedded = self.embedding(input_sentence) # [1, batch_size, ]\n","\n","        # Compute score vector\n","        score_vector = self.score(\n","            torch.cat([torch.cat((MAX_LEN + 2)*[hidden]), encoder_output], dim = 2)\n","        ).squeeze(-1) # [seq_len, batch_size] \n","\n","        # Compute attention weights\n","        attention_weights = F.softmax(score_vector, dim = 0) # [seq_len, batch_size]\n","\n","        # Compute context vector\n","        context_vector = torch.einsum('sb, sbh -> bh', attention_weights, encoder_output) # [batch_size, hidden_size]\n","\n","        # Compute attention vector\n","        attention_vector = self.attention(torch.cat([context_vector.unsqueeze(0), embedded], dim = 2))\n","\n","        # Pass into decoder\n","        output, decoder_hidden = self.gru(attention_vector, hidden)                    \n","        output = self.out(output).squeeze(0)                              \n","        return output, decoder_hidden"],"execution_count":51,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zOxsduH8c-Yt"},"source":["#  Training"]},{"cell_type":"code","metadata":{"id":"nWq4Mfj6dB7C","executionInfo":{"status":"ok","timestamp":1607030700945,"user_tz":360,"elapsed":21412,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["def translate_sentence(encoder, decoder, pair, ref_lang, targ_lang):\n","    \"\"\"\n","    Translate single sentence, returns\n","\n","    reference\n","    target\n","    candidate\n","    \"\"\"\n","    test_loss = 0\n","    candidate = []\n","    with torch.no_grad():\n","        reference = torch.tensor(pair[0]).unsqueeze(1).to(device)\n","        target =    torch.tensor(pair[1]).unsqueeze(1).to(device)\n","\n","        # Encoder pass\n","        encoder_output, encoder_hidden = encoder(reference) \n","  \n","        # Decoder pass\n","        decoder_input = target[0].unsqueeze(0)\n","        decoder_hidden = encoder_hidden\n","        candidate.append(decoder_input)\n","        for j in range(1, len(target)):\n","            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output) \n","            test_loss += loss_function(target[j], decoder_output) / len(target)\n","            decoder_input = F.log_softmax(decoder_output.unsqueeze(0), dim=-1).argmax(dim = -1)\n","            candidate.append(decoder_input)\n","            if decoder_input == targ_lang.word2index['<end>']:\n","                break\n","    \n","    reference = reference[reference > 0]\n","    target = target[target > 0]\n","\n","    reference = [ref_lang.index2word[int(s)] for s in reference]\n","    target =    [targ_lang.index2word[int(s)] for s in target]\n","    candidate = [targ_lang.index2word[int(s)] for s in candidate]\n","\n","    smoother = SmoothingFunction()\n","    bleu1 = sentence_bleu([target[1:]], candidate[1:], weights=(1,), smoothing_function=smoother.method1)\n","    bleu2 = sentence_bleu([target[1:]], candidate[1:], weights=(1/2, 1/2), smoothing_function=smoother.method1)\n","    bleu3 = sentence_bleu([target[1:]], candidate[1:], weights=(1/3, 1/3, 1/3), smoothing_function=smoother.method1)\n","    bleu4 = sentence_bleu([target[1:]], candidate[1:], weights=(1/4, 1/4, 1/4, 1/4), smoothing_function=smoother.method1)\n","    return bleu1, bleu2, bleu3, bleu4, test_loss, \" \".join(reference), \" \".join(target), \" \".join(candidate)\n","\n","def loss_function(real, pred):\n","    \"\"\" Only consider non-pad inputs in the loss; mask needed \"\"\"\n","    mask = real.ge(1).float()\n","    \n","    loss_ = F.cross_entropy(pred, real) * mask \n","    return torch.mean(loss_)\n","\n","def train_model(encoder, decoder, targ_lang, train, num_epochs, learning_rate, batch_size, breakp = 1e10):\n","    # Return training losses\n","    losses = []\n","    \n","    # Model, optimizer, criterion\n","    encoder = encoder.to(device)\n","    decoder = decoder.to(device)\n","    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n","\n","    # Build batches\n","    batches = [df for g, df in train.groupby(np.arange(len(train)) // batch_size)]\n","\n","    # Train\n","    for i in range(num_epochs):\n","        epoch_loss = 0\n","        for b in range(len(batches)):\n","            batch = batches[b]\n","            if len(batch) == batch_size: # Discard partial batches\n","                target = torch.tensor([s for s in batch[list(batch)[1]]]).T.to(device)\n","                reference = torch.tensor([s for s in batch[list(batch)[0]]]).T.to(device)\n","\n","                # Encoder pass: [max_len, batch_size, hidden_size], [1, batch_size, hidden_size]\n","                encoder_output, encoder_hidden = encoder(reference) \n","\n","                # Decoder pass: teacher forcing\n","                loss = 0\n","                decoder_input = target[0].unsqueeze(0) # [1, batch_size]\n","                decoder_hidden = encoder_hidden\n","                for j in range(1, len(target)):\n","                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output) # [batch_size, output_size], [1, batch_size, hidden_size]\n","                    loss += loss_function(target[j], decoder_output)\n","                    decoder_input = target[j].unsqueeze(0)\n","\n","                # Step\n","                loss.backward()\n","                encoder_optimizer.step()\n","                decoder_optimizer.step()\n","                encoder_optimizer.zero_grad()\n","                decoder_optimizer.zero_grad()\n","\n","                # Prints\n","                epoch_loss += loss.item() / (len(target) * len(batches))\n","\n","        # Training losses\n","        print(\"EPOCH {}/{}, LOSS {}\".format(i + 1, num_epochs, epoch_loss))\n","        losses.append(epoch_loss)\n","    return losses "],"execution_count":52,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9R9Gj0aKS7lw"},"source":["# Base model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XeFcWwFpqpLW","executionInfo":{"status":"ok","timestamp":1607030700947,"user_tz":360,"elapsed":21412,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"c34340a2-45cd-4460-bd9c-503dc3ae540b"},"source":["# Train test split\n","data = data.sample(frac = 1, replace = False)\n","train = data.iloc[:data.shape[0]//4 * 3]\n","test = data.iloc[data.shape[0]//4 * 3:]\n","\n","# Model\n","HIDDEN_DIM = 128\n","encoder = EncoderRNN(input_lang.n_words, HIDDEN_DIM)\n","decoder = DecoderRNN(output_lang.n_words, HIDDEN_DIM)\n","print(\"N Params: \", sum(p.numel() for p in encoder.parameters()) + sum(p.numel() for p in decoder.parameters()))"],"execution_count":53,"outputs":[{"output_type":"stream","text":["N Params:  560455\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KLv65YtkgxlA","executionInfo":{"status":"ok","timestamp":1607030966549,"user_tz":360,"elapsed":287012,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"d8d2bc77-8be6-47ac-eaa1-66653d7d1617"},"source":["losses = train_model(encoder, decoder, output_lang, train, 200, 1e-3, len(train)//9)"],"execution_count":54,"outputs":[{"output_type":"stream","text":["EPOCH 1/200, LOSS 3.6613569259643555\n","EPOCH 2/200, LOSS 2.8943136179888693\n","EPOCH 3/200, LOSS 2.3863480885823565\n","EPOCH 4/200, LOSS 2.249343165644893\n","EPOCH 5/200, LOSS 2.1819099496912076\n","EPOCH 6/200, LOSS 2.1327357115568937\n","EPOCH 7/200, LOSS 2.091776388662833\n","EPOCH 8/200, LOSS 2.05290193910952\n","EPOCH 9/200, LOSS 2.0155962308247886\n","EPOCH 10/200, LOSS 1.9790974722968209\n","EPOCH 11/200, LOSS 1.9432794606244124\n","EPOCH 12/200, LOSS 1.9081185835379144\n","EPOCH 13/200, LOSS 1.873607423570421\n","EPOCH 14/200, LOSS 1.8397090170118544\n","EPOCH 15/200, LOSS 1.8065095830846718\n","EPOCH 16/200, LOSS 1.7742526796129015\n","EPOCH 17/200, LOSS 1.7431057470816154\n","EPOCH 18/200, LOSS 1.7129359951725711\n","EPOCH 19/200, LOSS 1.6834054523044162\n","EPOCH 20/200, LOSS 1.6545926376625344\n","EPOCH 21/200, LOSS 1.6269185101544417\n","EPOCH 22/200, LOSS 1.600545741893627\n","EPOCH 23/200, LOSS 1.5754491841351546\n","EPOCH 24/200, LOSS 1.5514535197505246\n","EPOCH 25/200, LOSS 1.5284248104801883\n","EPOCH 26/200, LOSS 1.5064075434649433\n","EPOCH 27/200, LOSS 1.4854042088543928\n","EPOCH 28/200, LOSS 1.4655300069738315\n","EPOCH 29/200, LOSS 1.4465489387512207\n","EPOCH 30/200, LOSS 1.4284870359632704\n","EPOCH 31/200, LOSS 1.4110062387254505\n","EPOCH 32/200, LOSS 1.3943198345325611\n","EPOCH 33/200, LOSS 1.378243464010733\n","EPOCH 34/200, LOSS 1.3624555799696179\n","EPOCH 35/200, LOSS 1.3474328606216994\n","EPOCH 36/200, LOSS 1.3328177487408674\n","EPOCH 37/200, LOSS 1.3186343687551993\n","EPOCH 38/200, LOSS 1.3049405857368752\n","EPOCH 39/200, LOSS 1.2914911376105414\n","EPOCH 40/200, LOSS 1.2785941494835749\n","EPOCH 41/200, LOSS 1.2656298831657127\n","EPOCH 42/200, LOSS 1.2530910703870985\n","EPOCH 43/200, LOSS 1.241060036200064\n","EPOCH 44/200, LOSS 1.2291130666379575\n","EPOCH 45/200, LOSS 1.2175772543306704\n","EPOCH 46/200, LOSS 1.206815445864642\n","EPOCH 47/200, LOSS 1.1957711731946028\n","EPOCH 48/200, LOSS 1.1851604956167716\n","EPOCH 49/200, LOSS 1.174739714022036\n","EPOCH 50/200, LOSS 1.1646083107701055\n","EPOCH 51/200, LOSS 1.154803894184254\n","EPOCH 52/200, LOSS 1.1453075850451435\n","EPOCH 53/200, LOSS 1.1359157915468567\n","EPOCH 54/200, LOSS 1.1267656573542844\n","EPOCH 55/200, LOSS 1.1179239838211625\n","EPOCH 56/200, LOSS 1.1092842243335865\n","EPOCH 57/200, LOSS 1.1008925173017714\n","EPOCH 58/200, LOSS 1.092937637258459\n","EPOCH 59/200, LOSS 1.0852609828666404\n","EPOCH 60/200, LOSS 1.0770265614544903\n","EPOCH 61/200, LOSS 1.0695512383072465\n","EPOCH 62/200, LOSS 1.0620026941652652\n","EPOCH 63/200, LOSS 1.054817658883554\n","EPOCH 64/200, LOSS 1.0476652957774975\n","EPOCH 65/200, LOSS 1.0407292666258634\n","EPOCH 66/200, LOSS 1.0339786829771818\n","EPOCH 67/200, LOSS 1.027266502380371\n","EPOCH 68/200, LOSS 1.0206497686880607\n","EPOCH 69/200, LOSS 1.01422083819354\n","EPOCH 70/200, LOSS 1.0080526758123327\n","EPOCH 71/200, LOSS 1.0021717371764007\n","EPOCH 72/200, LOSS 0.9961751125476977\n","EPOCH 73/200, LOSS 0.9901200312155265\n","EPOCH 74/200, LOSS 0.9843393255163122\n","EPOCH 75/200, LOSS 0.978718881253843\n","EPOCH 76/200, LOSS 0.973123930118702\n","EPOCH 77/200, LOSS 0.9676267041100397\n","EPOCH 78/200, LOSS 0.9621640223043936\n","EPOCH 79/200, LOSS 0.9567727512783475\n","EPOCH 80/200, LOSS 0.9515988385235822\n","EPOCH 81/200, LOSS 0.9464902436291729\n","EPOCH 82/200, LOSS 0.941325646859628\n","EPOCH 83/200, LOSS 0.9364235666063097\n","EPOCH 84/200, LOSS 0.931413032390453\n","EPOCH 85/200, LOSS 0.9266725469518592\n","EPOCH 86/200, LOSS 0.921929359436035\n","EPOCH 87/200, LOSS 0.9171602637679488\n","EPOCH 88/200, LOSS 0.912490138301143\n","EPOCH 89/200, LOSS 0.9080335034264457\n","EPOCH 90/200, LOSS 0.9035418210206209\n","EPOCH 91/200, LOSS 0.899013810687595\n","EPOCH 92/200, LOSS 0.894540133299651\n","EPOCH 93/200, LOSS 0.8901531961229113\n","EPOCH 94/200, LOSS 0.8857779061352764\n","EPOCH 95/200, LOSS 0.8814274823224103\n","EPOCH 96/200, LOSS 0.8771632689016837\n","EPOCH 97/200, LOSS 0.8730180351822465\n","EPOCH 98/200, LOSS 0.8689660319575557\n","EPOCH 99/200, LOSS 0.8650273393701625\n","EPOCH 100/200, LOSS 0.8611882792578802\n","EPOCH 101/200, LOSS 0.8573742354357684\n","EPOCH 102/200, LOSS 0.8536517884996202\n","EPOCH 103/200, LOSS 0.8500484713801632\n","EPOCH 104/200, LOSS 0.846364736557007\n","EPOCH 105/200, LOSS 0.8427416218651667\n","EPOCH 106/200, LOSS 0.8390900647198714\n","EPOCH 107/200, LOSS 0.8356342403977005\n","EPOCH 108/200, LOSS 0.8322931042423956\n","EPOCH 109/200, LOSS 0.8291059070163302\n","EPOCH 110/200, LOSS 0.8260288768344456\n","EPOCH 111/200, LOSS 0.822923810393722\n","EPOCH 112/200, LOSS 0.8200702137417264\n","EPOCH 113/200, LOSS 0.8178801536560057\n","EPOCH 114/200, LOSS 0.8162310035140425\n","EPOCH 115/200, LOSS 0.8111970601258455\n","EPOCH 116/200, LOSS 0.8070245760458488\n","EPOCH 117/200, LOSS 0.8033308188120525\n","EPOCH 118/200, LOSS 0.8004565680468524\n","EPOCH 119/200, LOSS 0.7972476129178647\n","EPOCH 120/200, LOSS 0.7943844795227051\n","EPOCH 121/200, LOSS 0.7915336202692103\n","EPOCH 122/200, LOSS 0.7886547689084653\n","EPOCH 123/200, LOSS 0.7859144034209073\n","EPOCH 124/200, LOSS 0.7831629823755335\n","EPOCH 125/200, LOSS 0.7804444277728045\n","EPOCH 126/200, LOSS 0.7777963920875833\n","EPOCH 127/200, LOSS 0.7751920134932906\n","EPOCH 128/200, LOSS 0.7726137726395218\n","EPOCH 129/200, LOSS 0.7700684865315756\n","EPOCH 130/200, LOSS 0.7675782927760372\n","EPOCH 131/200, LOSS 0.7651660707261827\n","EPOCH 132/200, LOSS 0.7628553973303901\n","EPOCH 133/200, LOSS 0.7606477737426759\n","EPOCH 134/200, LOSS 0.7585442242798983\n","EPOCH 135/200, LOSS 0.7565108493522361\n","EPOCH 136/200, LOSS 0.7540789356938116\n","EPOCH 137/200, LOSS 0.7514443927341038\n","EPOCH 138/200, LOSS 0.748846036416513\n","EPOCH 139/200, LOSS 0.7464424680780481\n","EPOCH 140/200, LOSS 0.7440985926875362\n","EPOCH 141/200, LOSS 0.741732570860121\n","EPOCH 142/200, LOSS 0.7395390669504802\n","EPOCH 143/200, LOSS 0.7373502519395616\n","EPOCH 144/200, LOSS 0.7351545316201669\n","EPOCH 145/200, LOSS 0.7330841223398844\n","EPOCH 146/200, LOSS 0.7312132959012633\n","EPOCH 147/200, LOSS 0.7293714064138908\n","EPOCH 148/200, LOSS 0.7275791698031956\n","EPOCH 149/200, LOSS 0.7257189838974564\n","EPOCH 150/200, LOSS 0.723540050012094\n","EPOCH 151/200, LOSS 0.7213244879687274\n","EPOCH 152/200, LOSS 0.7192925700434932\n","EPOCH 153/200, LOSS 0.7174162776381882\n","EPOCH 154/200, LOSS 0.7155220773484973\n","EPOCH 155/200, LOSS 0.7137518282289858\n","EPOCH 156/200, LOSS 0.7121186521318225\n","EPOCH 157/200, LOSS 0.7105612048396357\n","EPOCH 158/200, LOSS 0.7089774608612062\n","EPOCH 159/200, LOSS 0.7073982821570504\n","EPOCH 160/200, LOSS 0.705525654333609\n","EPOCH 161/200, LOSS 0.7035915674986662\n","EPOCH 162/200, LOSS 0.7016078012960927\n","EPOCH 163/200, LOSS 0.699913501739502\n","EPOCH 164/200, LOSS 0.6980422779365822\n","EPOCH 165/200, LOSS 0.6961552831861707\n","EPOCH 166/200, LOSS 0.6943999926249186\n","EPOCH 167/200, LOSS 0.6927058785050003\n","EPOCH 168/200, LOSS 0.6910783008292869\n","EPOCH 169/200, LOSS 0.6895280414157443\n","EPOCH 170/200, LOSS 0.6879863562407317\n","EPOCH 171/200, LOSS 0.6864350548496954\n","EPOCH 172/200, LOSS 0.6848987826594599\n","EPOCH 173/200, LOSS 0.6834092758319996\n","EPOCH 174/200, LOSS 0.6820176795676902\n","EPOCH 175/200, LOSS 0.6806481502674244\n","EPOCH 176/200, LOSS 0.6792966083244043\n","EPOCH 177/200, LOSS 0.677996626606694\n","EPOCH 178/200, LOSS 0.6766928478523537\n","EPOCH 179/200, LOSS 0.6753821814501726\n","EPOCH 180/200, LOSS 0.6741061607996622\n","EPOCH 181/200, LOSS 0.6728651876802798\n","EPOCH 182/200, LOSS 0.6716781324810452\n","EPOCH 183/200, LOSS 0.6705377808323612\n","EPOCH 184/200, LOSS 0.6693243627195005\n","EPOCH 185/200, LOSS 0.6680797559243661\n","EPOCH 186/200, LOSS 0.6666619336163557\n","EPOCH 187/200, LOSS 0.6653207408057319\n","EPOCH 188/200, LOSS 0.6643165040899206\n","EPOCH 189/200, LOSS 0.6635920427463674\n","EPOCH 190/200, LOSS 0.6618580288357206\n","EPOCH 191/200, LOSS 0.6594960557089912\n","EPOCH 192/200, LOSS 0.6573121503547387\n","EPOCH 193/200, LOSS 0.6553373601701524\n","EPOCH 194/200, LOSS 0.6536342567867702\n","EPOCH 195/200, LOSS 0.6521980144359447\n","EPOCH 196/200, LOSS 0.6509680571379485\n","EPOCH 197/200, LOSS 0.6498900166264286\n","EPOCH 198/200, LOSS 0.6489408899236608\n","EPOCH 199/200, LOSS 0.6480055076104624\n","EPOCH 200/200, LOSS 0.6470573875639175\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NkNIfoBzDXKq"},"source":["# Base model + attention"]},{"cell_type":"code","metadata":{"id":"H6oXqCrmDXKq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607030966552,"user_tz":360,"elapsed":287013,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"07e5eca8-9165-4869-f9a4-97a942465cdd"},"source":["# Model\n","HIDDEN_DIM = 128\n","attention_encoder = EncoderRNN(input_lang.n_words, HIDDEN_DIM)\n","attention_decoder = AttentionDecoderRNN(output_lang.n_words, HIDDEN_DIM)\n","print(\"N Params: \", sum(p.numel() for p in attention_encoder.parameters()) + sum(p.numel() for p in attention_decoder.parameters()))"],"execution_count":55,"outputs":[{"output_type":"stream","text":["N Params:  626376\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g-JUGP1nDXKr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607031543864,"user_tz":360,"elapsed":864324,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"0e08cf68-778e-4b21-8f6f-462929ab9409"},"source":["attention_losses = train_model(attention_encoder, attention_decoder, output_lang, train, 200, 1e-3, len(train)//9)"],"execution_count":56,"outputs":[{"output_type":"stream","text":["EPOCH 1/200, LOSS 3.6425755465472185\n","EPOCH 2/200, LOSS 2.7799287018952548\n","EPOCH 3/200, LOSS 2.3978167110019264\n","EPOCH 4/200, LOSS 2.2717767114992493\n","EPOCH 5/200, LOSS 2.2185578522858798\n","EPOCH 6/200, LOSS 2.1813001102871366\n","EPOCH 7/200, LOSS 2.1497724321153426\n","EPOCH 8/200, LOSS 2.1190560128953724\n","EPOCH 9/200, LOSS 2.087126449302391\n","EPOCH 10/200, LOSS 2.0539993356775352\n","EPOCH 11/200, LOSS 2.019984421906648\n","EPOCH 12/200, LOSS 1.9850193129645455\n","EPOCH 13/200, LOSS 1.9500687210648149\n","EPOCH 14/200, LOSS 1.9156200267650463\n","EPOCH 15/200, LOSS 1.8821571138170028\n","EPOCH 16/200, LOSS 1.849751772703948\n","EPOCH 17/200, LOSS 1.8179233692310475\n","EPOCH 18/200, LOSS 1.7866047223409016\n","EPOCH 19/200, LOSS 1.755139386212384\n","EPOCH 20/200, LOSS 1.7239023491188332\n","EPOCH 21/200, LOSS 1.6915789356938116\n","EPOCH 22/200, LOSS 1.6591481279443812\n","EPOCH 23/200, LOSS 1.6259211964077418\n","EPOCH 24/200, LOSS 1.5929033844559282\n","EPOCH 25/200, LOSS 1.5584635204739041\n","EPOCH 26/200, LOSS 1.5235639324894656\n","EPOCH 27/200, LOSS 1.4876541208337855\n","EPOCH 28/200, LOSS 1.4514404579445168\n","EPOCH 29/200, LOSS 1.4157005769235116\n","EPOCH 30/200, LOSS 1.38105222913954\n","EPOCH 31/200, LOSS 1.3485235108269586\n","EPOCH 32/200, LOSS 1.3181264841998066\n","EPOCH 33/200, LOSS 1.289461374282837\n","EPOCH 34/200, LOSS 1.2628787093692355\n","EPOCH 35/200, LOSS 1.2378465246271202\n","EPOCH 36/200, LOSS 1.2130359013875323\n","EPOCH 37/200, LOSS 1.1896167154665345\n","EPOCH 38/200, LOSS 1.1672259083500616\n","EPOCH 39/200, LOSS 1.1457478293666135\n","EPOCH 40/200, LOSS 1.125269095102946\n","EPOCH 41/200, LOSS 1.105508874963831\n","EPOCH 42/200, LOSS 1.085913481535735\n","EPOCH 43/200, LOSS 1.0679304864671495\n","EPOCH 44/200, LOSS 1.0500933064354792\n","EPOCH 45/200, LOSS 1.033458250540274\n","EPOCH 46/200, LOSS 1.0168295524738453\n","EPOCH 47/200, LOSS 1.000882457803797\n","EPOCH 48/200, LOSS 0.9858527625048602\n","EPOCH 49/200, LOSS 0.9712099234263102\n","EPOCH 50/200, LOSS 0.957220642654984\n","EPOCH 51/200, LOSS 0.9440735092869512\n","EPOCH 52/200, LOSS 0.9315028014006439\n","EPOCH 53/200, LOSS 0.919926228346648\n","EPOCH 54/200, LOSS 0.9071248990518075\n","EPOCH 55/200, LOSS 0.8953366279602051\n","EPOCH 56/200, LOSS 0.8834538371474655\n","EPOCH 57/200, LOSS 0.8718752596113417\n","EPOCH 58/200, LOSS 0.8607683888188116\n","EPOCH 59/200, LOSS 0.8502612378862169\n","EPOCH 60/200, LOSS 0.8401395391534875\n","EPOCH 61/200, LOSS 0.8303739229838054\n","EPOCH 62/200, LOSS 0.8209051114541511\n","EPOCH 63/200, LOSS 0.8145856415783919\n","EPOCH 64/200, LOSS 0.8065275262903284\n","EPOCH 65/200, LOSS 0.7956240088851363\n","EPOCH 66/200, LOSS 0.7902577894705312\n","EPOCH 67/200, LOSS 0.7858530856944896\n","EPOCH 68/200, LOSS 0.7767885261111791\n","EPOCH 69/200, LOSS 0.7660853156337032\n","EPOCH 70/200, LOSS 0.757630674927323\n","EPOCH 71/200, LOSS 0.7508538122530337\n","EPOCH 72/200, LOSS 0.743787447611491\n","EPOCH 73/200, LOSS 0.7372032448097512\n","EPOCH 74/200, LOSS 0.731072469993874\n","EPOCH 75/200, LOSS 0.724921252992418\n","EPOCH 76/200, LOSS 0.7188420295715332\n","EPOCH 77/200, LOSS 0.712965311827483\n","EPOCH 78/200, LOSS 0.7073254673569291\n","EPOCH 79/200, LOSS 0.7019110608983923\n","EPOCH 80/200, LOSS 0.696684095594618\n","EPOCH 81/200, LOSS 0.6915996162979692\n","EPOCH 82/200, LOSS 0.6866782771216499\n","EPOCH 83/200, LOSS 0.6819473902384441\n","EPOCH 84/200, LOSS 0.6774722381874368\n","EPOCH 85/200, LOSS 0.673473768764072\n","EPOCH 86/200, LOSS 0.6706535860344216\n","EPOCH 87/200, LOSS 0.6697279259010598\n","EPOCH 88/200, LOSS 0.6692024778436731\n","EPOCH 89/200, LOSS 0.658415922412166\n","EPOCH 90/200, LOSS 0.6531283281467578\n","EPOCH 91/200, LOSS 0.6486217048433093\n","EPOCH 92/200, LOSS 0.6444701177102549\n","EPOCH 93/200, LOSS 0.6402575837241278\n","EPOCH 94/200, LOSS 0.6367251873016357\n","EPOCH 95/200, LOSS 0.6330374170232702\n","EPOCH 96/200, LOSS 0.6293747645837289\n","EPOCH 97/200, LOSS 0.6259026262495253\n","EPOCH 98/200, LOSS 0.6226540318241826\n","EPOCH 99/200, LOSS 0.619589262538486\n","EPOCH 100/200, LOSS 0.6167159742779201\n","EPOCH 101/200, LOSS 0.6142429378297594\n","EPOCH 102/200, LOSS 0.6125722240518641\n","EPOCH 103/200, LOSS 0.6109740778251931\n","EPOCH 104/200, LOSS 0.6097097396850586\n","EPOCH 105/200, LOSS 0.6046535836325752\n","EPOCH 106/200, LOSS 0.5999874715451842\n","EPOCH 107/200, LOSS 0.5972459669466372\n","EPOCH 108/200, LOSS 0.5943282489423398\n","EPOCH 109/200, LOSS 0.5915405308758771\n","EPOCH 110/200, LOSS 0.5889501041836209\n","EPOCH 111/200, LOSS 0.5866071692219487\n","EPOCH 112/200, LOSS 0.5844244250544796\n","EPOCH 113/200, LOSS 0.5824655824237399\n","EPOCH 114/200, LOSS 0.5813100293830589\n","EPOCH 115/200, LOSS 0.5796183480156792\n","EPOCH 116/200, LOSS 0.5777834609702781\n","EPOCH 117/200, LOSS 0.5744657339873137\n","EPOCH 118/200, LOSS 0.5717806771949486\n","EPOCH 119/200, LOSS 0.5689149830076429\n","EPOCH 120/200, LOSS 0.5666715657269513\n","EPOCH 121/200, LOSS 0.5646809030462194\n","EPOCH 122/200, LOSS 0.5626892999366477\n","EPOCH 123/200, LOSS 0.5605996714697943\n","EPOCH 124/200, LOSS 0.558990995089213\n","EPOCH 125/200, LOSS 0.5574558266886959\n","EPOCH 126/200, LOSS 0.5563725497987535\n","EPOCH 127/200, LOSS 0.5547895166609023\n","EPOCH 128/200, LOSS 0.5521947896039044\n","EPOCH 129/200, LOSS 0.5498288251735547\n","EPOCH 130/200, LOSS 0.548152451161985\n","EPOCH 131/200, LOSS 0.5460327289722584\n","EPOCH 132/200, LOSS 0.5443375110626221\n","EPOCH 133/200, LOSS 0.5429827195626719\n","EPOCH 134/200, LOSS 0.5419500933753119\n","EPOCH 135/200, LOSS 0.5405916549541332\n","EPOCH 136/200, LOSS 0.5392537867581403\n","EPOCH 137/200, LOSS 0.5378736257553101\n","EPOCH 138/200, LOSS 0.5374255621874774\n","EPOCH 139/200, LOSS 0.5356091084303679\n","EPOCH 140/200, LOSS 0.533863279554579\n","EPOCH 141/200, LOSS 0.5327420322983354\n","EPOCH 142/200, LOSS 0.5310971781059548\n","EPOCH 143/200, LOSS 0.5293921629587809\n","EPOCH 144/200, LOSS 0.5281458739881162\n","EPOCH 145/200, LOSS 0.5266653784999141\n","EPOCH 146/200, LOSS 0.5246569801259924\n","EPOCH 147/200, LOSS 0.5227891295044511\n","EPOCH 148/200, LOSS 0.5212209798671581\n","EPOCH 149/200, LOSS 0.5196046387707746\n","EPOCH 150/200, LOSS 0.5177173791108308\n","EPOCH 151/200, LOSS 0.5162554891021163\n","EPOCH 152/200, LOSS 0.5153339880484122\n","EPOCH 153/200, LOSS 0.5148913595411512\n","EPOCH 154/200, LOSS 0.5130677179053977\n","EPOCH 155/200, LOSS 0.5116380055745443\n","EPOCH 156/200, LOSS 0.5112040793454206\n","EPOCH 157/200, LOSS 0.5099253257115682\n","EPOCH 158/200, LOSS 0.5083583858278062\n","EPOCH 159/200, LOSS 0.5077902255234895\n","EPOCH 160/200, LOSS 0.507209923532274\n","EPOCH 161/200, LOSS 0.505781540164241\n","EPOCH 162/200, LOSS 0.5047828797940854\n","EPOCH 163/200, LOSS 0.5044273623713741\n","EPOCH 164/200, LOSS 0.5031316545274522\n","EPOCH 165/200, LOSS 0.5021118632069341\n","EPOCH 166/200, LOSS 0.5027442419970477\n","EPOCH 167/200, LOSS 0.5031551343423348\n","EPOCH 168/200, LOSS 0.50682975186242\n","EPOCH 169/200, LOSS 0.5157720645268757\n","EPOCH 170/200, LOSS 0.532429434635021\n","EPOCH 171/200, LOSS 0.510862425521568\n","EPOCH 172/200, LOSS 0.49979869966153756\n","EPOCH 173/200, LOSS 0.49761188913274695\n","EPOCH 174/200, LOSS 0.4903198657212434\n","EPOCH 175/200, LOSS 0.4902113631919579\n","EPOCH 176/200, LOSS 0.490024760917381\n","EPOCH 177/200, LOSS 0.48781065587644223\n","EPOCH 178/200, LOSS 0.48501262841401277\n","EPOCH 179/200, LOSS 0.4828925883328473\n","EPOCH 180/200, LOSS 0.482063576027199\n","EPOCH 181/200, LOSS 0.48239457607269287\n","EPOCH 182/200, LOSS 0.4835292454119082\n","EPOCH 183/200, LOSS 0.48553377169149897\n","EPOCH 184/200, LOSS 0.4874361709312156\n","EPOCH 185/200, LOSS 0.4858534689302798\n","EPOCH 186/200, LOSS 0.4825330707761976\n","EPOCH 187/200, LOSS 0.4787651167975532\n","EPOCH 188/200, LOSS 0.47838449919665305\n","EPOCH 189/200, LOSS 0.4777624474631415\n","EPOCH 190/200, LOSS 0.47622293896145296\n","EPOCH 191/200, LOSS 0.47470510888982703\n","EPOCH 192/200, LOSS 0.4735435688937151\n","EPOCH 193/200, LOSS 0.4731720156139797\n","EPOCH 194/200, LOSS 0.4737707729692812\n","EPOCH 195/200, LOSS 0.47479100580568667\n","EPOCH 196/200, LOSS 0.47513035050144903\n","EPOCH 197/200, LOSS 0.4737923895871197\n","EPOCH 198/200, LOSS 0.472357639559993\n","EPOCH 199/200, LOSS 0.4698434759069372\n","EPOCH 200/200, LOSS 0.46912811420581957\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AB7hW9IPS3gM"},"source":["# Base model + attention + deep encoder"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OFreqqb-S3gM","executionInfo":{"status":"ok","timestamp":1607033427023,"user_tz":360,"elapsed":1078,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"d77c61ab-40a3-4deb-acdc-d7fd57ffbe9b"},"source":["# Model\n","import copy\n","HIDDEN_DIM = 128\n","deep_encoder = EncoderRNN(input_lang.n_words, HIDDEN_DIM, 2)\n","deep_decoder = copy.deepcopy(attention_decoder) #AttentionDecoderRNN(output_lang.n_words, HIDDEN_DIM)\n","print(\"N Params: \", sum(p.numel() for p in deep_encoder.parameters()) + sum(p.numel() for p in deep_decoder.parameters()))"],"execution_count":61,"outputs":[{"output_type":"stream","text":["N Params:  725448\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qwXlbKStS3gM","executionInfo":{"status":"ok","timestamp":1607034052074,"user_tz":360,"elapsed":616827,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"819502cc-6d23-4669-9d0d-1bd9f5764f07"},"source":["deep_losses = train_model(deep_encoder, deep_decoder, output_lang, train, 200, 1e-3, len(train)//9)"],"execution_count":62,"outputs":[{"output_type":"stream","text":["EPOCH 1/200, LOSS 2.4983376397026906\n","EPOCH 2/200, LOSS 1.8655337934140808\n","EPOCH 3/200, LOSS 1.7140528361002605\n","EPOCH 4/200, LOSS 1.6248614876358596\n","EPOCH 5/200, LOSS 1.5608257717556424\n","EPOCH 6/200, LOSS 1.5054278550324616\n","EPOCH 7/200, LOSS 1.4514395042702006\n","EPOCH 8/200, LOSS 1.3966765933566625\n","EPOCH 9/200, LOSS 1.3393685994324862\n","EPOCH 10/200, LOSS 1.28068145999202\n","EPOCH 11/200, LOSS 1.2248569947701915\n","EPOCH 12/200, LOSS 1.172552470807676\n","EPOCH 13/200, LOSS 1.1232557561662464\n","EPOCH 14/200, LOSS 1.0773366645530418\n","EPOCH 15/200, LOSS 1.035304758283827\n","EPOCH 16/200, LOSS 0.9971522401880335\n","EPOCH 17/200, LOSS 0.9627924406969989\n","EPOCH 18/200, LOSS 0.9316566520267062\n","EPOCH 19/200, LOSS 0.9032287067837185\n","EPOCH 20/200, LOSS 0.8772410640010128\n","EPOCH 21/200, LOSS 0.853540199774283\n","EPOCH 22/200, LOSS 0.8318958194167526\n","EPOCH 23/200, LOSS 0.812050351390132\n","EPOCH 24/200, LOSS 0.7937350008222792\n","EPOCH 25/200, LOSS 0.7767820799792255\n","EPOCH 26/200, LOSS 0.7613199993416115\n","EPOCH 27/200, LOSS 0.7472492765497277\n","EPOCH 28/200, LOSS 0.7349522378709581\n","EPOCH 29/200, LOSS 0.7227477409221508\n","EPOCH 30/200, LOSS 0.7141356909716572\n","EPOCH 31/200, LOSS 0.703214830822415\n","EPOCH 32/200, LOSS 0.6926689942677816\n","EPOCH 33/200, LOSS 0.6833046718879983\n","EPOCH 34/200, LOSS 0.6754134584356237\n","EPOCH 35/200, LOSS 0.6678712147253532\n","EPOCH 36/200, LOSS 0.6606588142889518\n","EPOCH 37/200, LOSS 0.6539576848347981\n","EPOCH 38/200, LOSS 0.6482007326903166\n","EPOCH 39/200, LOSS 0.6446211735407511\n","EPOCH 40/200, LOSS 0.6364346698478416\n","EPOCH 41/200, LOSS 0.630649659368727\n","EPOCH 42/200, LOSS 0.6250697595101817\n","EPOCH 43/200, LOSS 0.6202513332720156\n","EPOCH 44/200, LOSS 0.6161409969683047\n","EPOCH 45/200, LOSS 0.6114406762299713\n","EPOCH 46/200, LOSS 0.6132809983359443\n","EPOCH 47/200, LOSS 0.6056483321719699\n","EPOCH 48/200, LOSS 0.6011168956756591\n","EPOCH 49/200, LOSS 0.596834244551482\n","EPOCH 50/200, LOSS 0.5931654241349962\n","EPOCH 51/200, LOSS 0.5902201643696539\n","EPOCH 52/200, LOSS 0.5878240223284121\n","EPOCH 53/200, LOSS 0.5867027220902619\n","EPOCH 54/200, LOSS 0.5856617468374747\n","EPOCH 55/200, LOSS 0.5810149025034022\n","EPOCH 56/200, LOSS 0.574599305788676\n","EPOCH 57/200, LOSS 0.5722692630909108\n","EPOCH 58/200, LOSS 0.5684931145773994\n","EPOCH 59/200, LOSS 0.5658852391772801\n","EPOCH 60/200, LOSS 0.5634890176631786\n","EPOCH 61/200, LOSS 0.5609707081759417\n","EPOCH 62/200, LOSS 0.5583701486940738\n","EPOCH 63/200, LOSS 0.5557545953326755\n","EPOCH 64/200, LOSS 0.553156817400897\n","EPOCH 65/200, LOSS 0.550613279695864\n","EPOCH 66/200, LOSS 0.5481515857908461\n","EPOCH 67/200, LOSS 0.5457792679468791\n","EPOCH 68/200, LOSS 0.5434968957194576\n","EPOCH 69/200, LOSS 0.5412842741719\n","EPOCH 70/200, LOSS 0.5391255926202845\n","EPOCH 71/200, LOSS 0.5370146610118725\n","EPOCH 72/200, LOSS 0.5349446446807297\n","EPOCH 73/200, LOSS 0.5329137554875126\n","EPOCH 74/200, LOSS 0.5309215342556989\n","EPOCH 75/200, LOSS 0.5289482363948116\n","EPOCH 76/200, LOSS 0.5269853229875917\n","EPOCH 77/200, LOSS 0.530559085033558\n","EPOCH 78/200, LOSS 0.5257321772751984\n","EPOCH 79/200, LOSS 0.5230187972386678\n","EPOCH 80/200, LOSS 0.5203791591856215\n","EPOCH 81/200, LOSS 0.5186009583649812\n","EPOCH 82/200, LOSS 0.516610896145856\n","EPOCH 83/200, LOSS 0.5147878064049615\n","EPOCH 84/200, LOSS 0.513331205756576\n","EPOCH 85/200, LOSS 0.5127149643721404\n","EPOCH 86/200, LOSS 0.51133362893705\n","EPOCH 87/200, LOSS 0.510796308517456\n","EPOCH 88/200, LOSS 0.5083488402543244\n","EPOCH 89/200, LOSS 0.5053258427867183\n","EPOCH 90/200, LOSS 0.5041661968937626\n","EPOCH 91/200, LOSS 0.502384799498099\n","EPOCH 92/200, LOSS 0.5007951127158271\n","EPOCH 93/200, LOSS 0.49984124413243053\n","EPOCH 94/200, LOSS 0.49876731854897954\n","EPOCH 95/200, LOSS 0.49945844544304746\n","EPOCH 96/200, LOSS 0.4972055973830046\n","EPOCH 97/200, LOSS 0.49676663345760774\n","EPOCH 98/200, LOSS 0.49415319054215046\n","EPOCH 99/200, LOSS 0.49246335471117936\n","EPOCH 100/200, LOSS 0.49064204427931046\n","EPOCH 101/200, LOSS 0.4892152062168828\n","EPOCH 102/200, LOSS 0.48806340606124315\n","EPOCH 103/200, LOSS 0.48707553192421243\n","EPOCH 104/200, LOSS 0.48635472633220533\n","EPOCH 105/200, LOSS 0.4878201882044474\n","EPOCH 106/200, LOSS 0.4961411599759702\n","EPOCH 107/200, LOSS 0.4921129721182364\n","EPOCH 108/200, LOSS 0.4826864887166906\n","EPOCH 109/200, LOSS 0.48022016772517445\n","EPOCH 110/200, LOSS 0.47883884112040204\n","EPOCH 111/200, LOSS 0.4771559194282249\n","EPOCH 112/200, LOSS 0.4762874311870999\n","EPOCH 113/200, LOSS 0.4752783554571646\n","EPOCH 114/200, LOSS 0.47387253796612777\n","EPOCH 115/200, LOSS 0.47208027486447934\n","EPOCH 116/200, LOSS 0.4703977461214419\n","EPOCH 117/200, LOSS 0.46894491601873334\n","EPOCH 118/200, LOSS 0.4676085489767569\n","EPOCH 119/200, LOSS 0.4663254243356211\n","EPOCH 120/200, LOSS 0.4650654351269757\n","EPOCH 121/200, LOSS 0.4638263958471793\n","EPOCH 122/200, LOSS 0.46261974617286966\n","EPOCH 123/200, LOSS 0.46146138950630466\n","EPOCH 124/200, LOSS 0.46036428433877447\n","EPOCH 125/200, LOSS 0.45933283258367463\n","EPOCH 126/200, LOSS 0.4583638111750285\n","EPOCH 127/200, LOSS 0.45746709682323317\n","EPOCH 128/200, LOSS 0.4566691628208867\n","EPOCH 129/200, LOSS 0.4560340245564778\n","EPOCH 130/200, LOSS 0.45572967882509585\n","EPOCH 131/200, LOSS 0.45627524234630445\n","EPOCH 132/200, LOSS 0.46004788522367124\n","EPOCH 133/200, LOSS 0.46701187116128434\n","EPOCH 134/200, LOSS 0.46134440987198433\n","EPOCH 135/200, LOSS 0.45491577519310844\n","EPOCH 136/200, LOSS 0.45196035173204213\n","EPOCH 137/200, LOSS 0.4502083018974022\n","EPOCH 138/200, LOSS 0.44823836838757547\n","EPOCH 139/200, LOSS 0.4470120756714432\n","EPOCH 140/200, LOSS 0.4458463545198793\n","EPOCH 141/200, LOSS 0.44472555760984067\n","EPOCH 142/200, LOSS 0.44366961055331755\n","EPOCH 143/200, LOSS 0.44271018328490075\n","EPOCH 144/200, LOSS 0.4418340788947212\n","EPOCH 145/200, LOSS 0.4410112610569707\n","EPOCH 146/200, LOSS 0.4402268241952967\n","EPOCH 147/200, LOSS 0.4395014091774269\n","EPOCH 148/200, LOSS 0.43887254485377564\n","EPOCH 149/200, LOSS 0.4384378062354194\n","EPOCH 150/200, LOSS 0.438243309656779\n","EPOCH 151/200, LOSS 0.4383384695759526\n","EPOCH 152/200, LOSS 0.440067520848027\n","EPOCH 153/200, LOSS 0.4413247373369005\n","EPOCH 154/200, LOSS 0.4377659956614176\n","EPOCH 155/200, LOSS 0.4337274365954929\n","EPOCH 156/200, LOSS 0.43252908741986307\n","EPOCH 157/200, LOSS 0.43146974952132616\n","EPOCH 158/200, LOSS 0.42994267410702175\n","EPOCH 159/200, LOSS 0.42885393566555446\n","EPOCH 160/200, LOSS 0.42789357238345677\n","EPOCH 161/200, LOSS 0.427115555162783\n","EPOCH 162/200, LOSS 0.4265889724095663\n","EPOCH 163/200, LOSS 0.42583861615922713\n","EPOCH 164/200, LOSS 0.42548393320154254\n","EPOCH 165/200, LOSS 0.4258122135091711\n","EPOCH 166/200, LOSS 0.42475692431131995\n","EPOCH 167/200, LOSS 0.4242773276788217\n","EPOCH 168/200, LOSS 0.42119392200752537\n","EPOCH 169/200, LOSS 0.4196942426540234\n","EPOCH 170/200, LOSS 0.41895444746370664\n","EPOCH 171/200, LOSS 0.4179078296378807\n","EPOCH 172/200, LOSS 0.4169283133965952\n","EPOCH 173/200, LOSS 0.416494572604144\n","EPOCH 174/200, LOSS 0.41609110655608\n","EPOCH 175/200, LOSS 0.41690390639834934\n","EPOCH 176/200, LOSS 0.4173538861451325\n","EPOCH 177/200, LOSS 0.41619351616612194\n","EPOCH 178/200, LOSS 0.4144015003133703\n","EPOCH 179/200, LOSS 0.41397971577114534\n","EPOCH 180/200, LOSS 0.41265232033199734\n","EPOCH 181/200, LOSS 0.41164426450376157\n","EPOCH 182/200, LOSS 0.4111477269066705\n","EPOCH 183/200, LOSS 0.4114518607104266\n","EPOCH 184/200, LOSS 0.41127374437120223\n","EPOCH 185/200, LOSS 0.41055767624466505\n","EPOCH 186/200, LOSS 0.4098973450837312\n","EPOCH 187/200, LOSS 0.41018826873214154\n","EPOCH 188/200, LOSS 0.4090039818375199\n","EPOCH 189/200, LOSS 0.40741191528461596\n","EPOCH 190/200, LOSS 0.4056830538643731\n","EPOCH 191/200, LOSS 0.4055963842957108\n","EPOCH 192/200, LOSS 0.4051205846998427\n","EPOCH 193/200, LOSS 0.4039844539430406\n","EPOCH 194/200, LOSS 0.4030942916870117\n","EPOCH 195/200, LOSS 0.4039528458206742\n","EPOCH 196/200, LOSS 0.4039597599594681\n","EPOCH 197/200, LOSS 0.4026125757782547\n","EPOCH 198/200, LOSS 0.40176094902886283\n","EPOCH 199/200, LOSS 0.4029731485578749\n","EPOCH 200/200, LOSS 0.4020451660509463\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r_2HbO1bUJ1H"},"source":["# Model comparisons"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"BlZrm8JKS3gN","executionInfo":{"status":"ok","timestamp":1607034055572,"user_tz":360,"elapsed":1222,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"e8dab6ba-fbed-4171-9fa8-5ec0da15fb68"},"source":["import matplotlib.pyplot as plt\n","plt.plot(losses, '-', label = 'Base')\n","plt.plot(attention_losses, '-', label = 'Attention')\n","plt.plot(deep_losses, '-', label = 'Deep Attention')\n","plt.ylim((0, 5))\n","plt.xlabel('Epoch')\n","plt.ylabel('CE Loss')\n","plt.legend(loc='best')\n","plt.show()"],"execution_count":63,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7T3MMe-S3gN","executionInfo":{"status":"ok","timestamp":1607032998996,"user_tz":360,"elapsed":2319449,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"767fc415-7635-4d75-eb9b-8845d1c6ea37"},"source":["for e, d in zip(\n","        [encoder, attention_encoder, deep_encoder],\n","        [decoder, attention_decoder, deep_decoder],\n","    ):\n","    total_bleu1 = 0\n","    total_bleu2 = 0\n","    total_bleu3 = 0\n","    total_bleu4 = 0\n","    for ind in range(test.shape[0]):\n","        bleu1, bleu2, bleu3, bleu4, loss, r, t, c = translate_sentence(e,\n","                                                                      d, \n","                                                                      test.iloc[ind], \n","                                                                      input_lang, \n","                                                                      output_lang)\n","        total_bleu1 += bleu1 * 100 / test.shape[0]\n","        total_bleu2 += bleu2 * 100 / test.shape[0]\n","        total_bleu3 += bleu3 * 100 / test.shape[0]\n","        total_bleu4 += bleu4 * 100 / test.shape[0]\n","        if ind < 10: \n","            print(r, \"|\", t, \"|\", c)\n","    print(\"{:.3f}, {:.3f}, {:.3f}, {:.3f}\".format(total_bleu1, total_bleu2, total_bleu3, total_bleu4))"],"execution_count":60,"outputs":[{"output_type":"stream","text":["<start> this could hurt our business <end> | <start> cela pourrait <unk> a notre <unk> <end> | <start> ca pourrait <unk> un coup de temps <end>\n","<start> we have to give tom a chance <end> | <start> nous devons donner une chance a tom <end> | <start> nous devons aller a tom de <unk> <end>\n","<start> she took me for my brother <end> | <start> elle ma <unk> avec mon frere <end> | <start> elle ma <unk> ma vie <end>\n","<start> he asked for some money <end> | <start> il demanda un peu dargent <end> | <start> il a demande un peu dargent <end>\n","<start> why does everybody love cats <end> | <start> pourquoi tout le monde aime les chats <end> | <start> pourquoi les <unk> <unk> <unk> <end>\n","<start> can you <unk> <end> | <start> savezvous <unk> <end> | <start> pouvezvous <unk> <end>\n","<start> everything is broken <end> | <start> tout est casse <end> | <start> tout est <unk> <end>\n","<start> were <unk> <end> | <start> nous sommes <unk> de <unk> <end> | <start> nous sommes <unk> <end>\n","<start> what do i look like <end> | <start> a quoi je <unk> <end> | <start> que <unk> <end>\n","<start> tom <unk> his seat <unk> <end> | <start> tom <unk> sa <unk> de securite <end> | <start> tom a <unk> ses <unk> <end>\n","58.261, 40.780, 29.350, 23.052\n","<start> this could hurt our business <end> | <start> cela pourrait <unk> a notre <unk> <end> | <start> ca pourrait faire <unk> notre affaire <end>\n","<start> we have to give tom a chance <end> | <start> nous devons donner une chance a tom <end> | <start> nous devons <unk> une chance <end>\n","<start> she took me for my brother <end> | <start> elle ma <unk> avec mon frere <end> | <start> elle ma <unk> mon frere <end>\n","<start> he asked for some money <end> | <start> il demanda un peu dargent <end> | <start> il a demande de largent <end>\n","<start> why does everybody love cats <end> | <start> pourquoi tout le monde aime les chats <end> | <start> pourquoi tout le monde <unk> les chats <end>\n","<start> can you <unk> <end> | <start> savezvous <unk> <end> | <start> pouvezvous <unk> <end>\n","<start> everything is broken <end> | <start> tout est casse <end> | <start> tout est <unk> <end>\n","<start> were <unk> <end> | <start> nous sommes <unk> de <unk> <end> | <start> nous sommes <unk> <end>\n","<start> what do i look like <end> | <start> a quoi je <unk> <end> | <start> que <unk> <end>\n","<start> tom <unk> his seat <unk> <end> | <start> tom <unk> sa <unk> de securite <end> | <start> tom a <unk> sa <unk> <end>\n","67.889, 53.907, 42.483, 34.856\n","<start> this could hurt our business <end> | <start> cela pourrait <unk> a notre <unk> <end> | <start> ca pourrait apprendre notre <unk> <end>\n","<start> we have to give tom a chance <end> | <start> nous devons donner une chance a tom <end> | <start> nous devons <unk> une chance <end>\n","<start> she took me for my brother <end> | <start> elle ma <unk> avec mon frere <end> | <start> elle ma <unk> pour mon frere <end>\n","<start> he asked for some money <end> | <start> il demanda un peu dargent <end> | <start> il <unk> de largent <end>\n","<start> why does everybody love cats <end> | <start> pourquoi tout le monde aime les chats <end> | <start> pourquoi estce que tout le monde est <unk> <end>\n","<start> can you <unk> <end> | <start> savezvous <unk> <end> | <start> peuxtu <unk> <end>\n","<start> everything is broken <end> | <start> tout est casse <end> | <start> tout est <unk> <end>\n","<start> were <unk> <end> | <start> nous sommes <unk> de <unk> <end> | <start> nous sommes <unk> <end>\n","<start> what do i look like <end> | <start> a quoi je <unk> <end> | <start> que <unk> <unk> <end>\n","<start> tom <unk> his seat <unk> <end> | <start> tom <unk> sa <unk> de securite <end> | <start> tom <unk> son <unk> <end>\n","67.108, 52.915, 41.509, 33.933\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TcRRwGjwrPF0","executionInfo":{"status":"ok","timestamp":1607034397559,"user_tz":360,"elapsed":324449,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"e832b2f2-4cf3-42df-d326-2fb36a30ce7c"},"source":["for e, d in zip(\n","        [deep_encoder],\n","        [deep_decoder],\n","    ):\n","    total_bleu1 = 0\n","    total_bleu2 = 0\n","    total_bleu3 = 0\n","    total_bleu4 = 0\n","    for ind in range(test.shape[0]):\n","        bleu1, bleu2, bleu3, bleu4, loss, r, t, c = translate_sentence(e,\n","                                                                      d, \n","                                                                      test.iloc[ind], \n","                                                                      input_lang, \n","                                                                      output_lang)\n","        total_bleu1 += bleu1 * 100 / test.shape[0]\n","        total_bleu2 += bleu2 * 100 / test.shape[0]\n","        total_bleu3 += bleu3 * 100 / test.shape[0]\n","        total_bleu4 += bleu4 * 100 / test.shape[0]\n","        if ind < 10: \n","            print(r, \"|\", t, \"|\", c)\n","    print(\"{:.3f}, {:.3f}, {:.3f}, {:.3f}\".format(total_bleu1, total_bleu2, total_bleu3, total_bleu4))"],"execution_count":64,"outputs":[{"output_type":"stream","text":["<start> this could hurt our business <end> | <start> cela pourrait <unk> a notre <unk> <end> | <start> ca pourrait faire de nos affaires <end>\n","<start> we have to give tom a chance <end> | <start> nous devons donner une chance a tom <end> | <start> nous devons <unk> tom <unk> <end>\n","<start> she took me for my brother <end> | <start> elle ma <unk> avec mon frere <end> | <start> elle ma <unk> pour mon frere <end>\n","<start> he asked for some money <end> | <start> il demanda un peu dargent <end> | <start> il a demande de largent <end>\n","<start> why does everybody love cats <end> | <start> pourquoi tout le monde aime les chats <end> | <start> pourquoi estce que tout le monde <unk> des chats <end>\n","<start> can you <unk> <end> | <start> savezvous <unk> <end> | <start> peuxtu <unk> <end>\n","<start> everything is broken <end> | <start> tout est casse <end> | <start> tout est <unk> <end>\n","<start> were <unk> <end> | <start> nous sommes <unk> de <unk> <end> | <start> nous sommes <unk> <end>\n","<start> what do i look like <end> | <start> a quoi je <unk> <end> | <start> que <unk> <end>\n","<start> tom <unk> his seat <unk> <end> | <start> tom <unk> sa <unk> de securite <end> | <start> tom <unk> son <unk> <end>\n","69.058, 55.411, 44.059, 36.338\n"],"name":"stdout"}]}]}