{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cs447project_english_german.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNhM8jkStEchOl0giENI4pH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bnqyBxYPLyG6","executionInfo":{"status":"ok","timestamp":1607032667780,"user_tz":360,"elapsed":6726,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"4b9ebbf4-3822-4230-9c7d-82abdef1b7a4"},"source":["# PyTorch \n","!pip install --upgrade torch\n","!pip install --upgrade torchtext"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n","Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n","Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Collecting torchtext\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/23/8499af6d9c22b29b01f66a2c11d38ce71cd1cafa2655913c29818ed4a00f/torchtext-0.8.0-cp36-cp36m-manylinux1_x86_64.whl (6.9MB)\n","\u001b[K     |████████████████████████████████| 6.9MB 10.1MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n","Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.7.0+cu101)\n","Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.8)\n","Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.11.8)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n","Installing collected packages: torchtext\n","  Found existing installation: torchtext 0.3.1\n","    Uninstalling torchtext-0.3.1:\n","      Successfully uninstalled torchtext-0.3.1\n","Successfully installed torchtext-0.8.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bnDeYuA_LnXJ","executionInfo":{"status":"ok","timestamp":1607032671175,"user_tz":360,"elapsed":10114,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["from collections import defaultdict\n","import nltk\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction, corpus_bleu\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import re\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm.notebook import tqdm\n","import unicodedata\n","\n","if __name__ == '__main__':\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zQ4KX9-QTZcI"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"tUeHdn_uTfZ1","executionInfo":{"status":"ok","timestamp":1607032671176,"user_tz":360,"elapsed":10111,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Converts the unicode file to ascii\n","def unicode_to_ascii(s):\n","    \"\"\"Normalizes latin chars with accent to their canonical decomposition\"\"\"\n","    return ''.join(c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn')\n","\n","# Preprocessing the sentence to add the start, end tokens and make them lower-case\n","def preprocess_sentence(w):\n","    w = unicode_to_ascii(w.lower().strip())\n","    w = re.sub(r'([?.!,¿])', r' \\1 ', w)\n","    w = re.sub(r'[\" \"]+', ' ', w)\n","    w = re.sub(r'[^\\w\\s]', '', w) \n","\n","    w = re.sub(r'[^a-zA-Z?.!,¿]+', ' ', w)\n","    \n","    w = w.rstrip().strip()\n","    w = '<start> ' + w + ' <end>'\n","    return w\n","\n","def max_length(tensor):\n","    return max(len(t) for t in tensor)\n","\n","def pad_sequences(x, max_len):\n","    padded = np.zeros((max_len), dtype=np.int64)\n","    if len(x) > max_len:\n","        padded[:] = x[:max_len]\n","    else:\n","        padded[:len(x)] = x\n","    return padded\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2indexFull = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n","        self.word2countFull = {\"<start>\": 1e10, \"<end>\": 1e10, \"<unk>\": 1e10, \"<pad>\": 1e10}\n","        self.index2wordFull = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n","        self.word2index = {}\n","        self.index2word = {}\n","        self.n_wordsFull = 4  # Count SOS and EOS\n","        self.n_words = 0\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2indexFull:\n","            self.word2indexFull[word] = self.n_wordsFull\n","            self.word2countFull[word] = 1\n","            self.index2wordFull[self.n_wordsFull] = word\n","            self.n_wordsFull += 1\n","        else:\n","            self.word2countFull[word] += 1\n","\n","    def reduceDictionary(self, threshold = 50):\n","        n_words = 0\n","        for word in self.word2indexFull.keys():\n","            if self.word2countFull[word] >= threshold:\n","                self.word2index[word] = n_words\n","                self.index2word[n_words] = word\n","                n_words += 1\n","        self.n_words = n_words\n","    \n","    def sentence2Index(self, sentence):\n","        output = []\n","        for word in sentence.split(' '):\n","            if word in self.word2index.keys():\n","                output.append(self.word2index[word])\n","            else:\n","                output.append(self.word2index[\"<unk>\"])\n","        return output\n","\n","def build_dataset(target_language, threshold):\n","    # Load in and process sentences\n","    lines = open(target_language+'.txt', encoding='UTF-8').read().strip().split('\\n')\n","    original_word_pairs = [[w for w in l.split('\\t')][:2] for l in lines]\n","\n","    data = pd.DataFrame(original_word_pairs, columns=['eng', target_language])\n","    data['eng'] = data.eng.apply(lambda w: preprocess_sentence(w))\n","    data[target_language] = data[target_language].apply(lambda w: preprocess_sentence(w))\n","\n","    # Remove all sentences with length longer than 10 (+ 2 for start/end)\n","    data['len_eng'] = data.eng.apply(lambda w: len(w.split(\" \")))\n","    data['len_'+target_language] = data[target_language].apply(lambda w: len(w.split(\" \")))\n","    data = data[(data['len_eng'] <= MAX_LEN + 2)*(data['len_'+target_language] <= MAX_LEN + 2)]\n","    data = data[['eng',target_language]]\n","\n","    # Build language dictionaries \n","    input_lang = Lang('eng')\n","    output_lang = Lang(target_language)\n","    for sentence in data['eng']:\n","      input_lang.addSentence(sentence)\n","\n","    for sentence in data[target_language]:\n","      output_lang.addSentence(sentence)\n","    input_lang.reduceDictionary(threshold)\n","    output_lang.reduceDictionary(threshold)\n","\n","    data['eng'] = data.eng.apply(lambda w: input_lang.sentence2Index(w))\n","    data[target_language] = data[target_language].apply(lambda w: output_lang.sentence2Index(w))\n","    data['eng'] = data.eng.apply(lambda w: pad_sequences(w, MAX_LEN + 2))\n","    data[target_language] = data[target_language].apply(lambda w: pad_sequences(w, MAX_LEN + 2))\n","\n","    # Filter out sentences that have more than 1 (10%) UNK\n","    eng_filter = data['eng'].apply(lambda w: np.sum(w == input_lang.word2index['<unk>']))\n","    target_filter = data[target_language].apply(lambda w: np.sum(w == output_lang.word2index['<unk>']))\n","    data = data[(eng_filter <= MAX_UNK) * (target_filter <= MAX_UNK)]\n","\n","    return input_lang, output_lang, data"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bJBt9wUfT3Jo","executionInfo":{"status":"ok","timestamp":1607032673436,"user_tz":360,"elapsed":12369,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"1940a185-d2fc-44b6-ad37-977654bab37e"},"source":["!wget http://www.manythings.org/anki/deu-eng.zip\n","!unzip -o deu-eng.zip"],"execution_count":4,"outputs":[{"output_type":"stream","text":["--2020-12-03 21:57:51--  http://www.manythings.org/anki/deu-eng.zip\n","Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 172.67.173.198, ...\n","Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 8440213 (8.0M) [application/zip]\n","Saving to: ‘deu-eng.zip’\n","\n","deu-eng.zip         100%[===================>]   8.05M  6.16MB/s    in 1.3s    \n","\n","2020-12-03 21:57:52 (6.16 MB/s) - ‘deu-eng.zip’ saved [8440213/8440213]\n","\n","Archive:  deu-eng.zip\n","  inflating: deu.txt                 \n","  inflating: _about.txt              \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zkGty2WyUFNT","executionInfo":{"status":"ok","timestamp":1607032687884,"user_tz":360,"elapsed":26815,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"befb1939-82e9-492d-d445-08c836a26ceb"},"source":["MAX_LEN = 10 # (+2 for <start>, <end>)\n","MAX_UNK = 1000 \n","THRESHOLD = 100\n","input_lang, output_lang, data = build_dataset('deu', THRESHOLD) #\n","print(\"Input words {}, Output words {}, N sentences {}\".format(input_lang.n_words, output_lang.n_words, data.shape[0]))\n","print(data)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/computation/expressions.py:204: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n","  f\"evaluating in Python space because the {repr(op_str)} \"\n"],"name":"stderr"},{"output_type":"stream","text":["Input words 1078, Output words 1163, N sentences 204544\n","                                                      eng                                               deu\n","0                    [1, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]              [1, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","1                    [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]              [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","2                    [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]              [1, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0]\n","3                    [1, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]              [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","4                    [1, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]              [1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","...                                                   ...                                               ...\n","222001     [1, 438, 3, 3, 3, 136, 823, 345, 438, 3, 3, 2]     [1, 186, 3, 3, 27, 186, 3, 3, 334, 943, 2, 0]\n","222105  [1, 983, 781, 653, 3, 44, 510, 3, 136, 3, 220, 2]  [1, 371, 3, 3, 50, 67, 633, 1053, 7, 1035, 3, 2]\n","222191          [1, 3, 3, 3, 3, 79, 983, 3, 159, 3, 2, 0]   [1, 3, 3, 268, 19, 77, 600, 371, 3, 1017, 3, 2]\n","222477           [1, 3, 3, 3, 485, 3, 275, 3, 3, 3, 2, 0]         [1, 3, 3, 3, 338, 3, 3, 479, 55, 3, 3, 2]\n","222886   [1, 775, 3, 79, 3, 183, 438, 3, 747, 27, 565, 2]       [1, 33, 3, 39, 3, 53, 3, 778, 881, 2, 0, 0]\n","\n","[204544 rows x 2 columns]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/computation/expressions.py:204: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n","  f\"evaluating in Python space because the {repr(op_str)} \"\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"MRYEK7LNLUgE"},"source":["# Models"]},{"cell_type":"code","metadata":{"id":"-fFrQ7qnLSNd","executionInfo":{"status":"ok","timestamp":1607032687886,"user_tz":360,"elapsed":26814,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Encoder (Takes a sentence seq_len -> returns output, hidden)\n","class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers=1):\n","        super(EncoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, num_layers = num_layers)\n","\n","    def forward(self, input_sentence):\n","        embedded = self.embedding(input_sentence)\n","        output, hidden = self.gru(embedded)  \n","\n","        # For deep\n","        hidden = hidden[-1].unsqueeze(0)         \n","        return output, hidden"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"RqjwRA2jRAcr","executionInfo":{"status":"ok","timestamp":1607032687888,"user_tz":360,"elapsed":26815,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Decoder (Takes a sentence seq_len -> returns output)\n","class DecoderRNN(nn.Module):\n","    def __init__(self, output_size, hidden_size):\n","        super(DecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim = 2)\n","\n","    def forward(self, input_sentence, hidden, encoder_output):\n","        embedded = self.embedding(input_sentence)              \n","        output, decoder_hidden = self.gru(embedded, hidden)                    \n","        output = self.out(output).squeeze(0)                              \n","        return output, decoder_hidden"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jex7T-VWurHx","executionInfo":{"status":"ok","timestamp":1607032687889,"user_tz":360,"elapsed":26814,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["# Decoder with attention (Takes a sentence seq_len -> returns output)\n","class AttentionDecoderRNN(nn.Module):\n","    def __init__(self, output_size, hidden_size):\n","        super(AttentionDecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","\n","        self.score = nn.Sequential(\n","            nn.Linear(2 * hidden_size, hidden_size),\n","            nn.Tanh(),\n","            nn.Linear(hidden_size, 1)\n","        )\n","\n","        self.attention = nn.Sequential(\n","            nn.Linear(2 * hidden_size, hidden_size),\n","            nn.Tanh()\n","        )\n","\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim = 2)\n","\n","    def forward(self, input_sentence, hidden, encoder_output):\n","        embedded = self.embedding(input_sentence) # [1, batch_size, ]\n","\n","        # Compute score vector\n","        score_vector = self.score(\n","            torch.cat([torch.cat((MAX_LEN + 2)*[hidden]), encoder_output], dim = 2)\n","        ).squeeze(-1) # [seq_len, batch_size] \n","\n","        # Compute attention weights\n","        attention_weights = F.softmax(score_vector, dim = 0) # [seq_len, batch_size]\n","\n","        # Compute context vector\n","        context_vector = torch.einsum('sb, sbh -> bh', attention_weights, encoder_output) # [batch_size, hidden_size]\n","\n","        # Compute attention vector\n","        attention_vector = self.attention(torch.cat([context_vector.unsqueeze(0), embedded], dim = 2))\n","\n","        # Pass into decoder\n","        output, decoder_hidden = self.gru(attention_vector, hidden)                    \n","        output = self.out(output).squeeze(0)                              \n","        return output, decoder_hidden"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zOxsduH8c-Yt"},"source":["#  Training"]},{"cell_type":"code","metadata":{"id":"nWq4Mfj6dB7C","executionInfo":{"status":"ok","timestamp":1607032687890,"user_tz":360,"elapsed":26812,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}}},"source":["def translate_sentence(encoder, decoder, pair, ref_lang, targ_lang):\n","    \"\"\"\n","    Translate single sentence, returns\n","\n","    reference\n","    target\n","    candidate\n","    \"\"\"\n","    test_loss = 0\n","    candidate = []\n","    with torch.no_grad():\n","        reference = torch.tensor(pair[0]).unsqueeze(1).to(device)\n","        target =    torch.tensor(pair[1]).unsqueeze(1).to(device)\n","\n","        # Encoder pass\n","        encoder_output, encoder_hidden = encoder(reference) \n","  \n","        # Decoder pass\n","        decoder_input = target[0].unsqueeze(0)\n","        decoder_hidden = encoder_hidden\n","        candidate.append(decoder_input)\n","        for j in range(1, len(target)):\n","            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output) \n","            test_loss += loss_function(target[j], decoder_output) / len(target)\n","            decoder_input = F.log_softmax(decoder_output.unsqueeze(0), dim=-1).argmax(dim = -1)\n","            candidate.append(decoder_input)\n","            if decoder_input == targ_lang.word2index['<end>']:\n","                break\n","    \n","    reference = reference[reference > 0]\n","    target = target[target > 0]\n","\n","    reference = [ref_lang.index2word[int(s)] for s in reference]\n","    target =    [targ_lang.index2word[int(s)] for s in target]\n","    candidate = [targ_lang.index2word[int(s)] for s in candidate]\n","\n","    smoother = SmoothingFunction()\n","    bleu1 = sentence_bleu([target[1:]], candidate[1:], weights=(1,), smoothing_function=smoother.method1)\n","    bleu2 = sentence_bleu([target[1:]], candidate[1:], weights=(1/2, 1/2), smoothing_function=smoother.method1)\n","    bleu3 = sentence_bleu([target[1:]], candidate[1:], weights=(1/3, 1/3, 1/3), smoothing_function=smoother.method1)\n","    bleu4 = sentence_bleu([target[1:]], candidate[1:], weights=(1/4, 1/4, 1/4, 1/4), smoothing_function=smoother.method1)\n","    return bleu1, bleu2, bleu3, bleu4, test_loss, \" \".join(reference), \" \".join(target), \" \".join(candidate)\n","\n","def loss_function(real, pred):\n","    \"\"\" Only consider non-pad inputs in the loss; mask needed \"\"\"\n","    mask = real.ge(1).float()\n","    \n","    loss_ = F.cross_entropy(pred, real) * mask \n","    return torch.mean(loss_)\n","\n","def train_model(encoder, decoder, targ_lang, train, num_epochs, learning_rate, batch_size, breakp = 1e10):\n","    # Return training losses\n","    losses = []\n","    \n","    # Model, optimizer, criterion\n","    encoder = encoder.to(device)\n","    decoder = decoder.to(device)\n","    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n","\n","    # Build batches\n","    batches = [df for g, df in train.groupby(np.arange(len(train)) // batch_size)]\n","\n","    # Train\n","    for i in range(num_epochs):\n","        epoch_loss = 0\n","        for b in range(len(batches)):\n","            batch = batches[b]\n","            if len(batch) == batch_size: # Discard partial batches\n","                target = torch.tensor([s for s in batch[list(batch)[1]]]).T.to(device)\n","                reference = torch.tensor([s for s in batch[list(batch)[0]]]).T.to(device)\n","\n","                # Encoder pass: [max_len, batch_size, hidden_size], [1, batch_size, hidden_size]\n","                encoder_output, encoder_hidden = encoder(reference) \n","\n","                # Decoder pass: teacher forcing\n","                loss = 0\n","                decoder_input = target[0].unsqueeze(0) # [1, batch_size]\n","                decoder_hidden = encoder_hidden\n","                for j in range(1, len(target)):\n","                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output) # [batch_size, output_size], [1, batch_size, hidden_size]\n","                    loss += loss_function(target[j], decoder_output)\n","                    decoder_input = target[j].unsqueeze(0)\n","\n","                # Step\n","                loss.backward()\n","                encoder_optimizer.step()\n","                decoder_optimizer.step()\n","                encoder_optimizer.zero_grad()\n","                decoder_optimizer.zero_grad()\n","\n","                # Prints\n","                epoch_loss += loss.item() / (len(target) * len(batches))\n","\n","        # Training losses\n","        print(\"EPOCH {}/{}, LOSS {}\".format(i + 1, num_epochs, epoch_loss))\n","        losses.append(epoch_loss)\n","    return losses "],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9R9Gj0aKS7lw"},"source":["# Base model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XeFcWwFpqpLW","executionInfo":{"status":"ok","timestamp":1607032688097,"user_tz":360,"elapsed":27018,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"bda4e1d1-78e9-441e-b6ef-f4b79e4d21a6"},"source":["# Train test split\n","data = data.sample(frac = 1, replace = False)\n","train = data.iloc[:data.shape[0]//4 * 3]\n","test = data.iloc[data.shape[0]//4 * 3:]\n","\n","# Model\n","HIDDEN_DIM = 128\n","encoder = EncoderRNN(input_lang.n_words, HIDDEN_DIM)\n","decoder = DecoderRNN(output_lang.n_words, HIDDEN_DIM)\n","print(\"N Params: \", sum(p.numel() for p in encoder.parameters()) + sum(p.numel() for p in decoder.parameters()))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["N Params:  635019\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KLv65YtkgxlA","executionInfo":{"status":"ok","timestamp":1607033038088,"user_tz":360,"elapsed":377007,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"13aae9f3-50d5-4059-d3fd-02e8e54bcb54"},"source":["losses = train_model(encoder, decoder, output_lang, train, 200, 1e-3, len(train)//9)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["EPOCH 1/200, LOSS 3.4156603813171387\n","EPOCH 2/200, LOSS 2.8196647961934405\n","EPOCH 3/200, LOSS 2.2994655768076577\n","EPOCH 4/200, LOSS 2.1306118011474613\n","EPOCH 5/200, LOSS 2.078759034474691\n","EPOCH 6/200, LOSS 2.0415146668752033\n","EPOCH 7/200, LOSS 2.012386131286621\n","EPOCH 8/200, LOSS 1.9838579495747881\n","EPOCH 9/200, LOSS 1.9553816000620523\n","EPOCH 10/200, LOSS 1.9263572216033937\n","EPOCH 11/200, LOSS 1.8962822596232096\n","EPOCH 12/200, LOSS 1.8655543804168702\n","EPOCH 13/200, LOSS 1.834837325414022\n","EPOCH 14/200, LOSS 1.804052146275838\n","EPOCH 15/200, LOSS 1.7734965483347573\n","EPOCH 16/200, LOSS 1.7431285063425699\n","EPOCH 17/200, LOSS 1.71350687344869\n","EPOCH 18/200, LOSS 1.6843976020812987\n","EPOCH 19/200, LOSS 1.6559871991475423\n","EPOCH 20/200, LOSS 1.6287623246510823\n","EPOCH 21/200, LOSS 1.6024671554565428\n","EPOCH 22/200, LOSS 1.576963504155477\n","EPOCH 23/200, LOSS 1.5525377909342448\n","EPOCH 24/200, LOSS 1.529240306218465\n","EPOCH 25/200, LOSS 1.5068501313527425\n","EPOCH 26/200, LOSS 1.4852965354919436\n","EPOCH 27/200, LOSS 1.4647048632303874\n","EPOCH 28/200, LOSS 1.4450177351633708\n","EPOCH 29/200, LOSS 1.4261524041493734\n","EPOCH 30/200, LOSS 1.408046086629232\n","EPOCH 31/200, LOSS 1.3905999819437662\n","EPOCH 32/200, LOSS 1.3736789067586261\n","EPOCH 33/200, LOSS 1.3571115334828692\n","EPOCH 34/200, LOSS 1.3408687591552737\n","EPOCH 35/200, LOSS 1.325002336502075\n","EPOCH 36/200, LOSS 1.3093981901804606\n","EPOCH 37/200, LOSS 1.2940390586853026\n","EPOCH 38/200, LOSS 1.2789409796396891\n","EPOCH 39/200, LOSS 1.2641882260640462\n","EPOCH 40/200, LOSS 1.2500589529673258\n","EPOCH 41/200, LOSS 1.2358317534128824\n","EPOCH 42/200, LOSS 1.2222425619761148\n","EPOCH 43/200, LOSS 1.2088222980499268\n","EPOCH 44/200, LOSS 1.1958905776341757\n","EPOCH 45/200, LOSS 1.1832747777303059\n","EPOCH 46/200, LOSS 1.1709638198216754\n","EPOCH 47/200, LOSS 1.1589357852935789\n","EPOCH 48/200, LOSS 1.1472071250279745\n","EPOCH 49/200, LOSS 1.1357773303985597\n","EPOCH 50/200, LOSS 1.1248029788335165\n","EPOCH 51/200, LOSS 1.1140706857045493\n","EPOCH 52/200, LOSS 1.1035048723220824\n","EPOCH 53/200, LOSS 1.0931889772415162\n","EPOCH 54/200, LOSS 1.0832693735758463\n","EPOCH 55/200, LOSS 1.0735947211583456\n","EPOCH 56/200, LOSS 1.0641912619272866\n","EPOCH 57/200, LOSS 1.0550281286239624\n","EPOCH 58/200, LOSS 1.0460997184117635\n","EPOCH 59/200, LOSS 1.037379550933838\n","EPOCH 60/200, LOSS 1.0293968359629315\n","EPOCH 61/200, LOSS 1.020878299077352\n","EPOCH 62/200, LOSS 1.0128688732783\n","EPOCH 63/200, LOSS 1.004963183403015\n","EPOCH 64/200, LOSS 0.9973267475763957\n","EPOCH 65/200, LOSS 0.9899503231048583\n","EPOCH 66/200, LOSS 0.9826637029647828\n","EPOCH 67/200, LOSS 0.9755728006362915\n","EPOCH 68/200, LOSS 0.9686891237894695\n","EPOCH 69/200, LOSS 0.9620250384012858\n","EPOCH 70/200, LOSS 0.9554999669392904\n","EPOCH 71/200, LOSS 0.9492992242177326\n","EPOCH 72/200, LOSS 0.9428430636723837\n","EPOCH 73/200, LOSS 0.9365003347396851\n","EPOCH 74/200, LOSS 0.930512809753418\n","EPOCH 75/200, LOSS 0.9245814959208171\n","EPOCH 76/200, LOSS 0.9188492695490518\n","EPOCH 77/200, LOSS 0.9132770458857218\n","EPOCH 78/200, LOSS 0.9078254302342732\n","EPOCH 79/200, LOSS 0.9024711132049561\n","EPOCH 80/200, LOSS 0.8972111145655315\n","EPOCH 81/200, LOSS 0.8920617659886678\n","EPOCH 82/200, LOSS 0.8870791196823119\n","EPOCH 83/200, LOSS 0.8823583205540975\n","EPOCH 84/200, LOSS 0.8774851640065511\n","EPOCH 85/200, LOSS 0.8726247946421306\n","EPOCH 86/200, LOSS 0.8678231716156005\n","EPOCH 87/200, LOSS 0.8632805267969766\n","EPOCH 88/200, LOSS 0.8587773243586223\n","EPOCH 89/200, LOSS 0.8543703397115071\n","EPOCH 90/200, LOSS 0.8500396887461344\n","EPOCH 91/200, LOSS 0.8457609017690022\n","EPOCH 92/200, LOSS 0.841541314125061\n","EPOCH 93/200, LOSS 0.8373948097229004\n","EPOCH 94/200, LOSS 0.833320919672648\n","EPOCH 95/200, LOSS 0.8293193578720094\n","EPOCH 96/200, LOSS 0.8253846724828084\n","EPOCH 97/200, LOSS 0.8215126355489095\n","EPOCH 98/200, LOSS 0.8177268822987874\n","EPOCH 99/200, LOSS 0.8139333089192708\n","EPOCH 100/200, LOSS 0.8114173253377279\n","EPOCH 101/200, LOSS 0.8080311854680379\n","EPOCH 102/200, LOSS 0.8038485685984293\n","EPOCH 103/200, LOSS 0.8004820346832277\n","EPOCH 104/200, LOSS 0.7977624893188476\n","EPOCH 105/200, LOSS 0.7944590489069621\n","EPOCH 106/200, LOSS 0.7903005599975585\n","EPOCH 107/200, LOSS 0.7867138544718424\n","EPOCH 108/200, LOSS 0.7835805972417197\n","EPOCH 109/200, LOSS 0.7802940050760904\n","EPOCH 110/200, LOSS 0.7771358648935953\n","EPOCH 111/200, LOSS 0.7741188605626425\n","EPOCH 112/200, LOSS 0.7711616436640423\n","EPOCH 113/200, LOSS 0.7682172377904256\n","EPOCH 114/200, LOSS 0.7652950048446656\n","EPOCH 115/200, LOSS 0.7624116977055868\n","EPOCH 116/200, LOSS 0.7595751762390136\n","EPOCH 117/200, LOSS 0.7567868868509928\n","EPOCH 118/200, LOSS 0.7540433168411255\n","EPOCH 119/200, LOSS 0.7513409694035849\n","EPOCH 120/200, LOSS 0.7486766894658407\n","EPOCH 121/200, LOSS 0.7460476716359457\n","EPOCH 122/200, LOSS 0.7434509356816609\n","EPOCH 123/200, LOSS 0.7408844073613484\n","EPOCH 124/200, LOSS 0.7383472442626954\n","EPOCH 125/200, LOSS 0.7358404874801637\n","EPOCH 126/200, LOSS 0.7333665211995443\n","EPOCH 127/200, LOSS 0.7309289455413819\n","EPOCH 128/200, LOSS 0.7285333315531412\n","EPOCH 129/200, LOSS 0.7261844555536906\n","EPOCH 130/200, LOSS 0.7238849401474\n","EPOCH 131/200, LOSS 0.721638019879659\n","EPOCH 132/200, LOSS 0.7194583892822266\n","EPOCH 133/200, LOSS 0.7173758506774902\n","EPOCH 134/200, LOSS 0.7153146346410115\n","EPOCH 135/200, LOSS 0.7130864222844442\n","EPOCH 136/200, LOSS 0.7108131090799968\n","EPOCH 137/200, LOSS 0.7085313081741333\n","EPOCH 138/200, LOSS 0.7064453363418579\n","EPOCH 139/200, LOSS 0.7043472846349079\n","EPOCH 140/200, LOSS 0.7022789478302003\n","EPOCH 141/200, LOSS 0.7002524773279826\n","EPOCH 142/200, LOSS 0.6982805967330934\n","EPOCH 143/200, LOSS 0.6963313897450765\n","EPOCH 144/200, LOSS 0.6943964560826619\n","EPOCH 145/200, LOSS 0.6924732367197672\n","EPOCH 146/200, LOSS 0.6905675967534384\n","EPOCH 147/200, LOSS 0.6886834224065146\n","EPOCH 148/200, LOSS 0.6868235349655152\n","EPOCH 149/200, LOSS 0.6849915583928426\n","EPOCH 150/200, LOSS 0.6831880489985149\n","EPOCH 151/200, LOSS 0.6814111868540447\n","EPOCH 152/200, LOSS 0.6796587308247884\n","EPOCH 153/200, LOSS 0.6779296875\n","EPOCH 154/200, LOSS 0.6762248516082763\n","EPOCH 155/200, LOSS 0.674546996752421\n","EPOCH 156/200, LOSS 0.6728999455769858\n","EPOCH 157/200, LOSS 0.6712873856226603\n","EPOCH 158/200, LOSS 0.6697201410929363\n","EPOCH 159/200, LOSS 0.668206795056661\n","EPOCH 160/200, LOSS 0.6665858109792074\n","EPOCH 161/200, LOSS 0.66493665377299\n","EPOCH 162/200, LOSS 0.6632755597432455\n","EPOCH 163/200, LOSS 0.6616316159566242\n","EPOCH 164/200, LOSS 0.6602169116338094\n","EPOCH 165/200, LOSS 0.6587171236673991\n","EPOCH 166/200, LOSS 0.657156205177307\n","EPOCH 167/200, LOSS 0.6554813782374065\n","EPOCH 168/200, LOSS 0.6540777603785196\n","EPOCH 169/200, LOSS 0.6525182485580444\n","EPOCH 170/200, LOSS 0.6510524670283\n","EPOCH 171/200, LOSS 0.6496745030085245\n","EPOCH 172/200, LOSS 0.6482520421346029\n","EPOCH 173/200, LOSS 0.6468441009521486\n","EPOCH 174/200, LOSS 0.6454879760742189\n","EPOCH 175/200, LOSS 0.6441401481628418\n","EPOCH 176/200, LOSS 0.6427916129430136\n","EPOCH 177/200, LOSS 0.6414716323216756\n","EPOCH 178/200, LOSS 0.6401827255884807\n","EPOCH 179/200, LOSS 0.6389037052790323\n","EPOCH 180/200, LOSS 0.6376384417215983\n","EPOCH 181/200, LOSS 0.6363981564839681\n","EPOCH 182/200, LOSS 0.635175085067749\n","EPOCH 183/200, LOSS 0.6339781681696575\n","EPOCH 184/200, LOSS 0.6328139464060466\n","EPOCH 185/200, LOSS 0.6316721439361572\n","EPOCH 186/200, LOSS 0.630580464998881\n","EPOCH 187/200, LOSS 0.6295836528142293\n","EPOCH 188/200, LOSS 0.6287202596664428\n","EPOCH 189/200, LOSS 0.6281667947769166\n","EPOCH 190/200, LOSS 0.6287658373514811\n","EPOCH 191/200, LOSS 0.6282470862070719\n","EPOCH 192/200, LOSS 0.6246107737223308\n","EPOCH 193/200, LOSS 0.6232662041982013\n","EPOCH 194/200, LOSS 0.621446426709493\n","EPOCH 195/200, LOSS 0.6202735662460328\n","EPOCH 196/200, LOSS 0.6190391461054485\n","EPOCH 197/200, LOSS 0.6178276538848878\n","EPOCH 198/200, LOSS 0.6167543093363445\n","EPOCH 199/200, LOSS 0.615699299176534\n","EPOCH 200/200, LOSS 0.6147365411122641\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NkNIfoBzDXKq"},"source":["# Base model + attention"]},{"cell_type":"code","metadata":{"id":"H6oXqCrmDXKq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607033038090,"user_tz":360,"elapsed":377007,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"d422ddfd-0c42-46d1-af1a-a0fccf9bcd08"},"source":["# Model\n","HIDDEN_DIM = 128\n","attention_encoder = EncoderRNN(input_lang.n_words, HIDDEN_DIM)\n","attention_decoder = AttentionDecoderRNN(output_lang.n_words, HIDDEN_DIM)\n","print(\"N Params: \", sum(p.numel() for p in attention_encoder.parameters()) + sum(p.numel() for p in attention_decoder.parameters()))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["N Params:  700940\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g-JUGP1nDXKr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607033771627,"user_tz":360,"elapsed":1110542,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"46eb16ec-46c1-4d08-9f11-d45e2a2a37ef"},"source":["attention_losses = train_model(attention_encoder, attention_decoder, output_lang, train, 200, 1e-3, len(train)//9)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["EPOCH 1/200, LOSS 3.3122084935506186\n","EPOCH 2/200, LOSS 2.5451439539591467\n","EPOCH 3/200, LOSS 2.275457127888998\n","EPOCH 4/200, LOSS 2.166823720932007\n","EPOCH 5/200, LOSS 2.1111319224039713\n","EPOCH 6/200, LOSS 2.076262124379476\n","EPOCH 7/200, LOSS 2.048429807027181\n","EPOCH 8/200, LOSS 2.0235949516296388\n","EPOCH 9/200, LOSS 1.9979519844055174\n","EPOCH 10/200, LOSS 1.9716910362243651\n","EPOCH 11/200, LOSS 1.9445461750030515\n","EPOCH 12/200, LOSS 1.9171135107676187\n","EPOCH 13/200, LOSS 1.889710505803426\n","EPOCH 14/200, LOSS 1.862348445256551\n","EPOCH 15/200, LOSS 1.8348708947499592\n","EPOCH 16/200, LOSS 1.806096108754476\n","EPOCH 17/200, LOSS 1.776286999384562\n","EPOCH 18/200, LOSS 1.744439156850179\n","EPOCH 19/200, LOSS 1.712066618601481\n","EPOCH 20/200, LOSS 1.6799107869466148\n","EPOCH 21/200, LOSS 1.6493005911509195\n","EPOCH 22/200, LOSS 1.6190707683563232\n","EPOCH 23/200, LOSS 1.5901972770690918\n","EPOCH 24/200, LOSS 1.5624743302663169\n","EPOCH 25/200, LOSS 1.5354634761810302\n","EPOCH 26/200, LOSS 1.5086391290028889\n","EPOCH 27/200, LOSS 1.482345199584961\n","EPOCH 28/200, LOSS 1.4562671025594076\n","EPOCH 29/200, LOSS 1.4296164830525715\n","EPOCH 30/200, LOSS 1.40417267481486\n","EPOCH 31/200, LOSS 1.3797285079956056\n","EPOCH 32/200, LOSS 1.3551878452301027\n","EPOCH 33/200, LOSS 1.3319615681966146\n","EPOCH 34/200, LOSS 1.3094114780426025\n","EPOCH 35/200, LOSS 1.2868373711903889\n","EPOCH 36/200, LOSS 1.2651147842407227\n","EPOCH 37/200, LOSS 1.2444089253743489\n","EPOCH 38/200, LOSS 1.2244995911916097\n","EPOCH 39/200, LOSS 1.2051390091578165\n","EPOCH 40/200, LOSS 1.1857139110565185\n","EPOCH 41/200, LOSS 1.1679179588953652\n","EPOCH 42/200, LOSS 1.150309944152832\n","EPOCH 43/200, LOSS 1.1334596157073975\n","EPOCH 44/200, LOSS 1.1183429400126141\n","EPOCH 45/200, LOSS 1.102833906809489\n","EPOCH 46/200, LOSS 1.0875498215357464\n","EPOCH 47/200, LOSS 1.0726498126983643\n","EPOCH 48/200, LOSS 1.058799147605896\n","EPOCH 49/200, LOSS 1.0450953880945841\n","EPOCH 50/200, LOSS 1.031984066963196\n","EPOCH 51/200, LOSS 1.0198670069376627\n","EPOCH 52/200, LOSS 1.006982898712158\n","EPOCH 53/200, LOSS 0.9961074431737265\n","EPOCH 54/200, LOSS 0.9857010841369629\n","EPOCH 55/200, LOSS 0.9727583010991414\n","EPOCH 56/200, LOSS 0.9624290943145752\n","EPOCH 57/200, LOSS 0.9516934235890706\n","EPOCH 58/200, LOSS 0.9418187300364176\n","EPOCH 59/200, LOSS 0.931948184967041\n","EPOCH 60/200, LOSS 0.9223818540573121\n","EPOCH 61/200, LOSS 0.9131293296813965\n","EPOCH 62/200, LOSS 0.9040961980819703\n","EPOCH 63/200, LOSS 0.8954559485117595\n","EPOCH 64/200, LOSS 0.8913553476333618\n","EPOCH 65/200, LOSS 0.8796154896418255\n","EPOCH 66/200, LOSS 0.8719303528467813\n","EPOCH 67/200, LOSS 0.8639083782831828\n","EPOCH 68/200, LOSS 0.8560405810674033\n","EPOCH 69/200, LOSS 0.8487960577011107\n","EPOCH 70/200, LOSS 0.8417370319366455\n","EPOCH 71/200, LOSS 0.834835163752238\n","EPOCH 72/200, LOSS 0.8281799713770548\n","EPOCH 73/200, LOSS 0.8218262513478597\n","EPOCH 74/200, LOSS 0.8161930163701376\n","EPOCH 75/200, LOSS 0.8135062694549561\n","EPOCH 76/200, LOSS 0.8071468750635782\n","EPOCH 77/200, LOSS 0.7980558474858602\n","EPOCH 78/200, LOSS 0.7917168696721394\n","EPOCH 79/200, LOSS 0.7855706214904785\n","EPOCH 80/200, LOSS 0.7799336751302084\n","EPOCH 81/200, LOSS 0.7745847622553507\n","EPOCH 82/200, LOSS 0.769335182507833\n","EPOCH 83/200, LOSS 0.764118218421936\n","EPOCH 84/200, LOSS 0.7590747992197673\n","EPOCH 85/200, LOSS 0.7541636308034262\n","EPOCH 86/200, LOSS 0.74937318166097\n","EPOCH 87/200, LOSS 0.7446814139684041\n","EPOCH 88/200, LOSS 0.7400812784830729\n","EPOCH 89/200, LOSS 0.7355694691340128\n","EPOCH 90/200, LOSS 0.731148624420166\n","EPOCH 91/200, LOSS 0.7268215656280518\n","EPOCH 92/200, LOSS 0.7225877443949382\n","EPOCH 93/200, LOSS 0.718446946144104\n","EPOCH 94/200, LOSS 0.7144034783045452\n","EPOCH 95/200, LOSS 0.7104671716690063\n","EPOCH 96/200, LOSS 0.7066580216089885\n","EPOCH 97/200, LOSS 0.7030241012573243\n","EPOCH 98/200, LOSS 0.6997082074483236\n","EPOCH 99/200, LOSS 0.6971576770146688\n","EPOCH 100/200, LOSS 0.6957262357076008\n","EPOCH 101/200, LOSS 0.692228905359904\n","EPOCH 102/200, LOSS 0.6856413841247558\n","EPOCH 103/200, LOSS 0.6819788535435994\n","EPOCH 104/200, LOSS 0.6784390687942504\n","EPOCH 105/200, LOSS 0.6747828324635823\n","EPOCH 106/200, LOSS 0.6716588338216146\n","EPOCH 107/200, LOSS 0.668599772453308\n","EPOCH 108/200, LOSS 0.6655333916346231\n","EPOCH 109/200, LOSS 0.6625131289164226\n","EPOCH 110/200, LOSS 0.6595504760742186\n","EPOCH 111/200, LOSS 0.6566458384195963\n","EPOCH 112/200, LOSS 0.6538051366806029\n","EPOCH 113/200, LOSS 0.6510371049245199\n","EPOCH 114/200, LOSS 0.6483336766560873\n","EPOCH 115/200, LOSS 0.6456633806228638\n","EPOCH 116/200, LOSS 0.6429944356282552\n","EPOCH 117/200, LOSS 0.6403677145640055\n","EPOCH 118/200, LOSS 0.6378718217213948\n","EPOCH 119/200, LOSS 0.6354647477467855\n","EPOCH 120/200, LOSS 0.6330109437306722\n","EPOCH 121/200, LOSS 0.6305896043777466\n","EPOCH 122/200, LOSS 0.6281203508377076\n","EPOCH 123/200, LOSS 0.6257022062937418\n","EPOCH 124/200, LOSS 0.6231753746668498\n","EPOCH 125/200, LOSS 0.6208289702733358\n","EPOCH 126/200, LOSS 0.6185481230417886\n","EPOCH 127/200, LOSS 0.6163037379582723\n","EPOCH 128/200, LOSS 0.6141106128692627\n","EPOCH 129/200, LOSS 0.6121317545572916\n","EPOCH 130/200, LOSS 0.6108146746953327\n","EPOCH 131/200, LOSS 0.610118047396342\n","EPOCH 132/200, LOSS 0.6068857272466024\n","EPOCH 133/200, LOSS 0.6041191339492797\n","EPOCH 134/200, LOSS 0.6015893777211507\n","EPOCH 135/200, LOSS 0.5996820092201232\n","EPOCH 136/200, LOSS 0.5976100484530131\n","EPOCH 137/200, LOSS 0.5956286271413167\n","EPOCH 138/200, LOSS 0.5938142657279968\n","EPOCH 139/200, LOSS 0.5921241879463196\n","EPOCH 140/200, LOSS 0.5910377264022827\n","EPOCH 141/200, LOSS 0.5890630404154459\n","EPOCH 142/200, LOSS 0.5888078451156616\n","EPOCH 143/200, LOSS 0.5851208567619324\n","EPOCH 144/200, LOSS 0.5839720408121746\n","EPOCH 145/200, LOSS 0.581870706876119\n","EPOCH 146/200, LOSS 0.5804737210273743\n","EPOCH 147/200, LOSS 0.5790562192598979\n","EPOCH 148/200, LOSS 0.5781406203905741\n","EPOCH 149/200, LOSS 0.5787657618522644\n","EPOCH 150/200, LOSS 0.582882289091746\n","EPOCH 151/200, LOSS 0.5813459396362304\n","EPOCH 152/200, LOSS 0.573961047331492\n","EPOCH 153/200, LOSS 0.5725879867871602\n","EPOCH 154/200, LOSS 0.5711133201917012\n","EPOCH 155/200, LOSS 0.57064874569575\n","EPOCH 156/200, LOSS 0.5686598658561708\n","EPOCH 157/200, LOSS 0.5663459499677023\n","EPOCH 158/200, LOSS 0.5635752081871034\n","EPOCH 159/200, LOSS 0.5617351055145263\n","EPOCH 160/200, LOSS 0.5603207985560099\n","EPOCH 161/200, LOSS 0.558957060178121\n","EPOCH 162/200, LOSS 0.5575456102689108\n","EPOCH 163/200, LOSS 0.556072731812795\n","EPOCH 164/200, LOSS 0.5545828938484192\n","EPOCH 165/200, LOSS 0.5531210343043009\n","EPOCH 166/200, LOSS 0.5517152547836305\n","EPOCH 167/200, LOSS 0.5503752827644348\n","EPOCH 168/200, LOSS 0.5490984360376994\n","EPOCH 169/200, LOSS 0.5478773554166158\n","EPOCH 170/200, LOSS 0.546704351902008\n","EPOCH 171/200, LOSS 0.5455762147903442\n","EPOCH 172/200, LOSS 0.5445015152295432\n","EPOCH 173/200, LOSS 0.5435181379318237\n","EPOCH 174/200, LOSS 0.542732036113739\n","EPOCH 175/200, LOSS 0.5423929889996846\n","EPOCH 176/200, LOSS 0.5424528916676838\n","EPOCH 177/200, LOSS 0.5421701391537984\n","EPOCH 178/200, LOSS 0.5434316317240397\n","EPOCH 179/200, LOSS 0.5392547130584716\n","EPOCH 180/200, LOSS 0.5365123351414999\n","EPOCH 181/200, LOSS 0.5355238636334737\n","EPOCH 182/200, LOSS 0.5341042240460714\n","EPOCH 183/200, LOSS 0.5327864170074462\n","EPOCH 184/200, LOSS 0.5318111737569173\n","EPOCH 185/200, LOSS 0.5308513919512431\n","EPOCH 186/200, LOSS 0.5298017779986063\n","EPOCH 187/200, LOSS 0.5287292480468749\n","EPOCH 188/200, LOSS 0.5276177128156027\n","EPOCH 189/200, LOSS 0.5264864802360535\n","EPOCH 190/200, LOSS 0.5259082635243734\n","EPOCH 191/200, LOSS 0.5256893157958985\n","EPOCH 192/200, LOSS 0.5253054539362589\n","EPOCH 193/200, LOSS 0.5248683253924052\n","EPOCH 194/200, LOSS 0.5224200407663981\n","EPOCH 195/200, LOSS 0.5218751867612202\n","EPOCH 196/200, LOSS 0.5206448872884114\n","EPOCH 197/200, LOSS 0.5193143328030904\n","EPOCH 198/200, LOSS 0.5181810776392619\n","EPOCH 199/200, LOSS 0.5174895604451498\n","EPOCH 200/200, LOSS 0.5166896939277649\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AB7hW9IPS3gM"},"source":["# Base model + attention + deep encoder"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OFreqqb-S3gM","executionInfo":{"status":"ok","timestamp":1607034268716,"user_tz":360,"elapsed":816,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"61f17f85-72e9-4cae-d260-ace41165a3c3"},"source":["# Model\n","import copy\n","HIDDEN_DIM = 128\n","deep_encoder = EncoderRNN(input_lang.n_words, HIDDEN_DIM, 2)\n","deep_decoder = copy.deepcopy(attention_decoder) #AttentionDecoderRNN(output_lang.n_words, HIDDEN_DIM)\n","print(\"N Params: \", sum(p.numel() for p in deep_encoder.parameters()) + sum(p.numel() for p in deep_decoder.parameters()))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["N Params:  800012\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qwXlbKStS3gM","executionInfo":{"status":"ok","timestamp":1607035050394,"user_tz":360,"elapsed":780814,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"00fabf80-171b-4a62-f9f4-34eeb4e370d8"},"source":["deep_losses = train_model(deep_encoder, deep_decoder, output_lang, train, 200, 1e-3, len(train)//9)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["EPOCH 1/200, LOSS 2.222773027420044\n","EPOCH 2/200, LOSS 1.8009695212046304\n","EPOCH 3/200, LOSS 1.6470070521036786\n","EPOCH 4/200, LOSS 1.5419470628102618\n","EPOCH 5/200, LOSS 1.4518310864766442\n","EPOCH 6/200, LOSS 1.3688152154286701\n","EPOCH 7/200, LOSS 1.288111702601115\n","EPOCH 8/200, LOSS 1.2132713874181114\n","EPOCH 9/200, LOSS 1.1471234798431398\n","EPOCH 10/200, LOSS 1.090377998352051\n","EPOCH 11/200, LOSS 1.0417444705963135\n","EPOCH 12/200, LOSS 1.000206176439921\n","EPOCH 13/200, LOSS 0.964358917872111\n","EPOCH 14/200, LOSS 0.9329685449600219\n","EPOCH 15/200, LOSS 0.9052703301111857\n","EPOCH 16/200, LOSS 0.8804794629414876\n","EPOCH 17/200, LOSS 0.8583083391189574\n","EPOCH 18/200, LOSS 0.8388901789983112\n","EPOCH 19/200, LOSS 0.8214262247085572\n","EPOCH 20/200, LOSS 0.8055035670598347\n","EPOCH 21/200, LOSS 0.7903485457102457\n","EPOCH 22/200, LOSS 0.7770015796025593\n","EPOCH 23/200, LOSS 0.7649224122365316\n","EPOCH 24/200, LOSS 0.7537423610687256\n","EPOCH 25/200, LOSS 0.7433633724848429\n","EPOCH 26/200, LOSS 0.7340614318847656\n","EPOCH 27/200, LOSS 0.7241527636845906\n","EPOCH 28/200, LOSS 0.7154453992843628\n","EPOCH 29/200, LOSS 0.7074616034825643\n","EPOCH 30/200, LOSS 0.7001141707102458\n","EPOCH 31/200, LOSS 0.6930804252624511\n","EPOCH 32/200, LOSS 0.6862994035085043\n","EPOCH 33/200, LOSS 0.6816640535990397\n","EPOCH 34/200, LOSS 0.6770487705866496\n","EPOCH 35/200, LOSS 0.6701577345530192\n","EPOCH 36/200, LOSS 0.6642420689264934\n","EPOCH 37/200, LOSS 0.6585001707077027\n","EPOCH 38/200, LOSS 0.6533879041671753\n","EPOCH 39/200, LOSS 0.6487526814142863\n","EPOCH 40/200, LOSS 0.6442858537038167\n","EPOCH 41/200, LOSS 0.6400619347890217\n","EPOCH 42/200, LOSS 0.6360157012939454\n","EPOCH 43/200, LOSS 0.6321463028589884\n","EPOCH 44/200, LOSS 0.6284212430318197\n","EPOCH 45/200, LOSS 0.6248942931493123\n","EPOCH 46/200, LOSS 0.6217066844304404\n","EPOCH 47/200, LOSS 0.6192428986231486\n","EPOCH 48/200, LOSS 0.6157377958297731\n","EPOCH 49/200, LOSS 0.6123197555541992\n","EPOCH 50/200, LOSS 0.6083393732706707\n","EPOCH 51/200, LOSS 0.6055848042170207\n","EPOCH 52/200, LOSS 0.6024666786193847\n","EPOCH 53/200, LOSS 0.5996891379356384\n","EPOCH 54/200, LOSS 0.5970738410949706\n","EPOCH 55/200, LOSS 0.5944436311721801\n","EPOCH 56/200, LOSS 0.5919010758399963\n","EPOCH 57/200, LOSS 0.5894692420959473\n","EPOCH 58/200, LOSS 0.5871741135915121\n","EPOCH 59/200, LOSS 0.585293173789978\n","EPOCH 60/200, LOSS 0.583609127998352\n","EPOCH 61/200, LOSS 0.580452545483907\n","EPOCH 62/200, LOSS 0.5776315569877624\n","EPOCH 63/200, LOSS 0.5755316495895386\n","EPOCH 64/200, LOSS 0.5736670970916748\n","EPOCH 65/200, LOSS 0.5716035803159077\n","EPOCH 66/200, LOSS 0.5696457624435425\n","EPOCH 67/200, LOSS 0.5677771290143331\n","EPOCH 68/200, LOSS 0.5660329143206279\n","EPOCH 69/200, LOSS 0.5643213510513305\n","EPOCH 70/200, LOSS 0.5625463565190634\n","EPOCH 71/200, LOSS 0.5610822399457295\n","EPOCH 72/200, LOSS 0.5574136694272359\n","EPOCH 73/200, LOSS 0.556100070476532\n","EPOCH 74/200, LOSS 0.5537565430005391\n","EPOCH 75/200, LOSS 0.5521239320437114\n","EPOCH 76/200, LOSS 0.5503416856129963\n","EPOCH 77/200, LOSS 0.5483968655268351\n","EPOCH 78/200, LOSS 0.5465688467025757\n","EPOCH 79/200, LOSS 0.5448942025502523\n","EPOCH 80/200, LOSS 0.5433368921279906\n","EPOCH 81/200, LOSS 0.5419558405876159\n","EPOCH 82/200, LOSS 0.540977402528127\n","EPOCH 83/200, LOSS 0.5400756597518921\n","EPOCH 84/200, LOSS 0.5376744866371156\n","EPOCH 85/200, LOSS 0.5370746572812398\n","EPOCH 86/200, LOSS 0.5350057284037272\n","EPOCH 87/200, LOSS 0.5341869473457336\n","EPOCH 88/200, LOSS 0.5343535542488098\n","EPOCH 89/200, LOSS 0.5376465201377868\n","EPOCH 90/200, LOSS 0.5368458072344462\n","EPOCH 91/200, LOSS 0.5297328551610311\n","EPOCH 92/200, LOSS 0.5279078245162964\n","EPOCH 93/200, LOSS 0.5258530100186667\n","EPOCH 94/200, LOSS 0.5235995809237162\n","EPOCH 95/200, LOSS 0.5224470337231953\n","EPOCH 96/200, LOSS 0.5214221795399984\n","EPOCH 97/200, LOSS 0.5202128251393636\n","EPOCH 98/200, LOSS 0.5187789479891459\n","EPOCH 99/200, LOSS 0.5172933419545491\n","EPOCH 100/200, LOSS 0.5159747878710429\n","EPOCH 101/200, LOSS 0.5148044387499491\n","EPOCH 102/200, LOSS 0.5137401342391968\n","EPOCH 103/200, LOSS 0.5127861579259236\n","EPOCH 104/200, LOSS 0.5119428714116414\n","EPOCH 105/200, LOSS 0.5110472242037456\n","EPOCH 106/200, LOSS 0.5099774479866027\n","EPOCH 107/200, LOSS 0.5089325348536173\n","EPOCH 108/200, LOSS 0.5085409005482991\n","EPOCH 109/200, LOSS 0.5085599064826966\n","EPOCH 110/200, LOSS 0.5067336638768514\n","EPOCH 111/200, LOSS 0.5041285157203674\n","EPOCH 112/200, LOSS 0.5019930680592855\n","EPOCH 113/200, LOSS 0.5007144967714946\n","EPOCH 114/200, LOSS 0.4992489099502564\n","EPOCH 115/200, LOSS 0.49785070021947225\n","EPOCH 116/200, LOSS 0.49654254515965773\n","EPOCH 117/200, LOSS 0.49536731243133536\n","EPOCH 118/200, LOSS 0.4942954778671264\n","EPOCH 119/200, LOSS 0.49329119523366294\n","EPOCH 120/200, LOSS 0.4923184037208557\n","EPOCH 121/200, LOSS 0.491368846098582\n","EPOCH 122/200, LOSS 0.4904532194137573\n","EPOCH 123/200, LOSS 0.48959325154622396\n","EPOCH 124/200, LOSS 0.48882350921630857\n","EPOCH 125/200, LOSS 0.48822703758875535\n","EPOCH 126/200, LOSS 0.48801295359929403\n","EPOCH 127/200, LOSS 0.48869899113972987\n","EPOCH 128/200, LOSS 0.49011695782343556\n","EPOCH 129/200, LOSS 0.49010027647018434\n","EPOCH 130/200, LOSS 0.4859462459882101\n","EPOCH 131/200, LOSS 0.484934910138448\n","EPOCH 132/200, LOSS 0.48325878381729126\n","EPOCH 133/200, LOSS 0.48118046522140506\n","EPOCH 134/200, LOSS 0.4796460469563802\n","EPOCH 135/200, LOSS 0.47852893273035685\n","EPOCH 136/200, LOSS 0.47755281527837123\n","EPOCH 137/200, LOSS 0.47651034593582153\n","EPOCH 138/200, LOSS 0.47540726264317834\n","EPOCH 139/200, LOSS 0.4743181506792704\n","EPOCH 140/200, LOSS 0.4732743064562479\n","EPOCH 141/200, LOSS 0.47226384878158567\n","EPOCH 142/200, LOSS 0.47127564350763956\n","EPOCH 143/200, LOSS 0.47030522028605143\n","EPOCH 144/200, LOSS 0.4693605462710062\n","EPOCH 145/200, LOSS 0.4684553543726603\n","EPOCH 146/200, LOSS 0.4676017403602601\n","EPOCH 147/200, LOSS 0.4668128093083699\n","EPOCH 148/200, LOSS 0.4661221941312154\n","EPOCH 149/200, LOSS 0.46562165816624956\n","EPOCH 150/200, LOSS 0.46559454202651973\n","EPOCH 151/200, LOSS 0.4667999386787415\n","EPOCH 152/200, LOSS 0.4678146163622538\n","EPOCH 153/200, LOSS 0.46910185416539507\n","EPOCH 154/200, LOSS 0.46364536285400393\n","EPOCH 155/200, LOSS 0.4617789030075074\n","EPOCH 156/200, LOSS 0.46029620567957563\n","EPOCH 157/200, LOSS 0.4587579170862833\n","EPOCH 158/200, LOSS 0.45787026484807336\n","EPOCH 159/200, LOSS 0.45695108970006304\n","EPOCH 160/200, LOSS 0.4559391021728515\n","EPOCH 161/200, LOSS 0.4550009767214457\n","EPOCH 162/200, LOSS 0.45414506991704306\n","EPOCH 163/200, LOSS 0.4533522804578146\n","EPOCH 164/200, LOSS 0.4526139140129089\n","EPOCH 165/200, LOSS 0.4519901196161906\n","EPOCH 166/200, LOSS 0.45153303941090905\n","EPOCH 167/200, LOSS 0.4511593381563822\n","EPOCH 168/200, LOSS 0.45099833408991497\n","EPOCH 169/200, LOSS 0.45103782415390015\n","EPOCH 170/200, LOSS 0.451015567779541\n","EPOCH 171/200, LOSS 0.4514389912287394\n","EPOCH 172/200, LOSS 0.45010769367218023\n","EPOCH 173/200, LOSS 0.44885582129160567\n","EPOCH 174/200, LOSS 0.44742943048477174\n","EPOCH 175/200, LOSS 0.446216348807017\n","EPOCH 176/200, LOSS 0.445392926534017\n","EPOCH 177/200, LOSS 0.44438890616099036\n","EPOCH 178/200, LOSS 0.4434913794199626\n","EPOCH 179/200, LOSS 0.4427102605501811\n","EPOCH 180/200, LOSS 0.4418615341186523\n","EPOCH 181/200, LOSS 0.44124234120051065\n","EPOCH 182/200, LOSS 0.4408727089564006\n","EPOCH 183/200, LOSS 0.4406132936477661\n","EPOCH 184/200, LOSS 0.440577753384908\n","EPOCH 185/200, LOSS 0.44058560132980346\n","EPOCH 186/200, LOSS 0.4409272909164429\n","EPOCH 187/200, LOSS 0.44118934075037636\n","EPOCH 188/200, LOSS 0.43964201609293624\n","EPOCH 189/200, LOSS 0.43749147653579706\n","EPOCH 190/200, LOSS 0.43593034744262693\n","EPOCH 191/200, LOSS 0.43483714262644446\n","EPOCH 192/200, LOSS 0.43377886215845746\n","EPOCH 193/200, LOSS 0.43288402557373046\n","EPOCH 194/200, LOSS 0.43208070596059156\n","EPOCH 195/200, LOSS 0.43139396905899047\n","EPOCH 196/200, LOSS 0.43095112244288125\n","EPOCH 197/200, LOSS 0.43029479583104446\n","EPOCH 198/200, LOSS 0.43000006675720215\n","EPOCH 199/200, LOSS 0.42983350753784183\n","EPOCH 200/200, LOSS 0.4291659832000732\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r_2HbO1bUJ1H"},"source":["# Model comparisons"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"BlZrm8JKS3gN","executionInfo":{"status":"ok","timestamp":1607035051753,"user_tz":360,"elapsed":1346,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"ab86e562-2fe9-48fa-8ab0-e01fb9551fe5"},"source":["import matplotlib.pyplot as plt\n","plt.plot(losses, '-', label = 'Base')\n","plt.plot(attention_losses, '-', label = 'Attention')\n","plt.plot(deep_losses, '-', label = 'Deep Attention')\n","plt.ylim((0, 5))\n","plt.xlabel('Epoch')\n","plt.ylabel('CE Loss')\n","plt.legend(loc='best')\n","plt.show()"],"execution_count":18,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7T3MMe-S3gN","executionInfo":{"status":"ok","timestamp":1607036104056,"user_tz":360,"elapsed":1053639,"user":{"displayName":"Shivesh Pathak","photoUrl":"","userId":"14785110140322240473"}},"outputId":"929229e3-67e5-4cd3-aa64-df025d97ad91"},"source":["for e, d in zip(\n","        [encoder, attention_encoder, deep_encoder],\n","        [decoder, attention_decoder, deep_decoder],\n","    ):\n","    total_bleu1 = 0\n","    total_bleu2 = 0\n","    total_bleu3 = 0\n","    total_bleu4 = 0\n","    for ind in range(test.shape[0]):\n","        bleu1, bleu2, bleu3, bleu4, loss, r, t, c = translate_sentence(e,\n","                                                                      d, \n","                                                                      test.iloc[ind], \n","                                                                      input_lang, \n","                                                                      output_lang)\n","        total_bleu1 += bleu1 * 100 / test.shape[0]\n","        total_bleu2 += bleu2 * 100 / test.shape[0]\n","        total_bleu3 += bleu3 * 100 / test.shape[0]\n","        total_bleu4 += bleu4 * 100 / test.shape[0]\n","        if ind < 10: \n","            print(r, \"|\", t, \"|\", c)\n","    print(\"{:.3f}, {:.3f}, {:.3f}, {:.3f}\".format(total_bleu1, total_bleu2, total_bleu3, total_bleu4))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["<start> give me your knife <end> | <start> geben sie mir ihr messer <end> | <start> geben sie mir dein messer <end>\n","<start> my <unk> is to learn french <end> | <start> mein <unk> ist es franzosisch zu lernen <end> | <start> mein franzosisch ist zu lernen <end>\n","<start> i think tom will like this <end> | <start> ich denke tom wird das mogen <end> | <start> ich glaube tom wird das gerne <end>\n","<start> tom and mary bought a home with a <unk> <end> | <start> tom und maria <unk> sich ein haus mit <unk> <end> | <start> tom und maria haben eine <unk> nach boston <unk> <end>\n","<start> <unk> more water to it <end> | <start> tut mehr wasser dazu <end> | <start> <unk> wir noch etwas zu essen <end>\n","<start> i used my <unk> <end> | <start> ich habe meine <unk> benutzt <end> | <start> ich habe meine <unk> <unk> <end>\n","<start> if you wish you can go <end> | <start> wenn sie wollen konnen sie gehen <end> | <start> wenn du kannst gehen konnen sie gehen <end>\n","<start> are you ok now <end> | <start> geht es dir jetzt gut <end> | <start> bist du jetzt in ordnung <end>\n","<start> i could not get out <end> | <start> ich konnte nicht heraus <end> | <start> ich konnte nicht <unk> <end>\n","<start> tom didnt sound <unk> <unk> <end> | <start> tom schien nicht <unk> besorgt zu sein <end> | <start> tom hat nicht <unk> <unk> <end>\n","61.709, 42.844, 30.862, 24.540\n","<start> give me your knife <end> | <start> geben sie mir ihr messer <end> | <start> gib mir dein messer <end>\n","<start> my <unk> is to learn french <end> | <start> mein <unk> ist es franzosisch zu lernen <end> | <start> mein <unk> ist franzosisch zu lernen <end>\n","<start> i think tom will like this <end> | <start> ich denke tom wird das mogen <end> | <start> ich denke tom wird das hier <end>\n","<start> tom and mary bought a home with a <unk> <end> | <start> tom und maria <unk> sich ein haus mit <unk> <end> | <start> tom und maria haben sich mit einem <unk> einer <unk> gekauft\n","<start> <unk> more water to it <end> | <start> tut mehr wasser dazu <end> | <start> <unk> <unk> <unk> <end>\n","<start> i used my <unk> <end> | <start> ich habe meine <unk> benutzt <end> | <start> ich habe meine <unk> <unk> <end>\n","<start> if you wish you can go <end> | <start> wenn sie wollen konnen sie gehen <end> | <start> wenn du willst kannst werden <end>\n","<start> are you ok now <end> | <start> geht es dir jetzt gut <end> | <start> bist du jetzt in ordnung <end>\n","<start> i could not get out <end> | <start> ich konnte nicht heraus <end> | <start> ich konnte nicht <unk> <end>\n","<start> tom didnt sound <unk> <unk> <end> | <start> tom schien nicht <unk> besorgt zu sein <end> | <start> tom <unk> nicht <unk> <end>\n","68.164, 51.383, 39.427, 32.279\n","<start> give me your knife <end> | <start> geben sie mir ihr messer <end> | <start> gib mir dein messer <end>\n","<start> my <unk> is to learn french <end> | <start> mein <unk> ist es franzosisch zu lernen <end> | <start> mein <unk> ist franzosisch zu lernen <end>\n","<start> i think tom will like this <end> | <start> ich denke tom wird das mogen <end> | <start> ich glaube tom wird das mogen <end>\n","<start> tom and mary bought a home with a <unk> <end> | <start> tom und maria <unk> sich ein haus mit <unk> <end> | <start> tom und maria haben sich einen <unk> bei einem <unk> gekauft\n","<start> <unk> more water to it <end> | <start> tut mehr wasser dazu <end> | <start> <unk> <unk> etwas wasser <end>\n","<start> i used my <unk> <end> | <start> ich habe meine <unk> benutzt <end> | <start> ich habe meine <unk> <unk> <end>\n","<start> if you wish you can go <end> | <start> wenn sie wollen konnen sie gehen <end> | <start> wenn du willst willst du <unk> <end>\n","<start> are you ok now <end> | <start> geht es dir jetzt gut <end> | <start> bist du jetzt in ordnung <end>\n","<start> i could not get out <end> | <start> ich konnte nicht heraus <end> | <start> ich konnte nicht <unk> <end>\n","<start> tom didnt sound <unk> <unk> <end> | <start> tom schien nicht <unk> besorgt zu sein <end> | <start> tom hat keine <unk> <unk> <unk> <end>\n","69.502, 53.033, 41.214, 34.009\n"],"name":"stdout"}]}]}